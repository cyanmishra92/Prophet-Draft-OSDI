
@article{jacobs1991adaptive,
  title={Adaptive mixtures of local experts},
  author={Jacobs, Robert A and Jordan, Michael I and Nowlan, Steven J and Hinton, Geoffrey E},
  journal={Neural computation},
  volume={3},
  number={1},
  pages={79--87},
  year={1991},
  publisher={MIT Press}
}

@article{jordan1994hierarchical,
  title={Hierarchical mixtures of experts and the EM algorithm},
  author={Jordan, Michael I and Jacobs, Robert A},
  journal={Neural computation},
  volume={6},
  number={2},
  pages={181--214},
  year={1994},
  publisher={MIT Press}
}

@article{miller,
  title={Note on the bias of information estimates},
  author={Miller, George},
  journal={Information theory in psychology: Problems and methods},
  year={1955},
  publisher={Free Press}
}

@article{fedus2022switch,
  title={Switch transformers: Scaling to trillion parameter models with simple and efficient sparsity},
  author={Fedus, William and Zoph, Barret and Shazeer, Noam},
  journal={Journal of Machine Learning Research},
  volume={23},
  number={120},
  pages={1--39},
  year={2022}
}

@inproceedings{pregated,
  title={Pre-gated moe: An algorithm-system co-design for fast and scalable mixture-of-expert inference},
  author={Hwang, Ranggi and Wei, Jianyu and Cao, Shijie and Hwang, Changho and Tang, Xiaohu and Cao, Ting and Yang, Mao},
  booktitle={2024 ACM/IEEE 51st Annual International Symposium on Computer Architecture (ISCA)},
  pages={1018--1031},
  year={2024},
  organization={IEEE}
}


@article{promoe,
  title={Promoe: Fast moe-based llm serving using proactive caching},
  author={Song, Xiaoniu and Zhong, Zihang and Chen, Rong and Chen, Haibo},
  journal={arXiv preprint arXiv:2410.22134},
  year={2024}
}

@article{expertflow,
  title={Expertflow: Optimized expert activation and token allocation for efficient mixture-of-experts inference},
  author={He, Xin and Zhang, Shunkang and Wang, Yuxin and Yin, Haiyan and Zeng, Zihao and Shi, Shaohuai and Tang, Zhenheng and Chu, Xiaowen and Tsang, Ivor and Soon, Ong Yew},
  journal={arXiv preprint arXiv:2410.17954},
  year={2024}
}

@article{fate,
  title={Fate: Fast Edge Inference of Mixture-of-Experts Models via Cross-Layer Gate},
  author={Fang, Zhiyuan and Hong, Zicong and Huang, Yuegui and Lyu, Yufeng and Chen, Wuhui and Yu, Yue and Yu, Fan and Zheng, Zibin},
  journal={arXiv preprint arXiv:2502.12224},
  year={2025}
}

@article{hobbit,
  title={Hobbit: A mixed precision expert offloading system for fast moe inference},
  author={Tang, Peng and Liu, Jiacheng and Hou, Xiaofeng and Pu, Yifei and Wang, Jing and Heng, Pheng-Ann and Li, Chao and Guo, Minyi},
  journal={arXiv preprint arXiv:2411.01433},
  year={2024}
}

@article{yang2025qwen3,
  title={Qwen3 technical report},
  author={Yang, An and Li, Anfeng and Yang, Baosong and Zhang, Beichen and Hui, Binyuan and Zheng, Bo and Yu, Bowen and Gao, Chang and Huang, Chengen and Lv, Chenxu and others},
  journal={arXiv preprint arXiv:2505.09388},
  year={2025}
}

@article{team2024qwen2,
  title={Qwen2 technical report},
  author={Team, Qwen},
  journal={arXiv preprint arXiv:2407.10671},
  year={2024}
}

@article{bai2023qwen,
  title={Qwen technical report},
  author={Bai, Jinze and Bai, Shuai and Chu, Yunfei and Cui, Zeyu and Dang, Kai and Deng, Xiaodong and Fan, Yang and Ge, Wenbin and Han, Yu and Huang, Fei and others},
  journal={arXiv preprint arXiv:2309.16609},
  year={2023}
}

@inproceedings{googleNatural,
  title={Natural Questions: A Benchmark for Question Answering Research},
  author={Kwiatkowski, Tom and Palomaki, Jennimaria and Redfield, Olivia and Collins, Michael and Parikh, Ankur and Alberti, Chris and Epstein, Danielle and Polosukhin, Illia and Devlin, Jacob and Lee, Kenton and Toutanova, Kristina and Jones, Llion and Kelcey, Matthew and Chang, Ming-Wei and Dai, Andrew M. and Uszkoreit, Jakob and Le, Quoc and Petrov, Slav},
  booktitle={Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  year={2019}
}

@misc{wiki,
  author = {{Wikipedia contributors}},
  title = {Wikipedia, The Free Encyclopedia},
  year = {2023},
  howpublished = {\\url{https://www.wikipedia.org/}},
  note = {Accessed: 2025-08-19}
}

@inproceedings{IMDB,
  title={Learning Word Vectors for Sentiment Analysis},
  author={Maas, Andrew L. and Daly, Raymond E. and Pham, Peter T. and Huang, Dan and Ng, Andrew Y. and Potts, Christopher},
  booktitle={Proceedings of the 49th Annual Meeting of the Association for Computational Linguistics (ACL)},
  year={2011},
  pages={142--150}
}

@inproceedings{CNN,
  title={Teaching Machines to Read and Comprehend},
  author={Hermann, Karl Moritz and Kocisky, Tomas and Grefenstette, Edward and Espeholt, Lasse and Kay, Will and Suleyman, Mustafa and Blunsom, Phil},
  booktitle={Advances in Neural Information Processing Systems (NeurIPS)},
  year={2015},
  pages={1693--1701}
}


@article{jiang2024mixtral,
  title={Mixtral of experts},
  author={Jiang, Albert Q and Sablayrolles, Alexandre and Roux, Antoine},
  journal={arXiv preprint arXiv:2401.04088},
  year={2024}
}

@article{deepseek2024v2,
  title={DeepSeek-V2: A Strong, Economical, and Efficient Mixture-of-Experts Language Model},
  author={DeepSeek-AI},
  journal={arXiv preprint arXiv:2405.04434},
  year={2024}
}

@article{wang2025emoe,
  title={eMoE: Task-aware Memory Efficient Mixture-of-Experts-Based Model Inference},
  author={Wang, Xiaoming and Liu, Jie and Zhang, Hao},
  journal={arXiv preprint arXiv:2503.06823},
  year={2025}
}


@article{li2025speculative,
  title={Speculative MoE: Communication Efficient Parallel MoE Inference with Speculative Token and Expert Pre-scheduling},
  author={Li, Zhuoming and Wang, Xulong and Zhang, Yilong},
  journal={arXiv preprint arXiv:2503.04398},
  year={2025}
}


@article{zhang2025daop,
  title={DAOP: Data-Aware Offloading and Predictive Pre-Calculation for Efficient MoE Inference},
  author={Zhang, Hao and Li, Wei and Chen, Ming},
  journal={arXiv preprint arXiv:2501.10375},
  year={2025}
}

@article{chen2025crosslayer,
  title={Accurate Expert Predictions in MoE Inference via Cross-Layer Gate},
  author={Chen, Ming and Wang, Jian and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.12224},
  year={2025}
}

@misc{li2025experttokenresonancemoebidirectional,
      title={Expert-Token Resonance MoE: Bidirectional Routing with Efficiency Affinity-Driven Active Selection}, 
      author={Jing Li and Zhijie Sun and Dachao Lin and Xuan He and Binfan Zheng and Yi Lin and Rongqian Zhao and Xin Chen},
      year={2025},
      eprint={2406.00023},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.00023}, 
}

@misc{ma2025moegpsguidlinespredictionstrategy,
      title={MoE-GPS: Guidlines for Prediction Strategy for Dynamic Expert Duplication in MoE Load Balancing}, 
      author={Haiyue Ma and Zhixu Du and Yiran Chen},
      year={2025},
      eprint={2506.07366},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2506.07366}, 
}

@article{liu2024prediction,
  title={Prediction Is All MoE Needs: Expert Load Distribution Goes from Fluctuating to Stabilizing},
  author={Liu, Peng and Zhang, Hao and Chen, Ming},
  journal={arXiv preprint arXiv:2404.16914},
  year={2024}
}

@article{moeinfinity,
  title={Moe-infinity: Offloading-efficient moe model serving},
  author={Xue, Leyang and Fu, Yao and Lu, Zhan and Mai, Luo and Marina, Mahesh},
  journal={arXiv preprint arXiv:2401.14361},
  year={2024}
}

@inproceedings{fastermoe,
  title={Fastermoe: modeling and optimizing training of large-scale dynamic pre-trained models},
  author={He, Jiaao and Zhai, Jidong and Antunes, Tiago and Wang, Haojie and Luo, Fuwen and Shi, Shangfeng and Li, Qin},
  booktitle={Proceedings of the 27th ACM SIGPLAN Symposium on Principles and Practice of Parallel Programming},
  pages={120--134},
  year={2022}
}

@article{expertchoice,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@article{megablocks,
  title={Megablocks: Efficient sparse training with mixture-of-experts},
  author={Gale, Trevor and Narayanan, Deepak and Young, Cliff and Zaharia, Matei},
  journal={Proceedings of Machine Learning and Systems},
  volume={5},
  pages={288--304},
  year={2023}
}

@inproceedings{pytorch,
author = {Ansel, Jason and Yang, Edward and He, Horace and Gimelshein, Natalia and Jain, Animesh and Voznesensky, Michael and Bao, Bin and Bell, Peter and Berard, David and Burovski, Evgeni and Chauhan, Geeta and Chourdia, Anjali and Constable, Will and Desmaison, Alban and DeVito, Zachary and Ellison, Elias and Feng, Will and Gong, Jiong and Gschwind, Michael and Hirsh, Brian and Huang, Sherlock and Kalambarkar, Kshiteej and Kirsch, Laurent and Lazos, Michael and Lezcano, Mario and Liang, Yanbo and Liang, Jason and Lu, Yinghai and Luk, C. K. and Maher, Bert and Pan, Yunjie and Puhrsch, Christian and Reso, Matthias and Saroufim, Mark and Siraichi, Marcos Yukio and Suk, Helen and Zhang, Shunting and Suo, Michael and Tillet, Phil and Zhao, Xu and Wang, Eikan and Zhou, Keren and Zou, Richard and Wang, Xiaodong and Mathews, Ajit and Wen, William and Chanan, Gregory and Wu, Peng and Chintala, Soumith},
title = {PyTorch 2: Faster Machine Learning Through Dynamic Python Bytecode Transformation and Graph Compilation},
year = {2024},
isbn = {9798400703850},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3620665.3640366},
doi = {10.1145/3620665.3640366},
booktitle = {Proceedings of the 29th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
pages = {929â€“947},
numpages = {19},
location = {La Jolla, CA, USA},
series = {ASPLOS '24}
}

@article{hftf,
  author       = {Thomas Wolf and
                  Lysandre Debut and
                  Victor Sanh and
                  Julien Chaumond and
                  Clement Delangue and
                  Anthony Moi and
                  Pierric Cistac and
                  Tim Rault and
                  R{\'{e}}mi Louf and
                  Morgan Funtowicz and
                  Jamie Brew},
  title        = {HuggingFace's Transformers: State-of-the-art Natural Language Processing},
  journal      = {CoRR},
  volume       = {abs/1910.03771},
  year         = {2019},
  url          = {http://arxiv.org/abs/1910.03771},
  eprinttype    = {arXiv},
  eprint       = {1910.03771},
  timestamp    = {Tue, 02 Jun 2020 12:49:01 +0200},
  biburl       = {https://dblp.org/rec/journals/corr/abs-1910-03771.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}


@misc{lo2025closerlookmixtureofexpertslarge,
      title={A Closer Look into Mixture-of-Experts in Large Language Models}, 
      author={Ka Man Lo and Zeyu Huang and Zihan Qiu and Zili Wang and Jie Fu},
      year={2025},
      eprint={2406.18219},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2406.18219}, 
}

@article{oldfield2024multilinear,
  title={Multilinear Mixture of Experts: Scalable Expert Specialization through Factorization},
  author={Oldfield, James and Panagakis, Yannis and Nicolaou, Mihalis A},
  journal={arXiv preprint arXiv:2402.12550},
  year={2024}
}

@article{zhang2024harder,
  title={Harder Tasks Need More Experts: Dynamic Routing in MoE Models},
  author={Zhang, Hao and Li, Wei and Chen, Ming},
  journal={arXiv preprint arXiv:2403.07652},
  year={2024}
}

@article{chen2024layerwise,
  title={Layerwise Recurrent Router for Mixture-of-Experts},
  author={Chen, Ming and Wang, Jian and Zhang, Hao},
  journal={arXiv preprint arXiv:2408.06793},
  year={2024}
}


@article{dai2024deepseekmoe,
  title={DeepSeekMoE: Towards Ultimate Expert Specialization in Mixture-of-Experts Language Models},
  author={Dai, Damai and Deng, Chengqi and Zhao, Cheng},
  journal={arXiv preprint arXiv:2401.06066},
  year={2024}
}


@article{zhang2024mone,
  title={MoNE: Replacing Redundant Experts with Lightweight Novices for Structured Pruning of MoE},
  author={Zhang, Hao and Li, Wei and Chen, Ming},
  journal={arXiv preprint arXiv:2507.00390},
  year={2024}
}



@article{chen2025probing,
  title={Probing Semantic Routing in Large Mixture-of-Expert Models},
  author={Chen, Ming and Wang, Jian and Zhang, Hao},
  journal={arXiv preprint arXiv:2502.10928},
  year={2025}
}



@article{wang2024decorrelated,
  title={Enhancing CTR Prediction with De-correlated Expert Networks},
  author={Wang, Jian and Chen, Ming and Zhang, Hao},
  journal={arXiv preprint arXiv:2505.17925},
  year={2024}
}

@inproceedings{cao2025moe,
  title={Moe-lightning: High-throughput moe inference on memory-constrained gpus},
  author={Cao, Shiyi and Liu, Shu and Griggs, Tyler and Schafhalter, Peter and Liu, Xiaoxuan and Sheng, Ying and Gonzalez, Joseph E and Zaharia, Matei and Stoica, Ion},
  booktitle={Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1},
  pages={715--730},
  year={2025}
}


@incollection{gupta2024dbrx,
  title={Dbrx: Creating an llm from scratch using databricks},
  author={Gupta, Nikhil and Yip, Jason},
  booktitle={Databricks Data Intelligence Platform: Unlocking the GenAI Revolution},
  pages={311--330},
  year={2024},
  publisher={Springer}
}

@inproceedings{dernbach2024glam,
  title={Glam: Fine-tuning large language models for domain knowledge graph alignment via neighborhood partitioning and generative subgraph encoding},
  author={Dernbach, Stefan and Agarwal, Khushbu and Zuniga, Alejandro and Henry, Michael and Choudhury, Sutanay},
  booktitle={Proceedings of the AAAI Symposium Series},
  volume={3},
  number={1},
  pages={82--89},
  year={2024}
}

@article{costa2022no,
  title={No language left behind: Scaling human-centered machine translation},
  author={Costa-Juss{\`a}, Marta R and Cross, James and {\c{C}}elebi, Onur and Elbayad, Maha and Heafield, Kenneth and Heffernan, Kevin and Kalbassi, Elahe and Lam, Janice and Licht, Daniel and Maillard, Jean and others},
  journal={arXiv preprint arXiv:2207.04672},
  year={2022}
}

@inproceedings{flame,
author = {Lin, Xuanda and Tian, Huinan and Xue, Wenxiao and Ma, Lanqi and Cao, Jialin and Zhang, Manting and Yu, Jun and Wang, Kun},
title = {FLAME: Fully Leveraging MoE Sparsity for Transformer on FPGA},
year = {2024},
isbn = {9798400706011},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3649329.3656507},
doi = {10.1145/3649329.3656507},
booktitle = {Proceedings of the 61st ACM/IEEE Design Automation Conference},
articleno = {248},
numpages = {6},
location = {San Francisco, CA, USA},
series = {DAC '24}
}

@article{zadouri2023pushing,
  title={Pushing mixture of experts to the limit: Extremely parameter efficient moe for instruction tuning},
  author={Zadouri, Ted and {\"U}st{\"u}n, Ahmet and Ahmadian, Arash and Ermi{\c{s}}, Beyza and Locatelli, Acyr and Hooker, Sara},
  journal={arXiv preprint arXiv:2309.05444},
  year={2023}
}

@article{doucet2025harmoeny,
  title={HarMoEny: Efficient Multi-GPU Inference of MoE Models},
  author={Doucet, Zachary and Sharma, Rishi and de Vos, Martijn and Pires, Rafael and Kermarrec, Anne-Marie and Balmau, Oana},
  journal={arXiv preprint arXiv:2506.12417},
  year={2025}
}

@misc{go2025moetuneroptimizedmixtureexpert,
      title={MoETuner: Optimized Mixture of Expert Serving with Balanced Expert Placement and Token Routing}, 
      author={Seokjin Go and Divya Mahajan},
      year={2025},
      eprint={2502.06643},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2502.06643}, 
}

@inproceedings{zhou2025floe,
title={FloE: On-the-Fly MoE Inference on Memory-constrained {GPU}},
author={Yuxin Zhou and zheng li and Jun Zhang and Jue WANG and Yiping Wang and Zhongle Xie and Ke Chen and Lidan Shou},
booktitle={Forty-second International Conference on Machine Learning},
year={2025},
url={https://openreview.net/forum?id=i5aHAkkhJH}
}

@misc{artetxe2022efficientlargescalelanguage,
      title={Efficient Large Scale Language Modeling with Mixtures of Experts}, 
      author={Mikel Artetxe and Shruti Bhosale and Naman Goyal and Todor Mihaylov and Myle Ott and Sam Shleifer and Xi Victoria Lin and Jingfei Du and Srinivasan Iyer and Ramakanth Pasunuru and Giri Anantharaman and Xian Li and Shuohui Chen and Halil Akin and Mandeep Baines and Louis Martin and Xing Zhou and Punit Singh Koura and Brian O'Horo and Jeff Wang and Luke Zettlemoyer and Mona Diab and Zornitsa Kozareva and Ves Stoyanov},
      year={2022},
      eprint={2112.10684},
      archivePrefix={arXiv},
      primaryClass={cs.CL},
      url={https://arxiv.org/abs/2112.10684}, 
}

@misc{kamahori2025fiddlercpugpuorchestrationfast,
      title={Fiddler: CPU-GPU Orchestration for Fast Inference of Mixture-of-Experts Models}, 
      author={Keisuke Kamahori and Tian Tang and Yile Gu and Kan Zhu and Baris Kasikci},
      year={2025},
      eprint={2402.07033},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2402.07033}, 
}


@article{zhong2025hybrimoe,
  title={HybriMoE: Hybrid CPU-GPU Scheduling and Cache Management for Efficient MoE Inference},
  author={Zhong, Shuzhang and Sun, Yanfan and Liang, Ling and Wang, Runsheng and Huang, Ru and Li, Meng},
  journal={arXiv preprint arXiv:2504.05897},
  year={2025}
}

@article{zhou2022mixture,
  title={Mixture-of-experts with expert choice routing},
  author={Zhou, Yanqi and Lei, Tao and Liu, Hanxiao and Du, Nan and Huang, Yanping and Zhao, Vincent and Dai, Andrew M and Le, Quoc V and Laudon, James and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={7103--7114},
  year={2022}
}

@inproceedings{bambhaniya2024moeeras,
title={MoE-{ERAS}: Expert Residency Aware Selection},
author={Abhimanyu Rajeshkumar Bambhaniya and Sashankh Chengavalli Kumar and Tushar Krishna},
booktitle={Machine Learning for Computer Architecture and Systems 2024},
year={2024},
url={https://openreview.net/forum?id=o43eHjPEMO}
}

@INPROCEEDINGS{prophet2,
  author={Wang, Wei and Lai, Zhiquan and Li, Shengwei and Liu, Weijie and Ge, Keshi and Liu, Yujie and Shen, Ao and Li, Dongsheng},
  booktitle={2023 IEEE International Conference on Cluster Computing (CLUSTER)}, 
  title={Prophet: Fine-grained Load Balancing for Parallel Training of Large-scale MoE Models}, 
  year={2023},
  volume={},
  number={},
  pages={82-94},
  keywords={Training;Schedules;Systematics;Computational modeling;Predictive models;Load management;Throughput;mixture of experts;distributed training},
  doi={10.1109/CLUSTER52292.2023.00015}}

@article{huang2023towards,
  title={Towards moe deployment: Mitigating inefficiencies in mixture-of-expert (moe) inference},
  author={Huang, Haiyang and Ardalani, Newsha and Sun, Anna and Ke, Liu and Lee, Hsien-Hsin S and Sridhar, Anjali and Bhosale, Shruti and Wu, Carole-Jean and Lee, Benjamin},
  journal={arXiv preprint arXiv:2303.06182},
  year={2023}
}

@article{wang2024auxiliary,
  title={Auxiliary-loss-free load balancing strategy for mixture-of-experts},
  author={Wang, Lean and Gao, Huazuo and Zhao, Chenggang and Sun, Xu and Dai, Damai},
  journal={arXiv preprint arXiv:2408.15664},
  year={2024}
}

@article{li2025comoe,
  title={CoMoE: Collaborative Optimization of Expert Aggregation and Offloading for MoE-based LLMs at Edge},
  author={Li, Muqing and Li, Ning and Yuan, Xin and Xu, Wenchao and Chen, Quan and Guo, Song and Zhang, Haijun},
  journal={arXiv preprint arXiv:2508.09208},
  year={2025}
}

@inproceedings{ghorbani2022scaling,
title={Scaling Laws for Neural Machine Translation},
author={Behrooz Ghorbani and Orhan Firat and Markus Freitag and Ankur Bapna and Maxim Krikun and Xavier Garcia and Ciprian Chelba and Colin Cherry},
booktitle={International Conference on Learning Representations},
year={2022},
url={https://openreview.net/forum?id=hR_SMu8cxCV}
}

@inproceedings{suo2025coserve,
  title={CoServe: Efficient Collaboration-of-Experts (CoE) Model Inference with Limited Memory},
  author={Suo, Jiashun and Liao, Xiaojian and Xiao, Limin and Ruan, Li and Wang, Jinquan and Su, Xiao and Huo, Zhisheng},
  booktitle={Proceedings of the 30th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 2},
  pages={178--191},
  year={2025}
}

@article{kong2025serving,
  title={Serving MoE models on resource-constrained edge devices via dynamic expert swapping},
  author={Kong, Rui and Li, Yuanchun and Wang, Weijun and Kong, Linghe and Liu, Yunxin},
  journal={IEEE Transactions on Computers},
  year={2025},
  publisher={IEEE}
}

@article{liu2024fantastic,
  title={Fantastic semantics and where to find them: Investigating which layers of generative llms reflect lexical semantics},
  author={Liu, Zhu and Kong, Cunliang and Liu, Ying and Sun, Maosong},
  journal={arXiv preprint arXiv:2403.01509},
  year={2024}
}

@inbook{adapmoe,
author = {Zhong, Shuzhang and Liang, Ling and Wang, Yuan and Wang, Runsheng and Huang, Ru and Li, Meng},
title = {AdapMoE: Adaptive Sensitivity-based Expert Gating and Management for Efficient MoE Inference},
year = {2025},
isbn = {9798400710773},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3676536.3676741},
booktitle = {Proceedings of the 43rd IEEE/ACM International Conference on Computer-Aided Design},
articleno = {51},
numpages = {9}
}

@inproceedings{rajbhandari2022deepspeed,
  title={Deepspeed-moe: Advancing mixture-of-experts inference and training to power next-generation ai scale},
  author={Rajbhandari, Samyam and Li, Conglong and Yao, Zhewei and Zhang, Minjia and Aminabadi, Reza Yazdani and Awan, Ammar Ahmad and Rasley, Jeff and He, Yuxiong},
  booktitle={International conference on machine learning},
  pages={18332--18346},
  year={2022},
  organization={PMLR}
}

@inproceedings{lrb,
  author    = {Song, Zhenyu and Berger, Daniel S. and Li, Kai and Lloyd, Wyatt},
  title     = {Learning Relaxed Belady for Content Distribution Network Caching},
  booktitle = {17th USENIX Symposium on Networked Systems Design and Implementation (NSDI)},
  year      = {2020},
  pages     = {529--544},
  url       = {https://www.usenix.org/conference/nsdi20/presentation/song},
}

@inproceedings{ktransformers,
  author    = {Chen, Azure and Yang, Haotian and others},
  title     = {{KTransformers}: Unleashing the Full Potential of {CPU/GPU} Hybrid Inference for {MoE} Models},
  booktitle = {ACM SIGOPS 31st Symposium on Operating Systems Principles (SOSP)},
  year      = {2025},
  doi       = {10.1145/3731569.3764843},
}

@article{gsm8k,
  title={Training Verifiers to Solve Math Word Problems},
  author={Cobbe, Karl and Kosaraju, Vineet and Bavarian, Mohammad and Chen, Mark and Jun, Heewoo and Kaiser, Lukasz and Plappert, Matthias and Tworek, Jerry and Hilton, Jacob and Nakano, Reiichiro and Hesse, Christopher and Schulman, John},
  journal={arXiv preprint arXiv:2110.14168},
  year={2021}
}

@article{humaneval,
  title={Evaluating Large Language Models Trained on Code},
  author={Chen, Mark and Tworek, Jerry and Jun, Heewoo and Yuan, Qiming and Pinto, Henrique Ponde de Oliveira and Kaplan, Jared and Edwards, Harri and Burda, Yuri and Joseph, Nicholas and Brockman, Greg and others},
  journal={arXiv preprint arXiv:2107.03374},
  year={2021}
}

@article{wikitext,
  title={Pointer Sentinel Mixture Models},
  author={Merity, Stephen and Xiong, Caiming and Bradbury, James and Socher, Richard},
  journal={arXiv preprint arXiv:1609.07843},
  year={2016}
}

@misc{alpaca,
  title={Stanford Alpaca: An Instruction-following LLaMA Model},
  author={Taori, Rohan and Gulrajani, Ishaan and Zhang, Tianyi and Dubois, Yann and Li, Xuechen and Guestrin, Carlos and Liang, Percy and Hashimoto, Tatsunori B.},
  year={2023},
  howpublished={\url{https://github.com/tatsu-lab/stanford_alpaca}}
}

@inproceedings{squad,
  title={SQuAD: 100,000+ Questions for Machine Comprehension of Text},
  author={Rajpurkar, Pranav and Zhang, Jian and Lopyrev, Konstantin and Liang, Percy},
  booktitle={Proceedings of the 2016 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={2383--2392},
  year={2016}
}

