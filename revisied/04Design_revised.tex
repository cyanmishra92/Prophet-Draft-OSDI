%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
\section{Design}
\label{sec:design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% \begin{figure}[h]
% \centering
% \includegraphics[width=\linewidth]{figs/\designarchitecture.pdf}
% \caption{\design overview: neural expert prefetching exploits temporal correlations, spatial clustering, batch-aware deduplication, and cross-architecture generalization.}
% \label{fig:\design_overview}
% \end{figure}

\todo{There are several repetitions in this sections from section 2. Maybe we can redice them?}

The structural patterns identified in our motivation analysis, power-law expert popularity, temporal correlations, and spatial clustering directly inform the design of \design. Rather than treating expert loading as a reactive memory management problem, we transform it into a predictive optimization challenge that leverages learnable routing patterns. This fundamental shift from reactive to proactive expert management distinguishes our approach from prior work such as ExpertFlow~\cite{expertflow} and Pre-gated MoE~\cite{pregated}, which either rely on simpler heuristics or require architectural modifications. This section presents our comprehensive system design, encompassing the neural prediction architecture (\Cref{subsec:neural-predictor}), batch-aware optimization algorithms (\Cref{subsec:batch-optimization}), hierarchical caching system (\Cref{subsec:caching}), and end-to-end integration (\Cref{subsec:integration}).

\subsection{Design Principles and Architecture Overview}
\label{subsec:design-principles}

Our design philosophy centers on three fundamental principles derived from the empirical observations in \Cref{sec:background}. \textbf{First}, we exploit temporal dependencies revealed by our mutual information analysis, which demonstrated 0.6217 bits of predictable information in expert routing sequences, representing 54\% uncertainty reduction compared to random selection. This temporal structure enables cross-layer routing pattern learning through a neural sequence model, contrasting with ProMoE's~\cite{promoe} stride-based prefetching that assumes fixed access patterns. \textbf{Second}, we leverage spatial clustering evident in the 13 distinct semantic expert clusters we identified, with their progressive layer-wise specialization ($R^2 = 0.633$ entropy decrease) enabling cluster-aware prediction strategies that exploit expert co-occurrence patterns. \textbf{Third}, we utilize batch-level patterns characterized by power-law scaling of unique experts ($\mathbb{E}[U(B)] \sim B^{0.932}$), creating substantial deduplication opportunities with up to 87.6\% memory savings in batch processing scenarios, an optimization dimension unexplored by single-token focused approaches like ProMoE. \todo{correlate the I/O, powerlaw, de-duplication, MI and clustering here? 1. Power-law, MI and Clustering tell us that there are patterns and we can exploit/learn the pattern using a learnable network 2. Powerlaw, and batched scaling tells that we can leverage deduplication, 3. power-law/expert popularity and clutering kind of suggests that we can cache things and it will be helpful.}

\todo{We'll probably remove here?} \Cref{fig:system-architecture} illustrates \design's architecture, which comprises four interconnected components operating in concert. The neural expert predictor processes routing history to forecast future expert selections with high accuracy, achieving $43\times$ improvement over random selection. The batch deduplication engine identifies and eliminates redundant expert loads across batch items, exploiting the sublinear scaling property we discovered. The hierarchical cache system implements prediction-driven prefetching across three tiers optimized for different confidence levels. Finally, the hardware-aware scheduler optimizes memory transfers based on available bandwidth and device characteristics. The system operates transparently at inference time, requiring no modifications to pre-trained MoE models, a critical advantage over approaches like Pre-gated MoE that necessitate architectural changes.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/SpecMoEarchitecture.pdf}
\caption{\design architecture: routing traces flow through neural prediction, batch optimization, and hierarchical caching to enable proactive prefetching.}
\label{fig:system-architecture}
\end{figure}

\subsection{Neural Expert Prediction}
\label{subsec:neural-predictor}

\subsubsection{Problem Formalization}

We formalize expert prediction as a sequence modeling task where the objective is to predict future expert selections based on historical routing patterns and current hidden states. Given a token at position $t$ in layer $\ell$, we define the input as $\mathcal{H} = \{E^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)}, H^{(\ell)}\}$ and the output as $\hat{E}^{(\ell+h)} = \text{Predictor}(\mathcal{H}; \theta)$, where $E^{(\ell-c+1:\ell)}$ represents the expert routing history over the past $c$ layers, $L^{(\ell-c+1:\ell)}$ encodes layer positions, $H^{(\ell)}$ denotes the current hidden states (post-attention activations that serve as router inputs), and $h$ is the prediction horizon. The predictor must maximize accuracy while maintaining minimal computational overhead relative to expert execution time.

\subsubsection{Dense Transformer Architecture}

Our predictor employs a dense transformer architecture, critically not an MoE model, to learn routing patterns. This design choice avoids the recursive complexity of using MoE to predict MoE routing while enabling efficient pattern learning. The architecture begins with an input embedding layer where expert identifiers are mapped to dense representations through a learned embedding matrix $\mathbf{W}_E \in \mathbb{R}^{N_E \times d}$, where $N_E$ denotes the expert vocabulary size (128 for Switch Transformer, 64 for Qwen MoE) and $d = 320$ is the embedding dimension. This dimensionality balances representational capacity with computational efficiency, ensuring the predictor remains lightweight relative to the main model.

We employ a dual positional encoding scheme that captures both layer-wise progression and token-wise sequence position through $\text{PE}_{\text{combined}} = \text{PE}_{\text{layer}}(l) + \text{PE}_{\text{token}}(t)$. The layer position embedding $\text{PE}_{\text{layer}}$ uses learned embeddings for each MoE layer, while $\text{PE}_{\text{token}}$ employs sinusoidal encoding to generalize across sequence lengths. This dual encoding enables the model to understand both the hierarchical structure of transformer layers and the sequential nature of token processing.

The model processes context sequences through multi-head self-attention layers that learn dependencies between expert selections across different layers. 
% The attention mechanism computes:
% \begin{align}
% \text{Attention}(Q, K, V) &= \text{softmax}\left(\frac{QK^T}{\sqrt{d_k}}\right)V \label{eq:attention} \\
% \text{MultiHead}(X) &= \text{Concat}(\text{head}_1, \ldots, \text{head}_h)W^O \label{eq:multihead}
% \end{align}
Each attention head 
%$\text{head}_i = \text{Attention}(XW_i^Q, XW_i^K, XW_i^V)$ 
captures different aspects of the routing dependencies. We use $h = 10$ attention heads with model dimension $d_{\text{model}} = 320$ and feed-forward dimension $d_{ff} = 1280$, parameters selected through extensive ablation studies to optimize the accuracy-efficiency tradeoff.

\subsubsection{Cross-Layer Fusion Mechanism}

A key innovation in our architecture is the cross-layer attention mechanism that explicitly models dependencies between routing decisions at different transformer layers. Unlike standard self-attention that operates within a single sequence, cross-layer attention enables the model to learn how expert selections at layer $\ell$ influence routing at layer $\ell + h$ through:
\begin{equation}
\text{CrossLayerAttn}(\mathbf{H}_{\ell-c+1:\ell}) = \sum_{i=\ell-c+1}^{\ell} \alpha_i \cdot \mathbf{H}_i \label{eq:crosslayer}
\end{equation}
where attention weights $\alpha_i$ are computed based on the relevance of layer $i$'s routing patterns to the prediction task. This captures the hierarchical nature of transformer processing, where earlier layers focus on syntactic features while deeper layers capture semantic patterns, a phenomenon we empirically validated through our clustering analysis.

%\subsubsection{Prediction Head and Confidence Estimation}

The final prediction head transforms the attended representations into expert selection probabilities and confidence scores through softmax and sigmoid functions.
% :
% \begin{align}
% \mathbf{p}_{\text{expert}} &= \text{softmax}(\mathbf{W}_{\text{pred}} \cdot \mathbf{h}_{\text{final}} + \mathbf{b}_{\text{pred}}) \label{eq:expert_pred} \\
% c_{\text{conf}} &= \sigma(\mathbf{W}_{\text{conf}} \cdot \mathbf{h}_{\text{final}} + b_{\text{conf}}) \label{eq:confidence}
% \end{align}
% where $\mathbf{p}_{\text{expert}} \in \mathbb{R}^{N_E}$ represents the probability distribution over experts and $c_{\text{conf}} \in [0, 1]$ indicates prediction confidence. 
The confidence score enables dynamic prefetching strategies that adjust aggressiveness based on prediction certainty, a crucial feature for managing limited cache resources effectively.

\subsubsection{Training Procedure and Data Collection}

We collect expert routing traces from pre-trained MoE models processing diverse datasets including Natural Questions for factual QA, IMDB for sentiment analysis, and CNN/DailyMail for summarization. This diversity ensures the predictor generalizes across different task types and routing patterns, addressing a limitation of prior work that often trains on task-specific patterns. For each forward pass, we record expert selections $E^{(\ell)}$ at each layer $\ell$, hidden states $H^{(\ell)}$ serving as router inputs, and token positions with attention masks.

The model is trained using a composite loss function that combines multiple objectives:
\begin{equation}
\mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_{\text{expert}} + \lambda_2 \mathcal{L}_{\text{ranking}} + \lambda_3 \mathcal{L}_{\text{conf}} \label{eq:total_loss}
\end{equation}
where the primary cross-entropy loss $\mathcal{L}_{\text{expert}}$ optimizes expert classification accuracy, the auxiliary ranking loss $\mathcal{L}_{\text{ranking}}$ encourages correct relative ordering of expert probabilities, and the confidence calibration loss $\mathcal{L}_{\text{conf}}$ uses the Brier score to ensure prediction confidence correlates with actual accuracy. We set $\lambda_1 = 1.0$, $\lambda_2 = 0.3$, and $\lambda_3 = 0.1$ based on validation performance across multiple datasets.

Through systematic evaluation, we determine optimal parameters of $c = 3$ for the context window and $h = 2$ for the prediction horizon. This configuration balances prediction accuracy with the practical constraints of prefetching latency. Longer horizons provide more prefetching time but suffer from decreased accuracy due to accumulated uncertainty. As shown in \Cref{tab:predictor-performance}, our predictor achieves 33.86\% top-1 accuracy for Switch Transformer and 29.84\% for Qwen MoE, representing $43\times$ and $19\times$ improvements over random baselines respectively, while maintaining minimal computational overhead of only 0.15-0.18ms inference latency.

\begin{table}[t]
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Configuration} & \textbf{Switch-T} & \textbf{Qwen} \\
\midrule
Model Parameters & 8.4M & 8.4M \\
Context Window ($c$) & 3 layers & 3 layers \\
Prediction Horizon ($h$) & 2 layers & 2 layers \\
Training Time & 3.5 hours & 4.2 hours \\
\midrule
Top-1 Accuracy & 33.86\% & 29.84\% \\
Top-5 Accuracy & 67.42\% & 71.35\% \\
Random Baseline & 0.78\% & 1.56\% \\
Improvement Factor & $43\times$ & $19\times$ \\
\midrule
Inference Latency & 0.15ms & 0.18ms \\
Memory Overhead & 0.32\% & 0.06\% \\
\bottomrule
\end{tabular}
\caption{Neural predictor performance across architectures.}
\label{tab:predictor-performance}
\end{table}

\subsection{Batch-Aware Optimization}
\label{subsec:batch-optimization}

\subsubsection{Expert Deduplication Algorithm}

The power-law distribution of expert popularity creates significant opportunities for batch-level optimization that prior single-token approaches like ProMoE cannot exploit. When processing multiple tokens simultaneously, different batch items often request overlapping sets of experts, a pattern we quantified through extensive empirical analysis. Our deduplication algorithm, presented in \Cref{alg:deduplication}, exploits this pattern to reduce memory transfers by identifying unique experts across all batch items and computing an optimal loading schedule that prioritizes high-frequency experts.

\begin{algorithm}[t]
\caption{Batch-Aware Expert Deduplication}
\label{alg:deduplication}
\begin{algorithmic}[1]
\Require Batch $\mathcal{B} = \{R_1, \ldots, R_B\}$, $R_i$ = experts for item $i$
\Ensure Unique set $\mathcal{U}$, schedule $\mathcal{S}$
\State $\mathcal{U} \gets \emptyset$; $\mathcal{F} \gets \{\}$ \Comment{Init unique set, freq map}
\For{each request $R_i \in \mathcal{B}$}
    \For{each expert $e \in R_i$}
        \State $\mathcal{U} \gets \mathcal{U} \cup \{e\}$
        \State $\mathcal{F}[e] \gets \mathcal{F}[e] + 1$
    \EndFor
\EndFor
\State Sort $\mathcal{U}$ by frequency $\mathcal{F}$ descending
\State $\mathcal{S} \gets$ ComputeOptimalSchedule($\mathcal{U}$, $\mathcal{F}$)
\State \Return $\mathcal{U}$, $\mathcal{S}$
\end{algorithmic}
\end{algorithm}

The algorithm reduces the total number of expert loads from $O(B \cdot k)$ to $O(B^{\alpha} \cdot k)$ where $\alpha = 0.932 < 1$, providing sublinear scaling that becomes increasingly beneficial at larger batch sizes. This theoretical guarantee, derived from our empirical validation of the power-law scaling property, ensures predictable performance improvements as batch sizes increase.

\subsubsection{Theoretical Foundation and Guarantees}

The effectiveness of deduplication stems from the power-law scaling property we empirically validated: $\mathbb{E}[U(B)] = C \cdot B^{\alpha}$ where $\alpha = 0.932$ and $C = 1.055$. This sublinear growth means that doubling the batch size increases unique experts by only 91\%, not 100\%, creating substantial optimization opportunities. The memory savings ratio can be expressed as:
\begin{equation}
\text{Savings}(B) = 1 - \frac{\mathbb{E}[U(B)]}{B \cdot k} = 1 - \frac{C \cdot B^{\alpha - 1}}{k} \label{eq:savings}
\end{equation}

For typical configurations with $B = 32$ and $k = 8$, this yields memory savings of 64.9\% and bandwidth reduction of 26.3\%, as validated in our experiments.

\subsubsection{Hardware-Aware Transfer Scheduling}

Beyond deduplication, we implement sophisticated transfer mechanisms that pipeline expert loading with computation. Rather than loading all experts before computation begins, the approach taken by reactive systems, we divide transfers into chunks that can be overlapped with ongoing computation, achieving:
\begin{equation}
T_{\text{total}} = \max(T_{\text{compute}}, T_{\text{transfer}_1}) + T_{\text{transfer}_{\text{rem}}} \label{eq:pipeline}
\end{equation}

This pipelining reduces effective loading latency by hiding transfer costs behind computation, a technique particularly effective when combined with our predictive prefetching.
The scheduler incorporates hardware-specific bandwidth constraints to prevent memory bottlenecks through the optimization:
\begin{equation}
\text{Schedule}(\mathcal{U}) = \argmax_{\mathcal{S}} \sum_{e \in \mathcal{S}} \text{utility}(e) \label{eq:schedule_opt}
\end{equation}
subject to $\sum_{e \in \mathcal{S}} \text{size}(e) \leq BW_{\text{available}} \times T_{\text{window}}$, where utility combines prediction confidence, expert frequency, and recency. This ensures efficient bandwidth utilization while prioritizing critical experts, adapting to diverse hardware configurations from edge devices to data center GPUs.

Using prediction confidence scores, we initiate transfers for high-confidence predictions before routing decisions are finalized. The preemptive set at time $t$ is defined as:
\begin{align}
\text{Preemptive}_t &= \{e : p_{\text{expert}}(e) > \theta_{\text{conf}}\} \label{eq:preemptive} \\
\text{Schedule}_t &= \text{Preemptive}_t \cup \text{OnDemand}_t \label{eq:combined_schedule}
\end{align}

The confidence threshold $\theta_{\text{conf}}$ is dynamically adjusted based on available bandwidth and cache capacity, ensuring the system adapts to varying hardware capabilities.

\begin{figure}[t]
\centering
\caption{Memory savings from batch-aware deduplication. Sublinear scaling ($B^{0.932}$) yields 87.6\% reduction at batch size 64.}
\label{fig:batch-deduplication}
\end{figure}

\subsection{Hierarchical Caching System}
\label{subsec:caching}

\design employs a hierarchical cache design that balances access latency with capacity constraints, structured across three tiers with carefully optimized allocations. The L1 cache, consuming 40\% of total capacity, stores highest-priority experts with 0.1ms access latency for immediate availability. The L2 cache, also allocated 40\% capacity, maintains medium-priority experts with 0.5ms access latency, serving as a buffer between hot and cold storage. The L3 cache, utilizing the remaining 20\% capacity, holds lower-priority experts with 2.0ms access latency, providing a last line of defense before expensive memory transfers. This allocation strategy, determined through extensive empirical evaluation across diverse workloads, optimizes the latency-capacity tradeoff while maintaining fairness through our iso-cache framework.

\subsubsection{Prediction-Driven Prefetching Policy}

The prefetching policy, detailed in \Cref{alg:prefetching}, maps prediction confidence to cache placement decisions with sophisticated tiering logic. High-confidence predictions exceeding the threshold $\theta_1 = 0.8$ are placed in L1 cache for immediate access, ensuring that experts most likely to be used incur minimal loading latency. Medium-confidence predictions in the range $0.5 < c \leq 0.8$ are directed to L2 cache, balancing accessibility with resource conservation. Remaining predictions are placed in L3 cache when capacity permits, providing speculative coverage for lower-confidence predictions. This tiered approach ensures that prefetching bandwidth is allocated efficiently based on prediction certainty, contrasting with the uniform caching strategies employed by systems like FateMoE~\cite{fatemoe}.

\begin{algorithm}[t]
\caption{Confidence-Based Hierarchical Prefetching}
\label{alg:prefetching}
\begin{algorithmic}[1]
\Require Predicted experts $\mathcal{P}$ with confidence $\mathcal{C}$
\Require Cache state $\mathcal{H} = \{L_1, L_2, L_3\}$
\State Sort $\mathcal{P}$ by confidence $\mathcal{C}$ descending
\State $\theta_1 \gets 0.8$, $\theta_2 \gets 0.5$ \Comment{Thresholds}
\For{each expert $e \in \mathcal{P}$}
    \If{$e \in L_1 \cup L_2 \cup L_3$}
        \State \textbf{continue} \Comment{Already cached}
    \EndIf
    \If{$\mathcal{C}[e] > \theta_1$ \textbf{and} $|L_1| < \text{cap}_{L_1}$}
        \State Prefetch $e$ to $L_1$
    \ElsIf{$\mathcal{C}[e] > \theta_2$ \textbf{and} $|L_2| < \text{cap}_{L_2}$}
        \State Prefetch $e$ to $L_2$
    \ElsIf{$|L_3| < \text{cap}_{L_3}$}
        \State Prefetch $e$ to $L_3$
    \EndIf
\EndFor
\end{algorithmic}
\end{algorithm}

\subsubsection{Cache Replacement and Promotion}

The cache implements sophisticated replacement and promotion mechanisms that adapt to access patterns over time. Within each tier, least recently used experts are evicted when capacity is reached through LRU.
% \begin{equation}
% \text{Evict}_{L_i} = \argmin_{e \in L_i} \text{LastAccess}(e) \label{eq:eviction}
% \end{equation}
Experts accessed from lower cache levels are promoted based on access frequency according to:
\begin{equation}
\text{Promote}(e, L_i \to L_{i-1}) \text{ if } \text{AccessCount}(e) > \theta_{\text{promote}} \label{eq:promotion}
\end{equation}

This dynamic promotion ensures that frequently accessed experts migrate to faster cache tiers over time, enabling the cache hierarchy to automatically adapt to workload-specific access patterns without manual tuning.

\subsubsection{Iso-Cache Constraint Enforcement}

To ensure fair evaluation against prior work, all strategies operate under identical cache constraints defined by:
\begin{align}
\text{TotalCache} &= L_1 + L_2 + L_3 = C_{\text{iso}} \label{eq:total_cache} \\
\text{ExpertCapacity} &= \lfloor C_{\text{iso}} / \text{ExpertSize} \rfloor \label{eq:expert_capacity}
\end{align}

where $C_{\text{iso}}$ is the iso-cache constraint (e.g., 100MB) applied uniformly across all strategies. This constraint ensures that performance improvements derive from intelligent prefetching rather than increased cache capacity, enabling fair comparison with systems like ExpertFlow's PLEC strategy. As shown in \Cref{tab:cache-config}, our hierarchical approach achieves 99.4\% overall hit rate under these constraints, demonstrating the effectiveness of confidence-based tiering.

\begin{table}[t]
\centering
\begin{tabular}{lccc}
\toprule
\textbf{Level} & \textbf{Capacity} & \textbf{Latency} & \textbf{Hit Rate} \\
\midrule
L1 (Hot) & 40\% & 0.1ms & 72.3\% \\
L2 (Warm) & 40\% & 0.5ms & 18.6\% \\
L3 (Cold) & 20\% & 2.0ms & 8.5\% \\
Memory & --- & 10.0ms & 0.6\% (miss) \\
\midrule
\textbf{Total} & 100MB & --- & 99.4\% \\
\bottomrule
\end{tabular}
\caption{Cache configuration under iso-cache constraints.}
\label{tab:cache-config}
\end{table}

\subsection{System Integration and Deployment}
\label{subsec:integration}

\subsubsection{Model-Agnostic Integration}

\design operates transparently at inference time without modifying pre-trained MoE models, a critical advantage over approaches like Pre-gated MoE~\cite{pregated} that require architectural changes and model retraining. The integration is achieved through strategic instrumentation of the MoE router to capture routing decisions and hidden states. Our system intercepts the router output, records the routing decisions along with hidden states for prediction model input, generates predictions for future layers, and initiates prefetch operations, all while allowing normal execution to proceed unimpeded.

The prefetch operations execute asynchronously to avoid blocking the forward pass, achieving:
\begin{equation}
T_{\text{forward}} = T_{\text{compute}} + \max(0, T_{\text{load}} - T_{\text{lead}}) \label{eq:async_forward}
\end{equation}
where $T_{\text{lead}}$ is the time between prediction and expert usage, typically spanning 2-3 layers. This asynchronous execution ensures that prefetching overhead never impacts the critical computation path, maintaining model serving latency while reducing expert loading stalls.

\subsubsection{Computational Overhead Analysis}

The neural predictor introduces minimal overhead relative to MoE computation. A single forward pass through the predictor requires only:
\begin{equation}
T_{\text{predict}} = T_{\text{embed}} + N_{\text{layers}} \cdot T_{\text{attn}} + T_{\text{head}} = 0.15\text{ms} \label{eq:predict_time}
\end{equation}

This represents merely 1\% of typical expert computation time (15ms), ensuring that prediction benefits far outweigh computational costs.

From a memory perspective, the predictor's 8.4M parameters constitute only 0.32\% of Switch Transformer's 2.6B parameters and 0.06\% of Qwen MoE's 14.3B parameters, a minimal footprint that does not materially impact GPU memory availability for model serving. While initial training requires 3.5 hours on a single GPU, this cost is amortized across thousands of inference requests, making the per-inference training cost negligible. Furthermore, the predictor can be continuously updated through online learning during deployment, adapting to workload-specific patterns without service interruption.

\subsubsection{Hardware-Aware Adaptation}

\design incorporates sophisticated hardware-specific optimizations based on device characteristics, computing transfer times as:
\begin{equation}
T_{\text{transfer}} = \frac{\text{ExpertSize} \times N_{\text{experts}}}{\text{BW}_{\text{effective}}} \times (1 + \text{Contention}) \label{eq:transfer_time}
\end{equation}

where $\text{BW}_{\text{effective}}$ accounts for the memory hierarchy bottleneck between GPU memory and system storage. The system profiles hardware capabilities at initialization and adjusts prefetching aggressiveness accordingly. On bandwidth-constrained edge devices such as Jetson Orin with 204GB/s unified memory bandwidth, the system reduces prefetch batch sizes to avoid congestion. Conversely, on data center GPUs like A100 with 1.5TB/s HBM bandwidth, it increases parallelism to fully utilize available resources. This adaptive approach ensures optimal performance across the entire spectrum of deployment scenarios.

The hardware profiling module characterizes key system parameters including memory bandwidth hierarchy (HBM, DDR4, PCIe), expert sizes and memory layout, cache sizes and latencies, and contention patterns under different loads. This information drives dynamic adaptation of prefetch scheduling, cache allocation policies, and transfer prioritization to match hardware capabilities.

\subsubsection{Multi-Architecture Adaptation}

\design dynamically adapts its strategies based on MoE architecture characteristics, demonstrating versatility across diverse routing patterns. For sparse routing architectures like Switch Transformer with top-1 routing selecting a single expert from 128 options, the system employs high precision prefetching focused on top-3 predictions, aggressive caching due to few experts needed per token, and minimal deduplication given limited expert overlap in small batches. In contrast, for dense routing architectures like Qwen MoE with top-8 routing selecting multiple experts from 64 options, the system switches to coverage-oriented prefetching that caches top-16 predictions, implements balanced caching for the increased experts per token, and leverages significant deduplication opportunities from high expert overlap across batch items. These architecture-specific adaptations ensure optimal performance across diverse MoE configurations without manual tuning.

\begin{figure}[t]
\centering
\caption{End-to-end \design workflow: asynchronous pipeline from routing traces to prefetching hides expert loading latency.}
\label{fig:workflow}
\end{figure}

\design's design systematically addresses the expert loading bottleneck through a synergistic combination of neural prediction, batch optimization, and hierarchical caching. The neural predictor achieves 33.86\% accuracy, a $43\times$ improvement over random selection, by exploiting temporal and spatial patterns in routing decisions that we empirically characterized. Batch-aware deduplication provides up to 87.6\% memory savings by leveraging the power-law expert popularity distribution we discovered. The hierarchical cache system with prediction-driven prefetching achieves 99.4\% hit rates under iso-cache constraints, matching or exceeding the performance of prior work while operating under identical resource constraints. Together, these components deliver $2\times$ to $15\times$ end-to-end speedup across diverse MoE architectures while maintaining minimal computational overhead of 0.32\% memory and 1\% latency. The system's model-agnostic design ensures compatibility with existing and future MoE architectures without requiring model modifications or retraining, distinguishing it from approaches like Pre-gated MoE that necessitate architectural changes.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 