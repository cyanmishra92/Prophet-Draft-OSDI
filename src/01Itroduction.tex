%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Introduction}
\label{sec:01Introduction}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%

Mixture-of-Experts (MoE) architectures have emerged as the dominant paradigm for scaling language models to unprecedented parameter counts while maintaining computational efficiency~\cite{jiang2024mixtral, gupta2024dbrx,dernbach2024glam, fedus2022switch,costa2022no, flame}. By routing each input token to a small subset of specialized expert networks rather than processing through the entire model~\cite{go2025moetuneroptimizedmixtureexpert}, MoE systems enable trillion-parameter models~\cite{fedus2022switch} to achieve the computational footprint of much smaller dense networks~\cite{cao2025moe,artetxe2022efficientlargescalelanguage}. Despite this promise, MoE models face a fundamental systems challenge that prevents them from realizing their theoretical efficiency gains: expert loading dominates inference time due to severe memory bandwidth bottlenecks~\cite{pregated, hobbit, zhang2025daop,kamahori2025fiddlercpugpuorchestrationfast}.

\noindent\textbf{The MoE Efficiency Paradox.}
The computational efficiency of MoE architectures stems from sparse activation patterns, where each token activates only a fraction of available experts~\cite{zhou2022mixture}. Switch Transformer~\cite{fedus2022switch} employs top-1 routing, activating 1 out of 128 experts per token, while Qwen MoE~\cite{yang2025qwen3, team2024qwen2, bai2023qwen} uses top-8 routing to activate 8 out of 64 experts.
This sparsity enables models to scale to trillions of parameters~\cite{fedus2022switch} while requiring only the computational resources of hundred-billion parameter dense models during inference, representing a breakthrough in efficient scaling.

However, this computational efficiency creates a severe memory paradox. While computation requirements scale with activated experts, memory requirements scale with \textit{total} experts, creating fundamental resource mismatches~\cite{zadouri2023pushing,cao2025moe, moeinfinity}. Switch Transformer~\cite{fedus2022switch} requires approximately 806MB per MoE layer to store all experts, while Qwen3-MoE~\cite{yang2025qwen3} demands 23GB for its 60 routing experts---far exceeding typical GPU memory capacity. More critically, the dynamic nature of expert routing creates unpredictable memory access patterns that defeat traditional caching strategies.


\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/moe_IO_benchmark_H100.pdf}
\vspace{-10pt}
\caption{I/O-to-compute profiling for Switch and Qwen models across batch sizes shows that host/device I/O and memory movement dominate execution time.}
\label{fig:io_bottleneck}
\vspace{-10pt}
\end{figure}

\noindent\textbf{The Memory Bandwidth Crisis.}
Prior works~\cite{pregated,promoe, expertflow, hobbit, fate} and our empirical analyses, depicted in \Cref{fig:io_bottleneck}, reveal that inference performance is dominated by expert loading rather than computation. While \Cref{fig:io_bottleneck} profiles Switch Transformer and Qwen1.5-MoE (78--96\% I/O), larger architectures exhibit even more severe bottlenecks: our profiling reveals GPT-OSS-20B spends 92.2\% of inference time on expert transfer, Mixtral-8x7B spends 89.3\%, and DeepSeek-MoE-16B spends 74.0\%.
The pronounced I/O--compute imbalance~\cite{zadouri2023pushing, doucet2025harmoeny} leads to GPU underutilization~\cite{zhou2025floe} and makes expert loading the principal determinant of latency across batch sizes and model scales.

\noindent\textbf{Why Prediction is Feasible.}
Effective prefetching requires exploitable structure in expert routing patterns---if routing decisions were truly random, prediction-based prefetching would provide no benefit over simple caching. Our comprehensive information-theoretic analysis (\Cref{sec:Motivation}) establishes that such structure exists and is quantifiable. Expert selection follows robust power-law distributions ($\alpha = 1.2$--$2.2$) across architectures, with the top 20\% of experts handling 75\% of routing decisions. More critically, cross-layer routing exhibits 0.62 bits of mutual information---reducing prediction uncertainty by 54\% compared to random chance. These patterns emerge from model architecture rather than input semantics: when trained on two datasets and evaluated on a third, prediction accuracy drops only 3--8\%, demonstrating that routing structure is model-intrinsic and generalizable. This principled foundation distinguishes Prophet from prior heuristic approaches that lack theoretical justification for prediction feasibility.

\noindent\textbf{Limitations of Current Approaches.}
Existing solutions to the expert loading bottleneck fall into two categories.
\textit{Reactive approaches} employ on-demand expert loading with traditional cache replacement policies~\cite{hobbit}, providing zero overlap between expert loading and computation. \textit{Predictive approaches} attempt to preload experts~\cite{pregated, expertflow, promoe} based on anticipated future needs, but suffer from critical limitations:
\textbf{First}, they optimize individual bottlenecks in isolation rather than addressing the coupled nature of prediction accuracy, memory bandwidth, and cache utilization.
\textbf{Second}, existing predictive systems rely on heuristics---stride-based patterns~\cite{promoe} or single-layer lookahead~\cite{expertflow}---that fail to capture complex cross-layer routing dependencies.
\textbf{Third}, none provide theoretical justification for why prediction should work or quantify the bounds on achievable accuracy.

\noindent\textbf{Problem Scope.}
Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. In this regime, prediction accuracy is paramount: a single misprediction incurs the full 13$\times$--15$\times$ I/O penalty that dominates token generation time. Prophet additionally demonstrates throughput benefits for batched workloads in our evaluation, where expert reuse patterns provide complementary memory savings.

To address these challenges, this paper introduces \design, a neural expert prefetching system that transforms MoE inference from reactive execution to predictive optimization. Prophet's core innovation is a lightweight transformer-based predictor (8.4M parameters, <2\% overhead) that captures routing dependencies across multiple MoE layers, achieving 81--87\% prediction accuracy. High-accuracy predictions enable proactive expert loading that hides transfer latency behind computation, eliminating the I/O bottleneck rather than merely managing it. Prophet operates as a plug-and-play solution requiring no modifications to pre-trained MoE models, avoiding the prohibitively expensive process of retraining trillion-parameter models while enabling deployment across diverse hardware configurations.

\design makes the following {\bf contributions} to advance the state-of-the-art in MoE inference optimization:

\noindent$\bullet$ \textbf{Information-theoretic framework:} The first principled analysis quantifying expert routing predictability through mutual information (0.62 bits, 54\% uncertainty reduction) and power-law distributions ($\alpha = 1.2$--$2.2$), establishing theoretical bounds that guide predictor design choices.

\noindent$\bullet$ \textbf{Cross-layer neural prediction:} A learned neural predictor that captures cross-layer routing dependencies, achieving 81--87\% prediction accuracy through dense transformer architecture with layer encoding---advancing beyond heuristic approaches used in prior work.

\noindent$\bullet$ \textbf{Prediction-guided caching:} Two-tier hierarchical caching that maps prediction confidence to resource allocation, achieving 99.4\% effective hit rates by prioritizing high-confidence predictions for proactive loading.

\noindent$\bullet$ \textbf{Cross-architecture and cross-domain evaluation:} Comprehensive evaluation across five MoE architectures (Switch Transformer, Mixtral-8x7B, Qwen1.5-MoE, Qwen3-30B, DeepSeek-MoE-16B, GPT-OSS-20B) and six datasets, demonstrating $1.5\times$--$12.7\times$ performance speedups and robust cross-domain generalization where patterns learned from one dataset transfer effectively to others.

The remainder of this paper provides background on MoE architectures (\Cref{sec:background}), establishes theoretical foundations through information-theoretic analysis (\Cref{sec:Motivation}), details Prophet's system design and algorithms (\Cref{sec:design}), presents implementation and comprehensive experimental evaluation (\Cref{sec:implementation}), discusses related work in MoE optimization (\Cref{sec:related}), and concludes with implications for future MoE deployment (\Cref{sec:conclusion}).

%%%%%
