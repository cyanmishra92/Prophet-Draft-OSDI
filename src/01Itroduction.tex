%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Introduction}
\label{sec:01Introduction}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%

Mixture-of-Experts (MoE) architectures have emerged as the dominant paradigm for scaling language models to unprecedented parameter counts while maintaining computational efficiency~\cite{jiang2024mixtral, gupta2024dbrx,dernbach2024glam, fedus2022switch,costa2022no, flame}. By routing each input token to a small subset of specialized expert networks rather than processing through the entire model~\cite{go2025moetuneroptimizedmixtureexpert}, MoE systems enable trillion-parameter models~\cite{fedus2022switch} to achieve the computational footprint of much smaller dense networks~\cite{cao2025moe,artetxe2022efficientlargescalelanguage}. Despite this promise, MoE models face a fundamental systems challenge that prevents them from realizing their theoretical efficiency gains: expert loading dominates inference time due to severe memory bandwidth bottlenecks~\cite{pregated, hobbit, zhang2025daop,kamahori2025fiddlercpugpuorchestrationfast}.

\noindent\textbf{The MoE Efficiency Paradox.}
The computational efficiency of MoE architectures stems from sparse activation patterns, where each token activates only a fraction of the available experts~\cite{zhou2025floe}. For example, Switch Transformer~\cite{fedus2022switch} employs top-1 routing, activating 1 out of 128 experts per token, while Qwen MoE~\cite{yang2025qwen3, team2024qwen2, bai2023qwen} uses top-8 routing to activate 8 out of 64 experts. This sparsity enables models to scale to trillions of parameters~\cite{fedus2022switch} while requiring only the computational resources of hundreds of billion parameter dense models during inference, representing a breakthrough in efficient scaling.

However, this computational efficiency creates a severe memory paradox. While computation requirements scale with activated experts, memory requirements scale with \textit{total} experts, creating fundamental resource mismatches~\cite{zadouri2023pushing,cao2025moe, moeinfinity}. Switch Transformer~\cite{fedus2022switch} requires approximately 806MB per MoE layer to store all experts, while Qwen3-MoE~\cite{yang2025qwen3} demands 23GB for its 60 routing experts---far exceeding typical GPU memory capacity. More critically, the dynamic nature of expert routing creates unpredictable memory access patterns that defeat traditional caching strategies.

\begin{figure}[t]
\centering
\includegraphics[width=0.7\linewidth]{figs/moe_IO_benchmark_H100.pdf}
\vspace{-10pt}
\caption{I/O-to-compute breakdown for MoE inference at batch=1. Expert transfer (red) dominates execution time across all five models, consuming 35--92\% of total latency.}
\label{fig:io_bottleneck}
\vspace{-10pt}
\end{figure}

\noindent\textbf{The Memory Bandwidth Crisis.}
Previous works~\cite{pregated,promoe, expertflow, hobbit, fate} and our empirical analysis reveal that MoE inference performance is dominated by expert loading rather than computation. \Cref{fig:io_bottleneck} profiles all five evaluation models at batch size 1: GPT-OSS-20B spends 92.2\% of inference time on expert transfer, Mixtral-8x7B spends 89.3\%, DeepSeek-MoE-16B spends 74.0\%, Qwen3-30B spends 64.8\%, and even the smallest model (Qwen1.5-MoE) spends 35.5\%. The pronounced I/O--compute imbalance~\cite{zadouri2023pushing, doucet2025harmoeny} leads to GPU underutilization~\cite{zhou2025floe} and makes expert loading the principal determinant of latency.

\noindent\textbf{Two Approaches to the Memory Challenge.}
Two strategies address expert memory constraints, each with fundamental limitations. \textit{Full GPU deployment} distributes all experts across multiple GPUs, requiring 8--16+ devices for large models while leaving most experts idle due to sparse activation---a cost-prohibitive approach where expensive hardware sits underutilized. \textit{On-demand loading} streams experts from CPU memory or SSD as needed, enabling single-GPU deployment but suffering the I/O penalties shown in \Cref{fig:io_bottleneck}: each expert transfer incurs 13$\times$--15$\times$ latency overhead compared to computation, making I/O the dominant cost. 

\noindent\textbf{The Prefetching Gap.}
Recognizing I/O dominance, prior work attempts to prefetch experts before they are needed. Architectural approaches like Pre-gated MoE~\cite{pregated} modify the router during training to predict experts one layer ahead, achieving near-optimal performance but requiring expensive model retraining --impractical for existing trillion-parameter deployments. Heuristic predictors~\cite{promoe, expertflow, fate} avoid retraining by using stride-based patterns or a single-layer lookahead, but achieve limited accuracy without understanding \textit{why} the prediction should work.
This gap leaves fundamental questions unanswered: \textit{\textbf{1.} Is there exploitable structure in expert routing, or are patterns essentially random?} \textit{\textbf{2.} Are routing decisions data-dependent or model-intrinsic?} \textit{\textbf{3.} What happens when predictors encounter data outside their training distribution?} \textit{\textbf{4.} Are there theoretical bounds on achievable prediction accuracy?} Without answers, predictor design remains ad-hoc guesswork, and practitioners cannot know whether improved prediction is even possible.

\noindent\textbf{Why Prediction is Feasible: }% An Information-Theoretic Foundation.}
Our comprehensive analysis (\Cref{sec:Motivation}) provides definitive answers to these questions. We establish that expert routing contains substantial exploitable structure: selection patterns follow power-law distributions ($\alpha = 1.2$--$2.2$) where the top 20\% of experts handle 75\% of routing decisions. Our entropy analysis reveals that a model with 128 experts has 7.00 bits maximum entropy, of which 4.11 bits (59\%) are exploitable for prediction---decomposed into 2.59 bits from cross-layer expert routing history (3 layers), 0.76 bits from hidden states, and 0.75 bits from attention patterns. Critically, these patterns are \textit{model-intrinsic} rather than data-dependent: when trained on one dataset and evaluated on another, prediction accuracy remains 67--98\% across all domain pairs, demonstrating that the routing structure emerges from the model architecture rather than dataset-specific features. This exploitable structure is \textit{uniformly distributed across layer positions}. 
%Early, middle, and late layers exhibit similar cross-layer dependencies validating a unified predictor architecture rather than layer-group-specific designs. 
This principled foundation transforms expert prefetching from engineering heuristic to empirically-grounded optimization.

\noindent\textbf{Problem Scope.}
Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. In this regime, prediction accuracy is paramount: a single misprediction incurs the full 13$\times$--15$\times$ I/O penalty that dominates token generation time. Prophet additionally demonstrates throughput benefits for batched workloads in our evaluation.

To address these challenges, this paper introduces \design, a neural expert prefetching system grounded in information-theoretic analysis. \design employs a lightweight transformer-based predictor (8.4M parameters, $<$1\% inference overhead) that captures cross-layer routing dependencies, achieving 80--89\% prediction accuracy. High-accuracy predictions enable proactive expert loading that hides transfer latency behind computation, eliminating the I/O bottleneck rather than merely managing it. Prophet operates as a plug-and-play solution requiring no modifications to pre-trained MoE models, avoiding the prohibitively expensive process of retraining trillion-parameter models while enabling deployment across diverse hardware configurations. \design makes the following {\bf contributions} to advance the state-of-the-art in MoE inference optimization:

\noindent$\bullet$ \textbf{Information-theoretic framework:} The first principled analysis quantifying expert routing predictability, revealing that 4.11 bits (59\% of maximum entropy) are exploitable---decomposed into contributions from expert routing history, hidden states, and attention patterns---with power-law distributions ($\alpha = 1.2$--$2.2$) providing empirical foundations that guide predictor design.

\noindent$\bullet$ \textbf{Cross-layer neural prediction:} A learned neural predictor that captures cross-layer routing dependencies, achieving 80--89\% prediction accuracy through dense transformer architecture with layer encoding.

\noindent$\bullet$ \textbf{Complete inference system:} An end-to-end system achieving $1.5\times$--$10.4\times$ TPOT speedups, matching state-of-the-art predictors (FATE, PreScope) while enabling 1.2--1.8$\times$ longer contexts through efficient expert caching.

\noindent$\bullet$ \textbf{Cross-architecture and cross-domain evaluation:} Comprehensive evaluation across five production MoE architectures (Mixtral-8x7B, Qwen1.5-MoE-A2.7B, Qwen3-30B, DeepSeek-MoE-16B, and GPT-OSS-20B) and six datasets, demonstrating robust cross-domain generalization where prediction accuracy remains 67--98\% across all domain pairs.

The paper proceeds with background (\Cref{sec:background}), information-theoretic analysis (\Cref{sec:Motivation}), system design (\Cref{sec:design}), evaluation (\Cref{sec:implementation}), related work (\Cref{sec:related}), and conclusions (\Cref{sec:conclusion}).

%%%%%
