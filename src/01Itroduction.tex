%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
\section{Introduction}
\label{sec:01Introduction}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 

Mixture-of-Experts (MoE) architectures have emerged as the dominant paradigm for scaling language models to unprecedented parameter counts while maintaining computational efficiency~\cite{jiang2024mixtral, gupta2024dbrx,dernbach2024glam, fedus2022switch,costa2022no, flame}. By routing each input token to a small subset of specialized expert networks rather than processing through the entire model~\cite{go2025moetuneroptimizedmixtureexpert}, MoE systems enable trillion-parameter models~\cite{fedus2022switch} to achieve the computational footprint of much smaller dense networks~\cite{cao2025moe,artetxe2022efficientlargescalelanguage}. Despite this promise, MoE models face a fundamental systems challenge that prevents them from realizing their theoretical efficiency gains: expert loading dominates (upto 87-95\%) of inference time due to severe memory bandwidth bottlenecks~\cite{pregated, hobbit, zhang2025daop,kamahori2025fiddlercpugpuorchestrationfast}.

\noindent\textbf{The MoE Efficiency Paradox.}
The computational efficiency of MoE architectures stems from sparse activation patterns, where each token activates only a fraction of available experts~\cite{zhou2022mixture}. Switch Transformer~\cite{fedus2022switch} employs top-1 routing, activating 1 out of 128 experts per token
, while Qwen MoE~\cite{yang2025qwen3, team2024qwen2, bai2023qwen} uses top-8 routing to activate 8 out of 64 experts.
This sparsity enables models to scale to trillions of parameters~\cite{fedus2022switch} while requiring only the computational resources of hundred-billion parameter dense models during inference, representing a breakthrough in efficient scaling.

However, this computational efficiency creates a severe memory paradox. While computation requirements scale with activated experts, memory requirements scale with \textit{total} experts, creating fundamental resource mismatches~\cite{zadouri2023pushing,cao2025moe, moeinfinity}. Switch Transformer~\cite{fedus2022switch} requires approximately 806MB per MoE layer to store all experts, while Qwen3-MoE~\cite{yang2025qwen3} demands 23GB for its 60 routing experts—far exceeding typical GPU memory capacity. More critically, the dynamic nature of expert routing creates unpredictable memory access patterns that defeat traditional caching strategies.


\begin{figure}[t]
\centering
%\includegraphics[width=0.9\linewidth]{figs/bottleneck_severity_scaling.pdf}
\includegraphics[width=\linewidth]{figs/moe_IO_benchmark_H100.pdf}
\vspace{-10pt}
\caption{I/O-to-compute profiling for Switch and Qwen models across batch sizes shows that host/device I/O and memory movement dominate execution time ($\sim$78–96\%).}
\label{fig:io_bottleneck}
\vspace{-10pt}
\end{figure}

\noindent\textbf{The Memory Bandwidth Crisis.}
Prior works~\cite{pregated,promoe, expertflow, hobbit, fate} and our empirical analyses,
depicted in \Cref{fig:io_bottleneck}, reveal that inference performance is dominated by expert loading rather than the computation they ensue. The pronounced I/O–compute imbalance~\cite{zadouri2023pushing, doucet2025harmoeny}, in which expert transfer time eclipses on-device computation,
leads to GPU under utilization~\cite{zhou2025floe} and makes expert loading the principal determinant of latency across batch sizes and scales. 
The manifestation of the memory-bandwidth constraint varies by setting: edge devices, limited by on-device memory, incur persistent I/O pressure due to frequent expert swapping~\cite{fate}, whereas data-center systems, despite larger memory pools, encounter models whose expert counts exceed GPU capacity~\cite{hobbit, promoe, expertflow}, necessitating layered, complex memory hierarchies. 

\noindent\textbf{Limitations of Current Approaches.}
Existing solutions to the expert loading bottleneck fall into two categories.
\textit{Reactive approaches} employ on-demand expert loading with traditional cache replacement policies~\cite{hobbit}, providing zero overlap between expert loading and computation. \textit{Predictive approaches} attempt to preload experts~\cite{pregated, expertflow, promoe} based on anticipated future needs, but suffer from critical limitations: 
\textbf{First}, they optimize individual bottlenecks in isolation rather than addressing the coupled nature of prediction accuracy, memory bandwidth, and cache utilization.
\textbf{Second}, most solutions target single-request scenarios, missing substantial batch-level optimization opportunities, where expert reuse patterns can provide substantial memory savings~\cite{fate}.
\textbf{Third}, existing predictive systems~\cite{promoe,expertflow} lack the sophistication to capture complex routing dependencies that span multiple transformer layers, limiting accuracy and effectiveness.


\noindent\textbf{Key Insight: Exploiting Expert Routing Structure.}
The fundamental limitation of current approaches is their inability to recognize and exploit the rich structure inherent in expert routing patterns. Through comprehensive analysis of expert routing traces across diverse MoE architectures (\Cref{sec:Motivation}), we identify the following exploitable structure that enables systematic optimization:
\textbf{\textit{Temporal correlations}} in expert routing provide substantial predictive power, with expert selections exhibiting systematic dependencies where current selections influence future routing decisions in predictable ways. \textbf{\textit{Power-law popularity distributions}} create extreme concentration effects, where a small fraction of experts handle most routing decisions, allowing intelligent caching strategies targeting experts frequently accessed. \textbf{\textit{Mutual-information}} reveals that experts organize into distinct specialization groups with predictable co-occurrence patterns, enabling optimization that takes advantage of expert relationships beyond simple frequency statistics. Most critically, we discover strong \textit{cross-layer dependencies} in expert routing that existing single-layer approaches cannot capture. When processing complex inputs, tokens routed to specific expert types in early layers predictably route to related expert types in deeper layers, creating learnable patterns that span the entire transformer stack. These dependencies provide the foundation for accurate neural prediction that can achieve significant improvements over heuristic approaches.

To address this, the paper introduces Prophet, a comprehensive neural expert prefetching system that transforms MoE inference from reactive execution to predictive optimization. Prophet's core innovation is a dense transformer-based predictor that captures routing dependencies across multiple MoE layers, achieving substantial prediction accuracy with minimal computational overhead. Prophet operates through three integrated components: a \textit{neural predictor} that processes multi-layer routing history to predict expert selections several layers ahead, a \textit{batch de-duplicator} that exploits power-law concentration effects for significant memory savings, and a \textit{hierarchical caching system} that implements confidence-based resource allocation across multiple cache tiers. Crucially, Prophet operates as a plug-and-play solution requiring no modifications to pre-trained MoE models, avoiding the prohibitively expensive process of retraining trillion-parameter models while enabling deployment across diverse hardware configurations: from desktop grade to datacenter quality. 
\design makes the following {\bf contributions} to advance the state-of-the-art in MoE inference optimization: 
%\textcolor{red}{validate the numbers and results}

\noindent$\bullet$ \textbf{Cross-layer neural prediction:} The first system to systematically exploit dependencies between routing decisions across transformer layers, achieving 87\% prediction accuracy through novel dense transformer architecture with layer encoding.

\noindent$\bullet$ \textbf{Batch-aware expert deduplication:} Theoretical and empirical analysis of expert reuse patterns in batch processing, enabling up to 87.6\% memory savings through systematic exploitation of power-law popularity distributions with provable scaling guarantees.

\noindent$\bullet$ \textbf{Hierarchical confidence-based caching:} Two-tier caching that maps prediction confidence to resource allocation, achieving 99.4\% hit rates under realistic memory constraints through sophisticated prefetching and replacement strategies.

\noindent$\bullet$ \textbf{Comprehensive cross-architecture evaluation:} The first systematic comparison of expert loading optimization across sparse and dense routing strategies with extensive baseline comparisons, demonstrating $1.5\times$--$3.2\times$ performance speedups and $1.5\times$--$15\times$ memory improvements with respect to the state-of-the-art.

The remainder of this paper provides background on MoE architectures (\Cref{sec:background}), establishes theoretical foundations (\Cref{sec:Motivation}), details Prophet's system design and algorithms (\Cref{sec:design}), presents implementation and comprehensive experimental evaluation (\Cref{sec:implementation}), discusses related work in MoE optimization (\Cref{sec:related}), and concludes with implications for future MoE deployment (\Cref{sec:conclusion}).

%%%%%
