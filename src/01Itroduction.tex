%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Introduction}
\label{sec:01Introduction}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%

Mixture-of-Experts (MoE) architectures have emerged as the dominant paradigm for scaling language models to unprecedented parameter counts while maintaining computational efficiency~\cite{jiang2024mixtral, gupta2024dbrx,dernbach2024glam, fedus2022switch,costa2022no, flame}. By routing each input token to a small subset of specialized expert networks rather than processing through the entire model~\cite{go2025moetuneroptimizedmixtureexpert}, MoE systems enable trillion-parameter models~\cite{fedus2022switch} to achieve the computational footprint of much smaller dense networks~\cite{cao2025moe,artetxe2022efficientlargescalelanguage}. Despite this promise, MoE models face a fundamental systems challenge that prevents them from realizing their theoretical efficiency gains: expert loading dominates inference time due to severe memory bandwidth bottlenecks~\cite{pregated, hobbit, zhang2025daop,kamahori2025fiddlercpugpuorchestrationfast}.

\noindent\textbf{The MoE Efficiency Paradox.}
The computational efficiency of MoE architectures stems from sparse activation patterns, where each token activates only a fraction of the available experts~\cite{zhou2022mixture}. For example, Switch Transformer~\cite{fedus2022switch} employs top-1 routing, activating 1 out of 128 experts per token, while Qwen MoE~\cite{yang2025qwen3, team2024qwen2, bai2023qwen} uses top-8 routing to activate 8 out of 64 experts. This sparsity enables models to scale to trillions of parameters~\cite{fedus2022switch} while requiring only the computational resources of hundreds of billion parameter dense models during inference, representing a breakthrough in efficient scaling.

However, this computational efficiency creates a severe memory paradox. While computation requirements scale with activated experts, memory requirements scale with \textit{total} experts, creating fundamental resource mismatches~\cite{zadouri2023pushing,cao2025moe, moeinfinity}. Switch Transformer~\cite{fedus2022switch} requires approximately 806MB per MoE layer to store all experts, while Qwen3-MoE~\cite{yang2025qwen3} demands 23GB for its 60 routing experts---far exceeding typical GPU memory capacity. More critically, the dynamic nature of expert routing creates unpredictable memory access patterns that defeat traditional caching strategies.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/moe_IO_benchmark_H100.pdf}
\vspace{-10pt}
\caption{I/O-to-compute profiling for Switch and Qwen models across batch sizes shows that host/device I/O and memory movement dominate execution time.}
\label{fig:io_bottleneck}
\vspace{-10pt}
\end{figure}

\noindent\textbf{The Memory Bandwidth Crisis.}
Previous works~\cite{pregated,promoe, expertflow, hobbit, fate} and our empirical analysis, depicted in \Cref{fig:io_bottleneck}, reveal that the inference performance is dominated by expert loading rather than computation. While \Cref{fig:io_bottleneck} profiles Switch Transformer and Qwen1.5-MoE (78--96\% I/O), larger architectures exhibit even more severe bottlenecks: our  profiling reveals that GPT-OSS-20B spends 92.2\% of inference time on expert transfer, Mixtral-8x7B spends 89.3\%, and DeepSeek-MoE-16B spends 74.0\%. The pronounced I/O--compute imbalance~\cite{zadouri2023pushing, doucet2025harmoeny} leads to GPU underutilization~\cite{zhou2025floe} and makes expert loading the principal determinant of latency.

\noindent\textbf{Two Approaches to the Memory Challenge.}
Two strategies address expert memory constraints, each with fundamental limitations. \textit{Full GPU deployment} distributes all experts across multiple GPUs, requiring 8--16+ devices for large models while leaving most experts idle due to sparse activation---a cost-prohibitive approach where expensive hardware sits underutilized. \textit{On-demand loading} streams experts from CPU memory or SSD as needed, enabling single-GPU deployment but suffering the I/O penalties shown in \Cref{fig:io_bottleneck}: each expert transfer incurs 13$\times$--15$\times$ latency overhead compared to computation, making I/O the dominant cost. 

\noindent\textbf{The Prefetching Gap.}
Recognizing I/O dominance, prior work attempts to prefetch experts before they are needed. Architectural approaches like Pre-gated MoE~\cite{pregated} modify the router during training to predict experts one layer ahead, achieving near-optimal performance but requiring expensive model retraining --impractical for existing trillion-parameter deployments. Heuristic predictors~\cite{promoe, expertflow, fate} avoid retraining by using stride-based patterns or a single-layer lookahead, but achieve limited accuracy without understanding \textit{why} the prediction should work.
This gap leaves fundamental questions unanswered: \textit{\textbf{1.} Is there exploitable structure in expert routing, or are patterns essentially random?} \textit{\textbf{2.} Are routing decisions data-dependent or model-intrinsic?} \textit{\textbf{3.} What happens when predictors encounter data outside their training distribution?} \textit{\textbf{4.} Are there theoretical bounds on achievable prediction accuracy?} Without answers, predictor design remains ad-hoc guesswork, and practitioners cannot know whether improved prediction is even possible.

\noindent\textbf{Why Prediction is Feasible: }% An Information-Theoretic Foundation.}
Our comprehensive analysis (\Cref{sec:Motivation}) provides definitive answers to these questions. We establish that expert routing contains substantial exploitable structure: selection patterns follow power-law distributions ($\alpha = 1.2$--$2.2$) where the top 20\% of experts handle 75\% of routing decisions, and cross-layer routing exhibits 0.62 bits of mutual information, reducing prediction uncertainty by 54\% compared to random chance. Critically, these patterns are \textit{model-intrinsic} rather than data-dependent: when trained on two datasets and evaluated on a third, the prediction accuracy drops only 3--8\%, demonstrating that the routing structure emerges from the model architecture. Our entropy analysis strengthens this foundation: a model with 128 experts has 7.00 bits maximum entropy, of which 4.11 bits (59\%) are exploitable for prediction, establishing theoretical bounds that guide predictor design. This principled foundation transforms expert prefetching from engineering heuristic to theoretically-grounded optimization.

\noindent\textbf{Problem Scope.}
Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. In this regime, prediction accuracy is paramount: a single misprediction incurs the full 13$\times$--15$\times$ I/O penalty that dominates token generation time. Prophet additionally demonstrates throughput benefits for batched workloads in our evaluation.

To address these challenges, this paper introduces \design, a neural expert prefetching system grounded in information-theoretic analysis. \design employs a lightweight transformer-based predictor (8.4M parameters, <2\% inference overhead) that captures cross-layer routing dependencies, achieving 81--87\% prediction accuracy, approaching the theoretical limits established by our analysis. High-accuracy predictions enable proactive expert loading that hides transfer latency behind computation, eliminating the I/O bottleneck rather than merely managing it. Prophet operates as a plug-and-play solution requiring no modifications to pre-trained MoE models, avoiding the prohibitively expensive process of retraining trillion-parameter models while enabling deployment across diverse hardware configurations. \design makes the following {\bf contributions} to advance the state-of-the-art in MoE inference optimization:

\noindent$\bullet$ \textbf{Information-theoretic framework:} The first principled analysis quantifying expert routing predictability through mutual information (0.62 bits, 54\% uncertainty reduction) and power-law distributions ($\alpha = 1.2$--$2.2$), establishing theoretical bounds that guide predictor design choices.

\noindent$\bullet$ \textbf{Cross-layer neural prediction:} A learned neural predictor that captures cross-layer routing dependencies, achieving 81--87\% prediction accuracy through dense transformer architecture with layer encoding (designed to approach information-theoretic limits).

\noindent$\bullet$ \textbf{Prediction-guided caching:} Hierarchical two-tier caching that maps prediction confidence to resource allocation, achieving 99.4\% effective hit rates by prioritizing high-confidence predictions for proactive loading.

\noindent$\bullet$ \textbf{Cross-architecture and cross-domain evaluation:} Comprehensive evaluation of five production MoE architectures (Mixtral-8x7B, Qwen1.5-MoE-A2.7B, Qwen3-30B, DeepSeek-MoE-16B, and GPT-OSS-20B) representing diverse expert counts (64--256), routing strategies (top-4 to top-8), and architectural designs. We exclude Switch Transformer as its top-1 routing is substantially easier to predict and well-studied in prior work; our target models present more challenging prediction tasks with top-k routing. The evaluation spans six datasets, demonstrating $1.5\times$--$12.7\times$ speedups and robust cross-domain generalization.

The remainder of this paper provides background on MoE architectures (\Cref{sec:background}), establishes theoretical foundations through information-theoretic analysis (\Cref{sec:Motivation}), details Prophet's system design and algorithms (\Cref{sec:design}), presents implementation and comprehensive experimental evaluation (\Cref{sec:implementation}), discusses related work in MoE optimization (\Cref{sec:related}), and concludes with implications for future MoE deployment (\Cref{sec:conclusion}).

%%%%%
