Mixture-of-Experts (MoE) models enable trillion-parameter language models through sparse activation, but suffer from severe memory bandwidth bottlenecks, where expert loading dominates 78--96\% of the inference time. Current approaches---whether heuristic or learned---fail to exploit the full structure inherent in expert routing patterns, leading to I/O-to-compute ratios of up to $15\times$ that limit practical deployment.

This paper introduces \textit{Prophet}, a neural expert prefetching system grounded in information-theoretic analysis. We first establish that expert routing contains 0.62 bits of mutual information (54\% uncertainty reduction) and follows power-law distributions ($\alpha = 1.2$--$2.2$), providing theoretical bounds that guide predictor design. Prophet employs a lightweight neural predictor to capture cross-layer routing dependencies, combined with batch-aware deduplication and hierarchical caching. Operating as a plug-and-play solution without model modifications, Prophet delivers $1.5\times$--$12.7\times$ performance speedups across five MoE architectures and demonstrates robust cross-domain generalization, where patterns learned from one dataset transfer effectively to others.
