Mixture-of-Experts (MoE) models enable trillion-parameter language models through sparse activation, but suffer from severe memory bandwidth bottlenecks, where expert loading dominates 78--96\% of the inference time. Current approaches---whether heuristic or learned---fail to exploit the full structure inherent in expert routing patterns, leading to I/O-to-compute ratios of up to $15\times$ that limit practical deployment.

This paper introduces \textit{Prophet}, a neural expert prefetching system grounded in information-theoretic analysis. We first establish that expert routing contains significant exploitable structure: 4.11 bits (59\% of maximum entropy) are predictable, decomposed into 2.59 bits from cross-layer expert routing history, 0.76 bits from hidden states, and 0.75 bits from attention patterns. Expert popularity follows power-law distributions ($\alpha = 1.2$--$2.2$), providing empirical foundations that guide predictor design. Prophet employs a lightweight neural predictor (8.4M parameters, $<$1\% overhead) with confidence-guided hierarchical caching, achieving 80--89\% prediction accuracy. Operating as a plug-and-play solution without model modifications, Prophet delivers $1.5\times$--$10.4\times$ TPOT speedups across five MoE architectures and demonstrates robust cross-domain generalization (67--98\% accuracy across 180 domain pairs).
