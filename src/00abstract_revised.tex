Mixture-of-Experts (MoE) models enable trillion-parameter language models through sparse activation, but suffer from severe memory bandwidth bottlenecks, where expert loading dominates 78--96\% of the inference time. Current approaches---whether heuristic or learned---fail to exploit the full structure inherent in expert routing patterns, leading to I/O-to-compute ratios of up to $15\times$ that limit practical deployment.

This paper introduces \textit{Prophet}, a neural expert prefetching system grounded in information-theoretic analysis. We first establish that expert routing contains significant exploitable structure: upto 59\% of maximum entropy are predictable, decomposed into cross-layer expert routing history,  hidden states, and attention patterns. Expert popularity follows power-law distributions ($\alpha = 1.2$--$2.2$), providing empirical foundations that guide predictor design. Prophet employs a lightweight neural predictor (8.4M parameters, $<$1\% overhead) with confidence-guided hierarchical caching, achieving 80--89\% prediction accuracy. Operating as a plug-and-play solution without model modifications, Prophet delivers $1.5\times$--$10.4\times$ TPOT speedups across five MoE architectures and demonstrates robust cross-domain generalization (67--98\% accuracy across 180 domain pairs).

% Mixture-of-Experts (MoE) models enable trillion-parameter language models through sparse activation, but suffer from severe memory bandwidth bottlenecks, where expert loading dominates 78--96\% of the inference time. Current approaches---whether heuristic or learned---fail to exploit the full structure inherent in expert routing patterns, leading to I/O-to-compute ratios of up to $15\times$ that limit practical deployment.

% This paper introduces \textit{Prophet}, a neural expert prefetching system grounded in information-theoretic analysis. We first establish that expert routing contains 0.62 bits of mutual information (54\% uncertainty reduction) and follows power-law distributions ($\alpha = 1.2$--$2.2$), providing theoretical bounds that guide predictor design. Prophet employs a lightweight neural predictor to capture cross-layer routing dependencies with confidence-guided hierarchical caching. Operating as a plug-and-play solution without model modifications, Prophet delivers $1.5\times$--$10.4\times$ TPOT speedups across five MoE architectures and demonstrates robust cross-domain generalization, where patterns learned from one dataset transfer effectively to others.