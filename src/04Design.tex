%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
\section{Design of Prophet}
\label{sec:design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Prophet integrates two key components grounded in our information-theoretic analysis (\Cref{sec:Motivation}): a \textbf{neural expert predictor} that forecasts expert needs multiple layers ahead by exploiting cross-layer routing dependencies, and a \textbf{prediction-guided hierarchical cache} that maps prediction confidence to cache tier placement. Prophet operates transparently at inference time without requiring modifications to MoE models.


\subsection{Neural Expert Prediction}
\label{subsec:neural-predictor}

\noindent\textbf{Comparison with Prior Predictive Approaches:}
Existing systems employ cross-layer information through various mechanisms: FATE~\cite{fate} uses cross-layer gates for predictive scheduling, MoE-Infinity~\cite{moeinfinity} traces activations across layers, and AdapMoE~\cite{adapmoe} performs single-layer lookahead prefetching. However, these approaches are fundamentally \textit{heuristic}: they exploit cross-layer information without quantifying its predictive value or optimizing the prediction mechanism itself. \Cref{tab:predictor_comparison} summarizes key differences.

\design differs in three key aspects: (1)~\textbf{Information-theoretic grounding:} Our analysis in \Cref{sec:Motivation} establishes that 63\% of exploitable predictive information comes from the 3-layer expert history---providing principled justification for context window selection rather than ad-hoc tuning. This exploitable fraction ($\sim$59\% of maximum entropy) remains consistent across architectures with 8--128 experts, despite variations in top-k routing and shared experts (\Cref{sec:Motivation}). (2)~\textbf{Learned sequence modeling:} Rather than fixed heuristics, we train a dense transformer to learn complex routing dependencies, achieving 86\% prediction accuracy compared to 12.3\% for frequency-based baselines. (3)~\textbf{Confidence-calibrated prefetching:} Prediction confidence scores enable tiered cache placement, maximizing GPU memory utilization for high-certainty predictions.

\noindent\textbf{Unified vs. Layer-Group-Aware Design:}
Recent work~\cite{prescope} proposes that MoE layers decompose into input, middle, and output groups with distinct routing characteristics, motivating per-layer-group predictors. However, our information-theoretic analysis (\Cref{sec:Motivation}) reveals that cross-layer mutual information is uniform across layer positions: the predictive relationship between layers $(\ell, \ell+h)$ is equally strong whether $\ell$ is early, middle, or late. This uniformity arises because routing decisions reflect progressive token transformation---a process governed by consistent architectural mechanisms throughout the network. A unified predictor captures these network-wide dependencies while learning transferable patterns (67--98\% cross-domain accuracy), whereas layer-group designs require $3\times$ the parameters while fragmenting cross-group information flow.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{System} & \textbf{Prediction Method} & \textbf{Context} & \textbf{Top-1 Acc.} \\
\midrule
ProMoE~\cite{promoe} & LRU-based & Current layer & $\sim$65\% \\
PreGated~\cite{pregated} & Router lookahead & 1 layer & $\sim$70\% \\
FATE~\cite{fate} & Cross-layer gates & Heuristic & $\sim$72\% \\
AdapMoE~\cite{adapmoe} & Single lookahead & 1 layer & $\sim$74\% \\
PreScope~\cite{prescope} & MLP (LLaPor) & 1 layer & N/R$^\dagger$ \\
\midrule
\textbf{\design} & Neural transformer & 3 layers & \textbf{86\%} \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\caption{Comparison with prior predictive approaches. \design's learned multi-layer context achieves higher prediction accuracy than heuristic and single-layer methods. Baseline accuracies measured under identical conditions using our reimplementation. $^\dagger$PreScope reports 94\% Top-4 accuracy but not Top-1; their layer-group-aware MLP uses single-layer context ($h$=1) without cross-domain evaluation.}
\label{tab:predictor_comparison}
\vspace{-10pt}
\end{table}

%%%%%%%%%%%%
\subsubsection{Formalization and Architecture Overview}

We formalize expert prediction as a sequence modeling task that predicts future expert selections for individual token positions based on historical routing  (the collected traces as described in \Cref{sec:Motivation}) and current hidden states. For a specific token at sequence position $t$ currently being processed at layer $\ell$, our goal is to predict which expert will be selected for this same token at a future layer $\ell+h$, where $h$ represents the prediction horizon.


\noindent\textbf{Input Representation:} The predictor input $\mathcal{H}_t$ for token position $t$ comprises three components:
$\mathcal{H}_t = \{E_t^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)},$$H_t^{(\ell)}\} $
% \begin{align}
% \mathcal{H}_t &= \{E_t^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)}, H_t^{(\ell)}\} \label{eq:predictor_input}
% \end{align}
where $E_t^{(\ell-c+1:\ell)}$ represents the expert routing history for token $t$ over the past $c$ layers, $L^{(\ell-c+1:\ell)}$ encodes layer positions, and $H_t^{(\ell)} \in \mathbb{R}^d$ denotes the current hidden state (which already encodes attention-relevant information through residual connections).
%\noindent\textbf{Prediction Target:} 
The predictor output is the predicted expert selection:
%\begin{align}
% 
$\hat{e}_t^{(\ell+h)} = \text{Predictor}(\mathcal{H}_t; \theta)$ 
%\label{eq:predictor_output}
%\end{align}
where $\theta$ represents the learnable parameters of the neural predictor model, including embedding matrices, transformer weights, and prediction head parameters.
%\noindent\textbf{Batch Processing:} 
For a batch of $B$ tokens
%$\{t_1, t_2, \ldots, t_B\}$ 
at layer $\ell$, we generate predictions for all tokens in the same positions simultaneously, enabling batch-level optimization and deduplication of predicted expert requirements.

\noindent\textbf{Context Window ($c$):} Empirically, $c = 3$ layers provides optimal accuracy-efficiency tradeoff; longer contexts yield diminishing returns, shorter ones lack sufficient history.

\noindent\textbf{Prediction Horizon ($h$):} The predictor outputs predictions for horizons $h \in \{1, 2, 3\}$ simultaneously, enabling the caching system to orchestrate prefetching across memory tiers: $h=1$ targets L1 (GPU), $h=2,3$ enable proactive L2/host loading. For typical MoE models, $h = 3$ provides 15--20ms lead time to hide expert loading behind computation.

\noindent\textbf{Architecture:} We employ a dense transformer (avoiding MoE-for-MoE complexity) with expert embeddings $\mathbf{W}_E \in \mathbb{R}^{N_E \times d}$ where $N_E$ is the expert vocabulary size (64--256) and $d = 320$. Layer position encodings $\mathbf{W}_L[\ell]$ enable the model to learn layer-specific routing characteristics. Architecture details appear in \Cref{subsubsec:training}.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/pipeline.drawio.pdf}
\caption{Neural expert predictor scheduling, illustrating routing history from layers $\ell-2$ to $\ell$ combined with current hidden states enables prediction of experts at layer $\ell+3$. 
}
\label{fig:predictor_architecture}
\end{figure}

\noindent\textbf{Toy Example:} Consider predicting expert selection for token $t$ at layer 8 while processing layer 6 (\Cref{fig:predictor_architecture}). The predictor input includes: expert IDs at layers 4, 5, 6 (routing history $E_t^{(4:6)}$), layer encodings $L^{(4:6)}$, and hidden state $H_t^{(6)}$. The prediction $\hat{e}_t^{(8)}$ enables prefetching with a 3-layer horizon.
%%%%%%%%%%%%

\subsubsection{Neural Predictor Architecture and Training}
\label{subsubsec:training}

%\noindent\textbf{Architecture Specification:} 
The predictor comprises 4 transformer layers with 10 attention heads, model dimension $d_{\text{model}} = 320$, and feed-forward dimension $d_{ff} = 1280$ (8.4M parameters total). These parameters balance prediction accuracy with computational efficiency, keeping overhead below 1\% of expert execution time. Hidden states undergo linear projection before combining with embedded routing history and layer encodings.

\noindent\textbf{Architecture Choice:} We employ a transformer rather than simpler architectures (MLP, LSTM) because the routing prediction task is fundamentally a variable-length sequence modeling problem over a multi-layer context window. Self-attention enables the model to learn which layer-to-layer transitions are most predictive for future expert selections---certain cross-layer patterns (e.g., expert-42 at layer 5 correlating with expert-78 at layer 8) may be strongly predictive regardless of intermediate selections. The transformer's learned expert embeddings naturally scale across architectures with 8--128 experts, while layer encodings capture position-specific power-law biases that vary by model. An MLP requires fixed concatenation of all inputs, losing this flexibility and limiting context to single-layer lookahead; an LSTM imposes strict sequential processing that may miss long-range dependencies.

Concurrent work PreScope~\cite{prescope} illustrates these tradeoffs: their MLP-based LLaPor predictor achieves 94\% \textit{Top-4} accuracy with single-layer ($h=1$) context, but does not report Top-1 accuracy or cross-domain transfer. In contrast, Prophet's transformer with 3-layer context achieves 80--89\% \textit{Top-1} accuracy: a stricter metric where only the single highest-probability prediction must be correct, while demonstrating 67--98\% accuracy across 36 cross-domain evaluation pairs (\Cref{sec:Motivation} and \Cref{fig:cross_domain}). The transformer's additional latency (versus an MLP) is justified by both the accuracy improvement and the extended prediction horizon ($h=3$) that provides sufficient lead time to hide expert loading latency.

\noindent\textbf{Training Data Collection:} We collect expert routing traces from MoE models processing six diverse datasets (\Cref{sec:Motivation}): Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText. This diversity ensures the predictor learns model-intrinsic routing patterns rather than dataset-specific features. For each forward pass, we record expert selections and hidden states at every layer for each token position.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figs/switch_prediction_accuracy.pdf}
\vspace{-10pt}
\caption{Predictor accuracy vs batch size for Switch transformer against the state-of-the-art. We observe a rapid decline is the accuracy beyond the prediction horizon, h = 3.}
\label{fig:predictor_accuracy}
%\vspace{-20pt}
\end{figure}

\noindent\textbf{Training:} We employ a composite loss $\mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_{\text{classification}} + \lambda_2 \mathcal{L}_{\text{ranking}} + \lambda_3 \mathcal{L}_{\text{confidence}}$, combining cross-entropy for prediction accuracy, ranking loss for probability ordering, and Brier score for confidence calibration. Training uses AdamW with cosine annealing, converging in 3.5--4.5 hours on a single A100. Systematic exploration confirms $c = 3$ and $h = 3$ provide optimal performance (\Cref{fig:predictor_accuracy}), outperforming state-of-the-art~\cite{promoe, pregated,prescope}.% ProMoE~\cite{promoe} and PreGated MoE~\cite{pregated}.

% \noindent\textbf{Prediction Accuracy and Overhead:}
% We define prediction accuracy as \textit{top-1 per-token-per-layer} accuracy: for each token $t$ at prediction horizon $h$, we check whether the predicted expert $\hat{e}_t^{(\ell+h)}$ matches the ground-truth expert selection $e_t^{(\ell+h)}$. This strict metric directly corresponds to prefetch utility, i.e., correct predictions enable cache hits, while incorrect predictions waste bandwidth. Across our evaluation, \design achieves 80--89\% top-1 accuracy across the five target architectures at horizon $h=3$, compared to 0.78--1.56\% random baselines showing over $50\times$ improvement.
% The 8.4M-parameter predictor adds $<$1\% latency and $<$2\% memory overhead; detailed breakdown in \Cref{subsec:predictor}. An anonymized implementation of the Prophet predictor is \href{https://anonymous.4open.science/r/prophet-artifact-5B42/README.md}{available here}.
%
%\footnote{\url{https://anonymous.4open.science/r/prophet-artifact-5B42/README.md}}.

%An anonymized implementation of the Prophet predictor is \href{https://anonymous.4open.science/r/prophet-artifact-5B42/README.md} {available here}.


%\noindent\textbf{Predictor Overhead:} The 8.4M-parameter predictor adds $<$1\% latency and $<$2\% memory overhead; detailed breakdown in \Cref{subsec:predictor}.

% \subsection{Batch-Aware Optimization}
% \label{subsec:batch-optimization}
% \subsubsection{Expert Deduplication Strategy}
% The power-law distribution of expert popularity creates significant opportunities for batch-level optimization that prior single-token approaches cannot exploit. The concentration effects we identified, where the most popular experts handle a disproportionate fraction of routing decisions, suggest that multiple tokens in a batch will frequently request overlapping sets of experts (\Cref{fig:batch_scaling_validation}). Our deduplication algorithm, as explained in \Cref{alg:deduplication} in \Cref{app:memoryCaching}, and illustrated in \Cref{fig:batch-deduplication} exploits this pattern by identifying unique experts across all batch items and computing an optimal loading schedule that prioritizes high-frequency experts.
% The algorithm reduces the total number of expert loads from $O(B \cdot k)$ for naive per-item loading to $O(B^{\alpha} \cdot k)$ where $\alpha = 0.932 < 1$, providing sublinear scaling that becomes increasingly beneficial at larger batch sizes. This theoretical guarantee stems directly from the power-law scaling property, which we empirically validated, ensuring predictable performance improvements as batch sizes increase.

% \subsubsection{Hardware-Aware Transfer Scheduling}
% Beyond deduplication, we implement sophisticated transfer mechanisms that pipeline expert loading with computation to hide memory bandwidth bottlenecks. Rather than loading all experts before computation begins, we divide the transfers into chunks that can overlap with the ongoing computation.
% This pipelining reduces effective loading latency by hiding transfer costs behind computation, a technique particularly effective when combined with our predictive prefetching that provides sufficient lead time for memory operations. The scheduler incorporates hardware-specific bandwidth constraints to prevent memory bottlenecks through utility-based optimization that combines prediction confidence, expert frequency, and recency. Using prediction confidence scores, we initiate transfers for high-confidence predictions before routing decisions are finalized, with the confidence threshold dynamically adjusted based on available bandwidth and cache capacity to ensure the system adapts to varying hardware capabilities.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.7\linewidth]{figs/DeDuplication.pdf}
% \caption{Expert deduplication before transferring to the GPU memory reduces bandwidth requirements significantly.}
% \label{fig:batch-deduplication}
% \vspace{-10pt}
% \end{figure}

\subsection{Prediction-Guided Expert Caching}
\label{subsec:caching}

Building on established expert caching techniques~\cite{moeinfinity,promoe,prescope}, Prophet adapts hierarchical caching to leverage its neural predictor's calibrated confidence scores. The key insight is that prediction confidence directly indicates cache placement priority: high-confidence predictions ($>$0.8) warrant L1 (GPU) placement for immediate access, while medium-confidence (0.5--0.8) predictions target L2 (host memory) for speculative coverage. This prediction-driven placement contrasts with reactive policies (LRU/LFU) that cannot anticipate future expert needs.
The two-tier hierarchy maps naturally to GPU-host memory organization: L1 cache utilizes GPU memory (24--80GB HBM) with sub-microsecond access, while L2 cache leverages host memory (128--1024GB). L2 transfer latency depends on expert size: $\sim$200$\mu$s for 6.3MB Switch experts, scaling to $\sim$12ms for 386MB Qwen experts at PCIe~4.0 bandwidth (32GB/s). Prophet's 3-layer prediction horizon (15--20ms lead time) hides this transfer by initiating prefetch before the expert is needed, achieving effective latencies of 200--500$\mu$s through overlap with ongoing computation. Cache capacity allocation follows the hot-cold distribution: L1 holds 20--50 high-confidence experts, sufficient to cover the power-law ``head'', while L2 provides comprehensive coverage for the long tail. Detailed transfer-compute overlap analysis is provided in \Cref{app:memoryCaching}.

\noindent\textbf{Distributed Deployment Benefits:} In pipeline-parallel and expert-parallel deployments where different GPUs handle different layers~\cite{expertaffinity, go2025moetuneroptimizedmixtureexpert}, expert locality benefits compound. Experts assigned to a GPU's layers exhibit higher temporal reuse within that GPU, reducing cache thrashing from other layers' experts. The power-law concentration ensures that each GPU's ``hot'' experts remain stable across sequences, improving per-GPU cache efficiency without requiring cross-GPU coordination for cache management. Further details on replacement policies, promotion strategies, and confidence calibration are provided in \Cref{app:memoryCaching}.


\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figs/ProphetDesign.pdf.pdf}
\caption{Overall design of Prophet: For every layer, we run the expert prediction (\circled{1}) which goes for the expert selection logic to find the expert in the cache (\circled{2a}). If a hit, the expert is loaded to the LLM (\circled{3}); otherwise, upon a miss, the expert is looked up in the host memory (\circled{2b}) and unique experts are selected (\circled{3b}) and loaded (\circled{4b}) to the GPU and LLM (\circled{3}).}
\vspace{-10pt}
\label{fig:overallDesign}
%\vspace{-20pt}
\end{figure}

\subsection{System Integration}
\label{subsec:integration}

\noindent\textbf{Component Orchestration.}
\Cref{fig:overallDesign} illustrates Prophet's component pipeline. The neural predictor (\Cref{subsec:neural-predictor}) processes routing history from previous $c=3$ layers to generate expert predictions with calibrated confidence scores for horizons $h \in \{1,2,3\}$. These predictions drive the hierarchical cache system (\Cref{subsec:caching}), which maps confidence scores to cache tier placement: high-confidence predictions ($>0.8$) target L1 (GPU) for immediate access, medium-confidence ($0.5$--$0.8$) target L2 (host memory) for speculative coverage, while low-confidence predictions trigger on-demand loading.

\noindent\textbf{Performance Breakdown.}
Prophet achieves $\sim$98\% effective hit rates despite 80--89\% top-1 prediction accuracy through three mechanisms: (1)~\textbf{Multi-horizon coverage}: predictions at $h \in \{1,2,3\}$ provide multiple opportunities---an expert missed at $h=3$ may be correctly predicted at $h=2$ or $h=1$; (2)~\textbf{Power-law caching}: the top 20\% most popular experts (accounting for 75\% of selections) remain in L1 cache across sequences, so mispredictions for popular experts still hit; (3)~\textbf{Confidence-tiered placement}: medium-confidence predictions (0.5--0.8) target L2 cache, providing speculative coverage that converts potential misses into L2 hits. The resulting $\sim$98\% hit rate decomposes into $\sim$85\% L1 hits (sub-$\mu$s) and $\sim$13\% L2 hits, with only $\sim$2\% cold misses (per-model breakdown in \Cref{app:memoryCaching}). The end-to-end speedup of $1.5\times$--$10.4\times$ depends on baseline I/O dominance: GPT-OSS (92\% I/O) achieves 10.4$\times$ because eliminating I/O dominates execution, while Qwen1.5-MoE (36\% I/O) achieves only 1.5$\times$ because compute already constitutes the majority of baseline time.

\noindent\textbf{Scalability.}
Prophet scales across several dimensions. \textit{Model size:} The predictor's 8.4M parameters remain constant regardless of target MoE model size; only the expert embedding dimension changes (128--256 experts). \textit{Prediction horizon:} While $h=3$ provides optimal accuracy-latency tradeoff for current hardware, the architecture supports longer horizons with graceful accuracy degradation (\Cref{fig:predictor_accuracy}); as memory bandwidth improves, shorter horizons may suffice. \textit{Multi-GPU deployment:} In pipeline-parallel settings where GPUs handle specific layer ranges, per-GPU predictors can specialize on their assigned layers, reducing prediction complexity while exploiting layer-specific routing patterns.

% \noindent\textbf{Hardware Evolution.}
% H100 GPUs increase HBM bandwidth to 3.35 TB/s, but PCIe 5.0 host-to-device bandwidth (64 GB/s bidirectional) remains the offloading bottleneck. Prophet's analysis suggests that as interconnect improves, shorter prediction horizons ($h=1$--$2$) may suffice, reducing predictor complexity. However, the fundamental I/O-compute imbalance persists for memory-constrained deployments: expert weights (GB-scale) dominate activated compute (MB-scale) regardless of interconnect generation, ensuring Prophet's predictive approach remains beneficial.

% \noindent\textbf{Continuous Batching Integration.}
% Prophet's prediction mechanism is compatible with continuous batching frameworks (vLLM, TGI) that dynamically schedule requests for throughput optimization. In continuous batching, new requests join ongoing batches at iteration boundaries. Prophet accommodates this by: (1) maintaining per-request routing history that persists across batch iterations, (2) updating predictions as new tokens generate, and (3) aggregating cache priorities across concurrent requests to maximize shared expert reuse. The power-law expert concentration (\Cref{sec:Motivation}) ensures that popular experts remain in L1 cache across request boundaries, reducing cold-start penalties for newly-joined requests.

\noindent\textbf{Flexibility.}
Prophet adapts automatically to diverse MoE architectures. For sparse routing (top-1), precision-focused prefetching with aggressive L1 caching is most effective. For dense routing (top-4 to top-8), coverage-oriented strategies with multi-expert predictions provide better returns. The plug-and-play design requires only: (1) one-time predictor training from routing traces (3.5--4.5 hours), and (2) cache tier configuration based on available memory. No modifications to pre-trained MoE model weights are needed---hidden states are accessed through standard inference framework APIs (e.g., HuggingFace \texttt{output\_hidden\_states=True}) or lightweight PyTorch forward hooks that intercept intermediate activations without altering model parameters or computation graphs.
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 