%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
\section{Design of Prophet}
\label{sec:design}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Prophet integrates two key components grounded in our information-theoretic analysis (\Cref{sec:Motivation}): a \textbf{neural expert predictor} that forecasts expert needs multiple layers ahead by exploiting cross-layer routing dependencies, and a \textbf{prediction-guided hierarchical cache} that maps prediction confidence to cache tier placement. Prophet operates transparently at inference time without requiring modifications to pre-trained MoE models.


\subsection{Neural Expert Prediction}
\label{subsec:neural-predictor}
The temporal correlations and layer-wise specialization patterns identified in our analysis motivate a neural sequence modeling approach. We employ a lightweight dense transformer to capture dependencies between routing decisions across multiple transformer layers, learning that tokens routed to certain experts in early layers are likely to require specific experts in deeper layers.

\noindent\textbf{Comparison with Prior Predictive Approaches:}
Existing systems employ cross-layer information through various mechanisms: FATE~\cite{fate} uses cross-layer gates for predictive scheduling, MoE-Infinity~\cite{moeinfinity} traces activations across layers, and AdapMoE~\cite{adapmoe} performs single-layer lookahead prefetching. However, these approaches are fundamentally \textit{heuristic}: they exploit cross-layer information without quantifying its predictive value or optimizing the prediction mechanism itself. \Cref{tab:predictor_comparison} summarizes key differences.

\design differs in three key aspects: (1)~\textbf{Information-theoretic grounding:} Our analysis in \Cref{sec:Motivation} establishes that 2.59 bits (63\%) of predictive information comes from the 3-layer expert history---providing principled justification for context window selection rather than ad-hoc tuning. (2)~\textbf{Learned sequence modeling:} Rather than fixed heuristics, we train a dense transformer to learn complex routing dependencies, achieving 86\% prediction accuracy compared to 12.3\% for frequency-based baselines. (3)~\textbf{Confidence-calibrated prefetching:} Prediction confidence scores enable tiered cache placement, maximizing GPU memory utilization for high-certainty predictions.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{System} & \textbf{Prediction Method} & \textbf{Context} & \textbf{Top-1 Acc.} \\
\midrule
ProMoE~\cite{promoe} & LRU-based & Current layer & $\sim$65\% \\
PreGated~\cite{pregated} & Router lookahead & 1 layer & $\sim$70\% \\
FATE~\cite{fate} & Cross-layer gates & Heuristic & $\sim$72\% \\
AdapMoE~\cite{adapmoe} & Single lookahead & 1 layer & $\sim$74\% \\
\midrule
\textbf{\design} & Neural transformer & 3 layers & \textbf{86\%} \\
\bottomrule
\end{tabular}
\vspace{-5pt}
\caption{Comparison with prior predictive approaches. \design's learned multi-layer context achieves 12--21\% higher prediction accuracy than heuristic methods. Baseline accuracies measured under identical conditions using our reimplementation on routing traces.}
\label{tab:predictor_comparison}
\vspace{-10pt}
\end{table}

%%%%%%%%%%%%
\subsubsection{Formalization and Architecture Overview}

We formalize expert prediction as a sequence modeling task that predicts future expert selections for individual token positions based on historical routing  (the collected traces as described in \Cref{sec:Motivation}) and current hidden states. For a specific token at sequence position $t$ currently being processed at layer $\ell$, our goal is to predict which expert will be selected for this same token at a future layer $\ell+h$, where $h$ represents the prediction horizon.


\noindent\textbf{Input Representation:} The predictor input $\mathcal{H}_t$ for token position $t$ comprises three components:
$\mathcal{H}_t = \{E_t^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)},$$H_t^{(\ell)}\} $
% \begin{align}
% \mathcal{H}_t &= \{E_t^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)}, H_t^{(\ell)}\} \label{eq:predictor_input}
% \end{align}
where $E_t^{(\ell-c+1:\ell)} = [e_t^{(\ell-c+1)}, e_t^{(\ell-c+2)}, \ldots, e_t^{(\ell)}]$ represents the expert routing history for token $t$ over the past $c$ layers, $L^{(\ell-c+1:\ell)} = [\ell-c+1, \ell-c+2, \ldots, \ell]$ encodes the layer positions within this context window, and $H_t^{(\ell)} \in \mathbb{R}^d$ denotes the current hidden state for token $t$ at layer $\ell$ that will be processed by future routers. While our information-theoretic analysis (\Cref{sec:Motivation}) identifies attention patterns as a potential information source (0.75 bits), hidden states $H_t^{(\ell)}$ already encode attention-relevant information through the transformer's residual connections---the hidden state at layer $\ell$ integrates all preceding attention computations, making explicit attention features redundant and simplifying the predictor architecture.
%\noindent\textbf{Prediction Target:} 
The predictor output is the predicted expert selection:
%\begin{align}
% 
$\hat{e}_t^{(\ell+h)} = \text{Predictor}(\mathcal{H}_t; \theta)$ 
%\label{eq:predictor_output}
%\end{align}
where $\theta$ represents the learnable parameters of the neural predictor model, including embedding matrices, transformer weights, and prediction head parameters.
%\noindent\textbf{Batch Processing:} 
For a batch of $B$ tokens
%$\{t_1, t_2, \ldots, t_B\}$ 
at layer $\ell$, we generate predictions for all tokens in the same positions simultaneously, enabling batch-level optimization and deduplication of predicted expert requirements.

\noindent\textbf{Context Window Selection ($c$):} The context window $c$ determines how many previous layers' routing decisions to incorporate. Our empirical analysis reveals that $c = 3$ layers provides optimal prediction accuracy while maintaining computational efficiency. Longer contexts ($c > 5$) yield diminishing returns due to the exponential decay of temporal correlations identified in \Cref{sec:Motivation}, while shorter contexts ($c < 2$) lack sufficient history for effective pattern learning.

\noindent\textbf{Prediction Horizon Selection ($h$):} The prediction horizon $h$ must provide sufficient lead time for expert prefetching while maintaining prediction accuracy. The optimal horizon depends on three factors: (1) expert loading latency determined by expert size and memory bandwidth, (2) non-expert computation time available for hiding I/O latency, and (3) prediction accuracy degradation with increasing horizon. Critically, the predictor outputs predictions for horizons $h \in \{1, 2, 3\}$ simultaneously rather than a single fixed horizon. This multi-horizon capability enables the caching system to orchestrate prefetching across memory tiers: immediate predictions ($h=1$) target L1 cache for experts needed imminently, while longer horizons ($h=2,3$) enable proactive loading from L2 cache or host memory for experts further ahead in the execution pipeline. For typical MoE models (e.g., 386MB Qwen experts), $h = 3$ provides 15--20ms lead time, sufficient to hide expert loading behind computation while maintaining high prediction accuracy.

\noindent\textbf{Architecture Design:} Our predictor employs a dense transformer architecture specifically designed for cross-layer routing prediction. The architecture avoids using MoE models to predict MoE routing, eliminating recursive complexity while enabling efficient pattern learning. Expert identifiers are embedded into dense representations through learned embedding matrices $\mathbf{W}_E \in \mathbb{R}^{N_E \times d}$, where $N_E$ denotes the expert vocabulary size (128 for Switch Transformer, 64 for Qwen1.5 MoE, 256 for Qwen3 MoE) and $d = 320$ represents the embedding dimension chosen to balance representational capacity with computational efficiency.

\noindent\textbf{Layer Position Encoding:} To capture the hierarchical structure of transformer processing, we employ layer position encodings that enable the model to understand the significance of routing decisions at different depths. Each layer index $\ell$ in the context window is mapped to a learned embedding vector $\mathbf{W}_L[\ell] \in \mathbb{R}^d$, allowing the predictor to weight routing patterns differently based on their layer positions. Early layers typically exhibit syntactic routing patterns while deeper layers capture semantic dependencies, and our layer position encodings enable the model to automatically learn these layer-specific characteristics.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/pipeline.drawio.pdf}
\caption{Neural expert predictor scheduling, illustrating routing history from layers $\ell-2$ to $\ell$ combined with current hidden states enables prediction of experts at layer $\ell+3$. 
}
\label{fig:predictor_architecture}
\end{figure}

\noindent\textbf{Toy Example:} Consider predicting expert selection for token $t$ at layer 8 while currently processing layer 6 (\Cref{fig:predictor_architecture}). The predictor input includes: expert IDs selected for token $t$ at layers 4, 5, and 6 (routing history $E_t^{(4:6)}$), layer position encodings $L^{(4:6)} = [4, 5, 6]$, and the hidden state $H_t^{(6)}$ that will be processed by the layer 8 router. The prediction $\hat{e}_t^{(8)}$ identifies which expert token $t$ will likely require at layer 8, enabling prefetching with a 3-layer horizon that provides sufficient lead time for expert loading.
This formalization enables systematic optimization of expert prefetching through neural sequence modeling while maintaining compatibility with existing pre-trained MoE models and providing theoretical guarantees on prediction performance through the information-theoretic bounds established in \Cref{sec:Motivation}.
%%%%%%%%%%%%

\subsubsection{Neural Predictor Architecture and Training}
Building on the problem formalization above, we now detail the neural predictor architecture and training methodology that realizes the expert prediction framework. Our predictor employs a dense transformer architecture specifically optimized for cross-layer routing prediction, avoiding the recursive complexity of using MoE models to predict MoE routing while enabling sophisticated pattern learning through multi-head self-attention mechanisms.

\noindent\textbf{Architecture Specification:} The predictor comprises 4 transformer layers with 10 attention heads each, utilizing model dimension $d_{\text{model}} = 320$ and feed-forward dimension $d_{ff} = 1280$. 
The architecture parameters were determined through extensive ablation studies balancing prediction accuracy with computational efficiency, ensuring the predictor overhead remains below 1\% of expert execution time.

\noindent\textbf{Input Processing and Embeddings:} Following the input formulation $\mathcal{H}_t = \{E_t^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)}, H_t^{(\ell)}\}$, the architecture processes each component through specialized embedding layers. Expert identifiers $E_t^{(\ell-c+1:\ell)}$ are mapped to dense representations via learned embedding matrix $\mathbf{W}_E \in \mathbb{R}^{N_E \times d_{\text{model}}}$, where each expert ID receives its own 320 dimensional representation. 
The layer position encodings 
are processed through a learned embedding matrix. This learned encoding enables the model to understand the hierarchical significance of routing decisions at different transformer depths, automatically learning that early layers capture syntactic patterns while deeper layers encode semantic dependencies. The hidden states $H_t^{(\ell)}$ undergo linear projection to match the model dimension before being combined with the embedded routing history and position encoding.

\noindent\textbf{Training Data Collection:} We collect expert routing traces from pre-trained MoE models processing six diverse datasets (\Cref{sec:Motivation}): Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText. This diversity ensures the predictor learns model-intrinsic routing patterns rather than dataset-specific features. For each forward pass, we record expert selections and hidden states at every layer for each token position.

\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figs/switch_prediction_accuracy.pdf}
\vspace{-10pt}
\caption{Predictor accuracy vs batch size for Switch transformer against the state-of-the-art. We observe a rapid decline is the accuracy beyond the prediction horizon, h = 3.}
\label{fig:predictor_accuracy}
%\vspace{-20pt}
\end{figure}

\noindent\textbf{Composite Loss Function and Training Objectives:} The model training employs a composite loss function that simultaneously optimizes prediction accuracy, ranking consistency, and confidence calibration:
$\mathcal{L}_{\text{total}} = \lambda_1 \mathcal{L}_{\text{classification}} + \lambda_2 \mathcal{L}_{\text{ranking}} + \lambda_3 \mathcal{L}_{\text{confidence}}$. 
The primary classification loss uses cross-entropy to optimize expert prediction accuracy. The auxiliary ranking loss encourages correct relative ordering of expert selection probabilities, ensuring that the model not only predicts the most likely expert but also maintains meaningful probability rankings for alternative experts. The confidence calibration loss employs the Brier score to ensure that prediction confidence scores correlate with actual accuracy, enabling reliable dynamic prefetching decisions. 

\noindent\textbf{Optimization and Hyperparameter Selection:} Training employs the AdamW optimizer with $\beta_1 = 0.9$, $\beta_2 = 0.999$, weight decay of 0.01, and a learning rate schedule combining linear warmup over 500 steps followed by cosine annealing. The model achieves convergence within 15,000 training steps, typically requiring 3.5--4.5 hours on a single NVIDIA A100 GPU depending on the target MoE architecture and dataset size.
Through systematic hyperparameter exploration across context windows $c \in \{1, 2, 3, 4, 5\}$ and prediction horizons $h \in \{1, 2, \dots 10\}$, we empirically determine that $c = 3$ layers and $h = 3$ layers provide optimal performance (\Cref{fig:predictor_accuracy}). The 3-layer context window captures sufficient routing history to identify emerging patterns without excessive computational overhead or overfitting to noise. The 3-layer prediction horizon balances prediction accuracy with pre-fetching constraints: longer horizons provide more pre-fetching lead time but suffer from exponentially decreasing accuracy due to accumulated uncertainty, while shorter horizons offer higher accuracy but insufficient time for hiding memory transfer latency behind computation.
Most importantly, \Cref{fig:predictor_accuracy} shows that our predictor model is more accurate in predicting the experts for future layers compared to state-of-the-art models (ProMoE~\cite{promoe} and PreGated MoE~\cite{pregated}).

\noindent\textbf{Prediction Accuracy Definition:}
We define prediction accuracy as \textit{top-1 per-token-per-layer} accuracy: for each token $t$ at prediction horizon $h$, we check whether the predicted expert $\hat{e}_t^{(\ell+h)}$ matches the ground-truth expert selection $e_t^{(\ell+h)}$. This strict metric directly corresponds to prefetch utility---correct predictions enable cache hits, while incorrect predictions waste bandwidth. Across our evaluation, \design achieves 81--87\% top-1 accuracy across the five target architectures at horizon $h=3$, compared to 0.78--1.56\% random baselines---over $50\times$ improvement.

\noindent\textbf{Cross-Domain Generalization:}
A critical question is whether the predictor overfits to training domains or learns transferable routing patterns. We validate generalization by training on traces from a subset of domains and evaluating on held-out domains (\Cref{fig:cross_domain_transfer}). Results show prediction accuracy remains 67--98\% across all 36 train-test domain pairs, with only 3--8\% accuracy degradation compared to in-domain evaluation. This confirms that \design learns model-intrinsic routing structure---how transformer layers progressively transform representations---rather than dataset-specific lexical patterns. This robustness eliminates the need for per-workload predictor retraining.

\noindent\textbf{Predictor Overhead:} The 8.4M-parameter predictor introduces minimal overhead: $<$1\% latency (0.8ms prediction vs. 15--50ms MoE forward pass) and $<$2\% memory (34MB vs. 3.2--12.8GB MoE parameters). Training requires 3.5--4.5 hours on a single A100 and amortizes across all subsequent inference runs. Detailed overhead breakdown is provided in \Cref{subsec:predictor}.

% \subsection{Batch-Aware Optimization}
% \label{subsec:batch-optimization}
% \subsubsection{Expert Deduplication Strategy}
% The power-law distribution of expert popularity creates significant opportunities for batch-level optimization that prior single-token approaches cannot exploit. The concentration effects we identified, where the most popular experts handle a disproportionate fraction of routing decisions, suggest that multiple tokens in a batch will frequently request overlapping sets of experts (\Cref{fig:batch_scaling_validation}). Our deduplication algorithm, as explained in \Cref{alg:deduplication} in \Cref{app:memoryCaching}, and illustrated in \Cref{fig:batch-deduplication} exploits this pattern by identifying unique experts across all batch items and computing an optimal loading schedule that prioritizes high-frequency experts.
% The algorithm reduces the total number of expert loads from $O(B \cdot k)$ for naive per-item loading to $O(B^{\alpha} \cdot k)$ where $\alpha = 0.932 < 1$, providing sublinear scaling that becomes increasingly beneficial at larger batch sizes. This theoretical guarantee stems directly from the power-law scaling property, which we empirically validated, ensuring predictable performance improvements as batch sizes increase.

% \subsubsection{Hardware-Aware Transfer Scheduling}
% Beyond deduplication, we implement sophisticated transfer mechanisms that pipeline expert loading with computation to hide memory bandwidth bottlenecks. Rather than loading all experts before computation begins, we divide the transfers into chunks that can overlap with the ongoing computation.
% This pipelining reduces effective loading latency by hiding transfer costs behind computation, a technique particularly effective when combined with our predictive prefetching that provides sufficient lead time for memory operations. The scheduler incorporates hardware-specific bandwidth constraints to prevent memory bottlenecks through utility-based optimization that combines prediction confidence, expert frequency, and recency. Using prediction confidence scores, we initiate transfers for high-confidence predictions before routing decisions are finalized, with the confidence threshold dynamically adjusted based on available bandwidth and cache capacity to ensure the system adapts to varying hardware capabilities.

% \begin{figure}[t]
% \centering
% \includegraphics[width=0.7\linewidth]{figs/DeDuplication.pdf}
% \caption{Expert deduplication before transferring to the GPU memory reduces bandwidth requirements significantly.}
% \label{fig:batch-deduplication}
% \vspace{-10pt}
% \end{figure}

\subsection{Prediction-Guided Expert Caching}
\label{subsec:caching}

Building on established expert caching techniques~\cite{moeinfinity,promoe,prescope}, Prophet adapts hierarchical caching to leverage its neural predictor's calibrated confidence scores. The key insight is that prediction confidence directly indicates cache placement priority: high-confidence predictions ($>$0.8) warrant L1 (GPU) placement for immediate access, while medium-confidence (0.5--0.8) predictions target L2 (host memory) for speculative coverage. This prediction-driven placement contrasts with reactive policies (LRU/LFU) that cannot anticipate future expert needs.

The two-tier hierarchy maps naturally to GPU-host memory organization: L1 cache utilizes GPU memory (24--80GB HBM) with sub-microsecond access, while L2 cache leverages host memory (128--1024GB) with PCIe transfer latency ($\sim$200--500$\mu$s). Cache capacity allocation follows the hot-cold distribution: L1 holds 20--50 high-confidence experts---sufficient to cover the power-law ``head''---while L2 provides comprehensive coverage for the long tail.

\noindent\textbf{Distributed Deployment Benefits:} In pipeline-parallel and expert-parallel deployments where different GPUs handle different layers~\cite{expertaffinity, moetuner}, expert locality benefits compound. Experts assigned to a GPU's layers exhibit higher temporal reuse within that GPU, reducing cache thrashing from other layers' experts. The power-law concentration ensures that each GPU's ``hot'' experts remain stable across sequences, improving per-GPU cache efficiency without requiring cross-GPU coordination for cache management. Further details on replacement policies, promotion strategies, and confidence calibration are provided in \Cref{app:memoryCaching}.


\begin{figure}[t]
\centering
\includegraphics[width=0.9\linewidth]{figs/ProphetDesign.pdf.pdf}
\caption{Overall design of Prophet: For every layer, we run the expert prediction (\circled{1}) which goes for the expert selection logic to find the expert in the cache (\circled{2a}). If a hit, the expert is loaded to the LLM (\circled{3}); otherwise, upon a miss, the expert is looked up in the host memory (\circled{2b}) and unique experts are selected (\circled{3b}) and loaded (\circled{4b}) to the GPU and LLM (\circled{3}).}
\vspace{-10pt}
\label{fig:overallDesign}
%\vspace{-20pt}
\end{figure}

\subsection{System Integration}
\label{subsec:integration}

\noindent\textbf{Component Orchestration.}
\Cref{fig:overallDesign} illustrates Prophet's component pipeline. The neural predictor (\Cref{subsec:neural-predictor}) processes routing history from previous $c=3$ layers to generate expert predictions with calibrated confidence scores for horizons $h \in \{1,2,3\}$. These predictions drive the hierarchical cache system (\Cref{subsec:caching}), which maps confidence scores to cache tier placement: high-confidence predictions ($>0.8$) target L1 (GPU) for immediate access, medium-confidence ($0.5$--$0.8$) target L2 (host memory) for speculative coverage, while low-confidence predictions trigger on-demand loading.

\noindent\textbf{Performance Breakdown.}
Prophet achieves $\sim$98\% effective hit rates, decomposing into $\sim$85\% L1 hits (sub-$\mu$s access) and $\sim$13\% L2 hits ($\sim$200$\mu$s PCIe transfer). The remaining $\sim$2\% are cold misses (2--5ms). While both L1 and L2 hits avoid cold-miss penalties, L2 hits still incur transfer overhead, explaining why high hit rates do not translate linearly to speedup. The end-to-end speedup of $1.5\times$--$10.4\times$ further depends on baseline I/O dominance: GPT-OSS (92\% I/O) achieves 10.4$\times$ because eliminating I/O dominates execution, while Qwen1.5-MoE (36\% I/O) achieves only 1.5$\times$ because compute already constitutes the majority of baseline time.

\noindent\textbf{Scalability.}
Prophet scales across several dimensions. \textit{Model size:} The predictor's 8.4M parameters remain constant regardless of target MoE model size; only the expert embedding dimension changes (128--256 experts). \textit{Prediction horizon:} While $h=3$ provides optimal accuracy-latency tradeoff for current hardware, the architecture supports longer horizons with graceful accuracy degradation (\Cref{fig:predictor_accuracy}); as memory bandwidth improves, shorter horizons may suffice. \textit{Multi-GPU deployment:} In pipeline-parallel settings where GPUs handle specific layer ranges, per-GPU predictors can specialize on their assigned layers, reducing prediction complexity while exploiting layer-specific routing patterns.

% \noindent\textbf{Continuous Batching Integration.}
% Prophet's prediction mechanism is compatible with continuous batching frameworks (vLLM, TGI) that dynamically schedule requests for throughput optimization. In continuous batching, new requests join ongoing batches at iteration boundaries. Prophet accommodates this by: (1) maintaining per-request routing history that persists across batch iterations, (2) updating predictions as new tokens generate, and (3) aggregating cache priorities across concurrent requests to maximize shared expert reuse. The power-law expert concentration (\Cref{sec:Motivation}) ensures that popular experts remain in L1 cache across request boundaries, reducing cold-start penalties for newly-joined requests.

\noindent\textbf{Flexibility.}
Prophet adapts automatically to diverse MoE architectures. For sparse routing (top-1), precision-focused prefetching with aggressive L1 caching is most effective. For dense routing (top-4 to top-8), coverage-oriented strategies with multi-expert predictions provide better returns. The plug-and-play design requires only: (1) one-time predictor training from routing traces (3.5--4.5 hours), and (2) cache tier configuration based on available memory. No modifications to pre-trained MoE models are needed.
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 