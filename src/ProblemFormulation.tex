\subsubsection{Problem Formalization and Architecture Overview}
We frame expert prediction as a sequence modeling task: for each token at position \(t\), currently being processed at layer \(\ell\), our aim is to predict which expert will be invoked at layer \(\ell + h\), leveraging the token’s historical routing trace and current hidden state. Formally, given the history of expert selections \(E_t^{(\ell-c+1:\ell)}\), layer indices \(L^{(\ell-c+1:\ell)}\), and the hidden representation \(H_t^{(\ell)}\), we define the predictor input as:
\[
\mathcal{H}_t = \{E_t^{(\ell-c+1:\ell)}, L^{(\ell-c+1:\ell)}, H_t^{(\ell)}\}.
\]
The predictor then estimates the future routing decision:
\[
\hat{e}_t^{(\ell + h)} = \mathrm{Predictor}(\mathcal{H}_t; \theta),
\]
where \(\theta\) encompasses the learnable parameters of our dense transformer-based model, including expert and layer embeddings, transformer weights, and the final prediction head.

Key hyperparameters — the context window \(c\) and prediction horizon \(h\) — are chosen to balance prediction accuracy with practical latency needs. Empirically, \(c = 3\) grants optimal performance without undue computational cost, reflecting the exponential decay of temporal correlations described earlier. A horizon of \(h = 3\) layers offers a practical 15–20 ms prediction lead time, sufficient to hide I/O latency for both Switch Transformer and Qwen MoE configurations.

Our predictor architecture employs learned embeddings for both expert identities and layer positions. Expert identifiers are mapped via \(\mathbf{W}_E \in \mathbb{R}^{N_E \times d}\), where \(N_E\) corresponds to the number of experts (e.g., 128 in Switch Transformer, 64 or 256 in Qwen variants). Layer indices are similarly embedded via \(\mathbf{W}_L[\ell] \in \mathbb{R}^d\), enabling the model to differentially weight contributions from syntactic shallow layers compared to semantically rich deeper layers. These layer position encodings aid the model in capturing hierarchical routing behaviors without hand-engineering, aligning with observations from recent studies on the spatial structure of MoE routing.

When handling a batch of \(B\) tokens at layer \(\ell\), our predictor processes all inputs in parallel, enabling batch-level de-duplication by identifying redundant expert requests across tokens. \Cref{tab:predictor-performance} summarizes the predictor's characteristics and performance: it consumes approximately 8.4 M parameters, trains in a few hours per architecture, and achieves prediction accuracies exceeding 80%, far outperforming the random baseline and maintaining minimal inference latency and memory overhead.

A toy example clarifies operation: when forecasting expert selection for token \(t\) at layer 8 while currently at layer 6, the predictor ingests routing history for layers 4–6, corresponding layer embeddings \([4,5,6]\), and the hidden state from layer 6. It outputs \(\hat{e}_t^{(8)}\), enabling proactive prefetching with a two-layer horizon.
Overall, this formalization transforms expert prefetching into a systematic, theory-informed optimization task. It aligns with information-theoretic insights from \Cref{sec:Motivation}, preserves compatibility with pre-trained MoE models, and provides a transparent, end-to-end mechanism for improving inference efficiency across MoE architectures.
