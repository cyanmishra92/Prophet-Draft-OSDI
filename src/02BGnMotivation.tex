%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
\section{Background and Motivation}
\label{sec:background}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 

\subsection{Current Bottlenecks in MoE}
LLMs are built from transformer layers (\Cref{fig:moe_architecture}(a)), each consisting of an attention mechanism (with linear projections), normalization layers, and a position-wise feedforward network, all connected by residual pathways. Classically, the feedforward network of a transformer block, in most LLM architectures, is a dense MLP block (\Cref{fig:moe_architecture}(b)). In contrast, MoE models modify this design (depicted in \Cref{fig:moe_architecture}(c),(d)) by replacing the monolithic feedforward MLP with a large set of expert networks and a learned router that activates only a sparse subset of them for each token, thus increasing model capacity without a proportional increase in computational cost. Consequently, for an input token $\mathbf{x}^{(l)}$ in layer $l$, the router computes the expert selection probabilities through $G^{(l)}(\mathbf{x}^{(l)}) = \text{softmax}(\mathbf{W}_g^{(l)} \cdot \mathbf{x}^{(l)} + \mathbf{b}_g^{(l)})$, where $\mathbf{W}_g^{(l)}$ and $\mathbf{b}_g^{(l)}$ are learnable parameters. The routing network essentially learns which experts are most relevant for each input token based on its learned representations. The routing mechanism selects the top-$k$ experts with the highest probabilities, forming $\mathcal{S}_{\text{selected}} = \text{top-k}(G^{(l)}(\mathbf{x}^{(l)}))$. The final output combines the outputs of selected experts weighted by their gating scores: $\mathbf{y}^{(l)} = \sum_{i \in \mathcal{S}_{\text{selected}}} G^{(l)}_i(\mathbf{x}^{(l)}) \cdot E_i^{(l)}(\mathbf{x}^{(l)})$, where each expert network $E_i^{(l)}$ processes tokens within its specialized domain, and the weighted combination dynamically blends the outputs of experts based on their relevance.

Modern MoE models exhibit significant architectural diversity in their routing strategies. Sparse routing approaches, such as Switch Transformer~\cite{fedus2022switch}, employ top-1 (\Cref{fig:moe_architecture}(c)) expert selection, activating only 1 out of 128 experts per token. Dense routing architectures like Qwen1.5-MoE-A2.7B~\cite{bai2023qwen} employ 64 experts per MoE layer, comprising 4 shared always-active experts along with 4 out of 60 routing experts selected by the top-k router (8 experts per token), whereas Qwen3-235B-A22B~\cite{yang2025qwen3} scales this approach up to 128 experts with 8 activated per token (\Cref{fig:moe_architecture}(d));
architectural details are provided in \Cref{appedndix:MoEModels}.

\begin{figure}[]
\centering
\includegraphics[width=\linewidth]{figs/LLM-MoE-Arch.pdf}
\caption{LLM architecture comparison (a) Generic LLMs represented as layers of transformer blocks (b) inside view of a traditional dense LLM layer, (c) inside view of a MoE layer with top-1 routing (e.g. SwitchTransformer), and (d) MoE layer with top-k routing and shared experts where Expert-4 is always activated (e.g. Qwen1.5-MoE). The routing mechanism determines expert selection strategies and creates fundamentally different optimization challenges.}
\label{fig:moe_architecture}
%\vspace{-10pt}
\end{figure}

Despite computational efficiency advantages, MoE models suffer from severe memory and bandwidth bottlenecks driven by high I/O-to-compute ratios. For example, transferring a 6.3\,MB Switch Transformer~\cite{fedus2022switch} expert over PCIe~4.0 takes $197\,\mu$s versus $15\,\mu$s for computation ($13\times$ slower), while loading a 386\,MB Qwen1.5-MoE-A2.7B~\cite{bai2023qwen} expert requires 12\,ms versus 0.8\,ms ($15\times$ slower). Our experiments on both sparse and dense routing models using diverse LLM benchmarks spanning question answering, reasoning, and language modeling suggest that I/O dominates 78--96\% of execution time (creating an execution bottleneck, as shown in \Cref{fig:io_bottleneck}) across different batch sizes. Critically, expert parameters constitute the dominant memory component: 64--88\% of total memory in typical MoE deployments, compared to 5--20\% for KV cache and 7--17\% for non-expert parameters.

\noindent\textbf{Expert Parameters vs KV Cache:}
 Memory composition varies with sequence length and batch configuration. Expert parameters are fixed per model (Switch Transformer: $128 \times 6.3$MB $= 806$MB; Qwen1.5-MoE: $64 \times 386$MB $\approx 24$GB), while the KV cache scales as $2 \cdot L \cdot d \cdot B \cdot T$ with layers $L$, hidden dimension $d$, batch size $B$, and sequence length $T$. For Switch Transformer with batch size 8 and sequence length 256, the KV cache occupies $\sim$200MB (expert parameters are $4\times$ larger). For Qwen1.5-MoE, this ratio reaches $15\times$. Only at extreme sequence lengths ($>$16K tokens) does KV cache approach expert parameter size.

Crucially, \design targets the \textit{I/O bottleneck} (expert loading latency), not memory capacity. The KV cache resides persistently on the GPU during inference and does not require prediction-driven prefetching. Expert parameters, in contrast, must be loaded from host memory on-demand when GPU memory is constrained, creating the $13\times$--$15\times$ I/O penalties that dominate execution time. This I/O-centric bottleneck justifies our focus on expert prefetching over KV cache management --a design choice complementary to KV cache compression techniques~\cite{kvcache_survey} and distinguishes \design from approaches targeting dense model inference.

\subsection{The Case for Prediction}
Severe I/O bottlenecks motivate proactive expert loading strategies that can hide memory transfer latency behind computation~\cite{pregated}. Rather than waiting for routing decisions to materialize, intelligent prefetching can preload experts based on predicted future needs, potentially eliminating the $13\times$--$15\times$ I/O penalties that currently dominate execution time. However, effective prefetching requires understanding whether expert routing patterns contain an \textbf{exploitable structure}. Clearly, if routing decisions were truly random, prediction-based prefetching would not provide any benefit over simple caching strategies. Existing studies on strategies such as predictive pre-fetching~\cite{bambhaniya2024moeeras, pregated,doucet2025harmoeny} and  expert caching~\cite{promoe, moeinfinity, zhong2025hybrimoe} do not offer a theoretical or empirical justification for their chosen methods, lacking a clear understanding of why \emph{predictive} pre-fetching is necessary or how to optimally design such predictors; this absence of foundational insights motivates our comprehensive analysis of expert routing patterns in real-world MoE deployments across diverse datasets.

%%%%%

\section{Information Theoretic Analysis} 
\label{sec:Motivation} 
To rigorously establish the feasibility of intelligent expert prefetching, we conducted a comprehensive analysis of expert routing patterns across six benchmark datasets spanning diverse domains: Natural Questions~\cite{googleNatural} (factual QA), GSM8K~\cite{gsm8k} (mathematical reasoning), HumanEval~\cite{humaneval} (code generation), WikiText~\cite{wikitext} (language modeling), Alpaca~\cite{alpaca} (instruction following), and SQuAD~\cite{squad} (reading comprehension). This diversity, ranging from structured math problems to open-ended instructions, provides a rigorous test of whether routing patterns are truly model-intrinsic rather than dataset-specific. With the context window constrained to 256 tokens, we exhaustively logged, for each input sequence, the expert-routing decision at every decoded token, thereby yielding 37,200 execution traces and approximately 3.34 million expert selections across mixture-of-experts models in the Switch Transformer and Qwen-MoE families. Our findings uncover fundamental structural patterns that existing methods fail to exploit jointly, offering strong motivation for neural predictor approaches over simple frequency-based baselines. 

\begin{figure*}[t]
  \centering
  \includegraphics[width=\linewidth]{figs/fig_powerlaw_combined.pdf}
  \vspace{-15pt}
  \caption{Power-law analysis across six MoE architectures. \textbf{(Left panels)} Rank-frequency distributions showing expert popularity follows power-law with exponent $\alpha = 1.2$--$2.2$ across models. Higher $\alpha$ (Qwen-30B, GPT-OSS) indicates stronger concentration where fewer experts dominate. \textbf{(Right panels)} Layer-expert activation heatmaps for Qwen-30B (128 experts) and DeepSeek (64 experts) showing cross-layer routing patterns that enable prediction.}
  \label{fig:power_law}
\end{figure*}

% \begin{figure*}[t]
%   \centering
%   \begin{subfigure}[b]{0.68\linewidth}
%     \includegraphics[width=\linewidth]{figs/fig_powerlaw_combined.pdf}
%     \vspace{-15pt}
%     \caption{Power-law analysis across six MoE architectures. \textbf{(Left)} Rank-frequency distributions showing expert popularity follows power-law with $\alpha = 1.2$--$2.2$. \textbf{(Right)} Layer-expert activation heatmaps showing cross-layer patterns.}
%     \label{fig:power_law}
%   \end{subfigure}
%   \hfill
%   \begin{subfigure}[b]{0.30\linewidth}
%     \includegraphics[width=\linewidth]{figs/cross_domain_transfer.pdf}
%     \vspace{-15pt}
%     \caption{Zero-shot cross-domain transfer on DeepSeek-MoE (67--98\% accuracy). \textbf{Domains:} D1=NQ, D2=SQuAD, D3=GSM8K, D4=HumanEval, D5=Alpaca, D6=WikiText.}
%     \label{fig:cross_domain_transfer}
%   \end{subfigure}
%   \vspace{-5pt}
%   \caption{Expert routing analysis. (a) Power-law distributions reveal heavy-tailed expert popularity enabling prefetch strategies. (b) Cross-domain transfer matrix demonstrates model-intrinsic routing patterns---accuracy remains high across all 36 domain pairs, confirming Prophet learns architectural structure rather than dataset-specific features.}
%   \label{fig:routing_analysis}
% \end{figure*}

\subsection{Heavy-Tailed Expert Popularity}
To assess whether experts exhibit ``hot'' and ``cold'' behavior across layers, we conducted a power-law analysis on ranked expert usage~\cite{ghorbani2022scaling}. The ranking across experts by selection frequency and the fitting across a power-law model reveal that expert selection follows a power-law distribution with exponent $\alpha = 1.2$--$2.2$ across six MoE architectures (\Cref{fig:power_law}). Models with higher $\alpha$ (Qwen-30B: $\alpha=2.1$, GPT-OSS: $\alpha=2.2$) exhibit a higher concentration, while lower $\alpha$ (Qwen-2.7B, DeepSeek: $\alpha=1.2$) indicates a more diverse expert usage. This heavy-tailed behavior results in extreme concentration: the top 20\% of experts account for up to 75\% of all routing decisions.
The power-law structure has two implications. First, it enables universal prefetching strategies that target consistently popular experts. Second, for throughput-oriented batched inference, power-law concentration enables batch-aware deduplication---unique experts scale sublinearly with batch size ($\mathbb{E}[U(B)] \sim B^{0.932}$), reducing bandwidth requirements. We exploit this property in batched evaluation.% (\Cref{sec:implementation}).

Crucially, however, simple frequency-based methods fall short: due to temporal fluctuations and layer-specific specialization in expert popularity, naive frequency ranking yields only $\sim$12.3\% prediction accuracy, underscoring the need for cross-layer pattern modeling. The power-law exponent $\alpha$ also provides insight into scalability: higher $\alpha$ indicates stronger concentration where prediction is easier, while lower $\alpha$ suggests more diverse expert usage. Even at lower $\alpha$ values ($\alpha=1.2$ for DeepSeek), the cross-layer dependencies captured in the following section provide sufficient predictability. For emerging architectures activating more experts (e.g., top-12), we expect similar patterns to hold as long as routing decisions remain correlated across layers---a property inherent to hierarchical transformer processing. 

\subsection{Cross-Layer Routing Dependencies}
Beyond popularity patterns, we examine whether expert routing exhibits temporal structure across layers. Using mutual information (MI) analysis, we quantify how much knowing recent routing history reduces uncertainty about future expert selections. We observe: pairwise mutual information between adjacent layers is $I(E^{(\ell)}; E^{(\ell+1)}) = 0.62$ bits---the reduction in uncertainty about layer $\ell+1$'s expert given knowledge of layer $\ell$'s selection alone. However, this single-layer signal is only part of the picture; when we expand to incorporate a 3-layer context window with hidden states (detailed below), the total exploitable information increases substantially.

\noindent\textbf{Uniformity Across Layer Positions.}
A natural question is whether cross-layer dependencies vary by layer position---i.e., whether early, middle, and late layers exhibit fundamentally different routing behaviors that would motivate layer-group-specific predictors. Our analysis reveals the opposite: mutual information between layers $(\ell, \ell+h)$ remains stable regardless of whether $\ell$ falls in early, middle, or late network positions. This uniformity suggests that routing patterns reflect the progressive transformation of token representations---a consistent architectural mechanism rather than layer-position-specific behaviors. Consequently, a unified predictor that captures network-wide dependencies is both simpler and more principled than fragmented layer-group designs.

Prior systems such as FATE~\cite{fate} and AdapMoE~\cite{adapmoe} employ cross-layer information through heuristic gates or single-layer lookahead. However, these approaches lack empirical grounding for their design choices. Our information-theoretic analysis reveals \textit{why} cross-layer context matters and \textit{how much} information each source provides.

\noindent\textbf{Estimation Methodology.} MI values are computed via 
%plug-in 
entropy estimators over 37,200 execution traces ($\sim$3.34M expert selections) with Miller-Madow bias correction. Bootstrap resampling (1000 iterations) yields 95\% confidence intervals; the pairwise MI estimate is $I(E^{(\ell)}; E^{(\ell+1)}) = 0.62 \pm 0.04$ bits.

\noindent\textbf{Information Budget.} For a model with 128 experts, maximum entropy is 7.00 bits. When we expand beyond pairwise MI to incorporate a 3-layer context window with hidden states, our analysis reveals 4.11 bits (59\%) are exploitable for prediction, decomposed as:% follows:
% \begin{itemize}[leftmargin=*, nosep]
    % \item 
    \textbf{Expert context from preceding 3 layers (2.59 bits, 63\%):} The sequence of recently-selected experts provides the dominant predictive signal. When a token routes through experts $e_1 \rightarrow e_2 \rightarrow e_3$ across layers, this trajectory strongly constrains which experts will be selected next.
    %\item 
    \textbf{Hidden states (0.76 bits, 18\%):} Token representations carry routing-relevant information that correlates with expert selection.
    %\item 
    \textbf{Attention patterns (0.75 bits, 18\%):} Attention weights reflect semantic structure that influences routing decisions.
% \end{itemize}
The temporal structure exhibits layer-dependent specialization: early layers show syntactic correlations with surface-level patterns, while deeper layers exhibit semantic dependencies spanning longer contexts~\cite{liu2024fantastic, rajbhandari2022deepspeed, zhang2024harder}. This hierarchical nature motivates prediction architectures that capture dependencies across multiple layers simultaneously.



\subsection{Implications for Predictor Design}
The information budget reveals that 63\% of exploitable information (2.59 of 4.11 bits) comes from the \textit{sequential pattern of expert selections} across layers, making expert prediction fundamentally a \textbf{sequence modeling problem}. This insight motivates our transformer-based predictor design (\Cref{sec:design}), which encodes expert history, learns cross-layer dependencies through self-attention, and predicts multiple layers ahead ($h=3$) to provide prefetch lead time.

Critically, these routing patterns are \textbf{model-intrinsic} rather than dataset-dependent. To validate this, we evaluate cross-domain generalization across six diverse domains: Natural Questions (D1), SQuAD (D2), GSM8K (D3), HumanEval (D4), Alpaca (D5), and WikiText (D6).

\begin{wrapfigure}{r}{0.4\linewidth}
  \vspace{-12pt}
  \centering
  \includegraphics[width=\linewidth]{figs/cross_domain_transfer.pdf}
  \vspace{-15pt}
  \caption{Zero-shot cross-domain transfer on DeepSeek-MoE (67--98\% accuracy).}
  \label{fig:cross_domain_transfer}
  \vspace{-10pt}
\end{wrapfigure}
\Cref{fig:cross_domain_transfer} visualizes the complete $6\times6$ transfer matrix for DeepSeek-MoE: prediction accuracy remains consistently high (67--98\%) across all 36 train-test domain pairs. Even the most challenging transfers (e.g., training on code and testing on math) achieve over 70\% accuracy. 
This robustness confirms that \design learns the model's internal routing structure---how transformer layers progressively transform representations---rather than dataset-specific lexical or semantic patterns.
We provide comprehensive cross-domain evaluation across all five MoE architectures in \Cref{sec:implementation}, demonstrating that this model-intrinsic property holds broadly. The information-theoretic characterization established here therefore applies across diverse workloads, enabling deployment without per-dataset retraining.
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 