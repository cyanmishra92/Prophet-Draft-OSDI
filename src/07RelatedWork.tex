%\vspace{-10pt}
\section{Related Work} 
\label{sec:related} %Dee

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 


%\textcolor{red}{Related works for: MoE, expert routing (prefetching, speculating), expert caching, expert load balancing, token to expert relationship, expert usage characterization}

MoE architectures have evolved from seminal work~\cite{jacobs1991adaptive, jordan1994hierarchical} to state-of-the-art systems~\cite{jiang2024mixtral, deepseek2024v2, dai2024deepseekmoe}. We organize related work into three categories: (\textbf{1}) expert prediction, (\textbf{2}) expert orchestration, and (\textbf{3}) expert usage characterization. 

\noindent\textbf{Expert Prediction:}
Prior work on expert prediction has evolved from reactive routing to proactive prediction strategies. Speculative MoE~\cite{li2025speculative} and DAOP~\cite{zhang2025daop} predict experts before routing decisions through speculative execution. Cross-layer approaches~\cite{chen2025crosslayer} and task-aware systems~\cite{wang2025emoe} leverage adjacent layer information, while MoE-GPS~\cite{ma2025moegpsguidlinespredictionstrategy} provides systematic predictor selection guidelines. Concurrent work DuoServe-MoE~\cite{duoserve} employs decode-stage lookahead predictors achieving up to 7.54$\times$ speedup on specific models. PreScope~\cite{prescope} introduces layer-group-aware predictors (LLaPor) achieving 94\% Top-4 accuracy by specializing architectures for input/middle/output layer groups, combined with global cross-layer scheduling. Prophet differs fundamentally: our information-theoretic analysis reveals that cross-layer mutual information is \textit{uniform across layer positions} (\Cref{sec:Motivation}), motivating a unified predictor that captures network-wide dependencies rather than fragmented layer-specific models. This principled design achieves 80--89\% Top-1 accuracy (a stricter metric than Top-4) while requiring a single model across all layers. Prophet further differs through (1) information-theoretic grounding that quantifies achievable prediction accuracy, (2) demonstrated cross-domain transfer (67--98\% accuracy across domain pairs), and (3) systematic evaluation across five diverse architectures. A systematic comparison with prior approaches is provided in \Cref{app:predictor}.

\noindent\textbf{Expert Orchestration:}
Research efforts in the direction of orchestration of experts include addressing system-level challenges like housing the memory-intensive expert parameters in multi-tiered memory hierarchies, opportunistically caching the required experts in limited device memory along with minimizing communicational overhead~\cite{li2025comoe,suo2025coserve,kong2025serving,adapmoe}. Advanced caching systems include ProMoE~\cite{promoe}, MoE-Infinity~\cite{moeinfinity}, and HOBBIT~\cite{hobbit}. Learning-based caching approaches such as LRB~\cite{lrb} have demonstrated the potential of using machine learning to approximate optimal caching decisions; Prophet extends this principle to MoE expert prediction with cross-layer dependencies. Distributed training optimizations are explored by FasterMoE~\cite{fastermoe}, Expert Choice Routing~\cite{expertchoice}, and MegaBlocks~\cite{megablocks}, focusing on load balancing and communication efficiency. Prophet's high predictor accuracy enables loading more distinct experts into GPU memory without OOM events while reducing bandwidth contention.

\noindent\textbf{Expert Substitution and Compression:}
Complementary approaches address MoE inference through expert modification rather than prediction. SMoE~\cite{smoe} and BuddyMoE~\cite{buddymoe} exploit expert redundancy by substituting less-loaded experts, reducing communication in distributed settings. FloE~\cite{floe} combines expert compression with prediction, achieving memory reduction through low-rank approximation. FineMoE~\cite{finemoe} uses trajectory-based semantic maps for fine-grained expert selection. These approaches modify expert representations or routing semantics, whereas Prophet preserves original model behavior through prediction-only prefetching. The approaches are complementary: Prophet's predictions could guide which experts to compress or substitute, combining the benefits.% of both paradigms.

\noindent\textbf{CPU/GPU Hybrid Inference:}
Recent systems have explored heterogeneous execution to leverage both CPU and GPU resources for MoE inference. MoE-Lightning~\cite{cao2025moe} introduces CPU-GPU-I/O pipelining with hierarchical roofline modeling for memory-constrained GPUs, achieving up to 10$\times$ throughput gains. KTransformers~\cite{ktransformers} employs AMX-specialized CPU kernels with asynchronous scheduling for hybrid inference, demonstrating significant speedups on DeepSeek models. Prophet is \textit{orthogonal and complementary} to these approaches: while they optimize \textit{where} computation happens (CPU vs. GPU placement), Prophet optimizes \textit{when} experts are loaded through predictive prefetching. Prophet's predictions could guide CPU/GPU placement decisions, enabling combined deployment where predictions inform which experts to keep on GPU versus offload to CPU.

\noindent\textbf{Expert Usage Characterization:}
Prior work in this domain includes exploring the correlation among experts to complement optimization strategies~\cite{lo2025closerlookmixtureofexpertslarge, chen2024layerwise, zhang2024harder, oldfield2024multilinear}. The literature on specialization and redundancy spans DeepSeekMoE~\cite{dai2024deepseekmoe}, which enables fine-grained expert segmentation for behavioral profiling, and MoNE~\cite{zhang2024mone}, which addresses redundancy via a dual-metric assessment. Complementary work on semantic routing~\cite{chen2025probing} and correlation modeling~\cite{wang2024decorrelated} substantiates input-semantic routing and expert de-correlation techniques. In contrast to these primarily analytic approaches, Prophet couples such insights with system-level mechanisms, thereby translating them into actionable policies. Moreover, while correlation-driven studies are often model family-specific, Prophet leverages its context window to capture evolving temporal correlation characteristic of real-world deployments. Finally, an information-theoretic analysis of expert popularity reveals power-law scaling, conferring scalability across deployment regimes, and furnishing principled metrics for quantifying expert participation during inference.
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 