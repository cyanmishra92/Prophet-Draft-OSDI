%\vspace{-10pt}
\section{Related Work} 
\label{sec:related} %Dee

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 


%\textcolor{red}{Related works for: MoE, expert routing (prefetching, speculating), expert caching, expert load balancing, token to expert relationship, expert usage characterization}

The concept of employing ensembles of specialized neural networks, opportunistically used individually or in combination for task-specific computations, has garnered considerable attention in recent years. The technical details of this approach have been extensively discussed, from seminal work~\cite{jacobs1991adaptive, jordan1994hierarchical} to state-of-the-art research~\cite{jiang2024mixtral, deepseek2024v2, dai2024deepseekmoe}. We have arranged previous efforts in this direction into three categories: (\textbf{1}) expert prediction, \textbf{(2)} expert orchestration, and \textbf{(3)} expert usage characterization. 

\noindent\textbf{Expert Prediction:}
Prior work on expert prediction has evolved from reactive routing to proactive prediction strategies. Speculative MoE~\cite{li2025speculative} and DAOP~\cite{zhang2025daop} predict experts before routing decisions through speculative execution. Cross-layer approaches~\cite{chen2025crosslayer} and task-aware systems~\cite{wang2025emoe} leverage adjacent layer information, while MoE-GPS~\cite{ma2025moegpsguidlinespredictionstrategy} provides systematic predictor selection guidelines. Concurrent work DuoServe-MoE~\cite{duoserve} employs decode-stage lookahead predictors achieving up to 7.54$\times$ speedup on specific models. PreScope~\cite{prescope} introduces layer-group-aware predictors (LLaPor) achieving 94\% Top-4 accuracy by specializing architectures for input/middle/output layer groups, combined with global cross-layer scheduling. Prophet differs fundamentally: our information-theoretic analysis reveals that cross-layer mutual information is \textit{uniform across layer positions} (\Cref{sec:Motivation}), motivating a unified predictor that captures network-wide dependencies rather than fragmented layer-specific models. This principled design achieves 80--89\% Top-1 accuracy (a stricter metric than Top-4) while requiring a single model across all layers. Prophet further differs through (1) information-theoretic grounding that quantifies achievable prediction accuracy, (2) demonstrated cross-domain transfer (67--98\% accuracy across domain pairs), and (3) systematic evaluation across five diverse architectures.

\noindent\textbf{Expert Orchestration:}
Research efforts in the direction of orchestration of experts include addressing system-level challenges like housing the memory-intensive expert parameters in multi-tiered memory hierarchies, opportunistically caching the required experts in limited device memory along with minimizing communicational overhead~\cite{li2025comoe,suo2025coserve,kong2025serving,adapmoe}. Advanced caching systems include ProMoE~\cite{promoe}, MoE-Infinity~\cite{moeinfinity}, and HOBBIT~\cite{hobbit}. Learning-based caching approaches such as LRB~\cite{lrb} have demonstrated the potential of using machine learning to approximate optimal caching decisions in CDN contexts; Prophet extends this principle to MoE expert prediction with cross-layer dependencies. Distributed training optimizations are explored by FasterMoE~\cite{fastermoe}, Expert Choice Routing~\cite{expertchoice}, and MegaBlocks~\cite{megablocks}, focusing on load balancing and communication efficiency. An often overlooked but critical metric optimized by Prophet is the memory bandwidth and the count of unique experts that can be housed per inference. By combining high predictor accuracy with deduplication, Prophet loads more distinct experts into GPU memory without triggering out-of-memory events, while concomitantly reducing memory-bandwidth contention.

\noindent\textbf{CPU/GPU Hybrid Inference:}
Recent systems have explored heterogeneous execution to leverage both CPU and GPU resources for MoE inference. MoE-Lightning~\cite{cao2025moe} introduces CPU-GPU-I/O pipelining with hierarchical roofline modeling for memory-constrained GPUs, achieving up to 10$\times$ throughput gains. KTransformers~\cite{ktransformers} employs AMX-specialized CPU kernels with asynchronous scheduling for hybrid inference, demonstrating significant speedups on DeepSeek models. Prophet is \textit{orthogonal and complementary} to these approaches: while they optimize \textit{where} computation happens (CPU vs. GPU placement), Prophet optimizes \textit{when} experts are loaded through predictive prefetching. Prophet's predictions could guide CPU/GPU placement decisions, enabling combined deployment where predictions inform which experts to keep on GPU versus offload to CPU.

\noindent\textbf{Expert Usage Characterization:}
Prior work in this domain includes exploring the correlation among experts to complement optimization strategies~\cite{lo2025closerlookmixtureofexpertslarge, chen2024layerwise, zhang2024harder, oldfield2024multilinear}. The literature on specialization and redundancy spans DeepSeekMoE~\cite{dai2024deepseekmoe}, which enables fine-grained expert segmentation for behavioral profiling, and MoNE~\cite{zhang2024mone}, which addresses redundancy via a dual-metric assessment. Complementary work on semantic routing~\cite{chen2025probing} and correlation modeling~\cite{wang2024decorrelated} substantiates input-semantic routing and expert de-correlation techniques. In contrast to these primarily analytic approaches, Prophet couples such insights with system-level mechanisms, thereby translating them into actionable policies. Moreover, while correlation-driven studies are often model family-specific, Prophet leverages its context window to capture evolving temporal correlation characteristic of real-world deployments. Finally, an information-theoretic analysis of expert popularity reveals power-law scaling, conferring scalability across deployment regimes, and furnishing principled metrics for quantifying expert participation during inference.
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 