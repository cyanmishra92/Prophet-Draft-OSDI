\vspace{-10pt}
\section{Conclusions}
\label{sec:conclusion}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
Prophet addresses the fundamental memory bandwidth bottleneck constraining practical deployment of MoE models. Through information-theoretic analysis, we established that expert routing contains exploitable structure: power-law distributions ($\alpha = 1.2$--$2.2$) and cross-layer dependencies provide up to 59\% of maximum entropy as exploitable information---an architecture-invariant ratio across models with 8--128 experts---enabling accurate neural prediction for expert prefetching.

Building on these insights, \emph{Prophet} combines learned cross-layer prediction (80--89\% accuracy) with confidence-guided hierarchical caching, transforming MoE inference from reactive to predictive memory management. Our evaluation across five production architectures demonstrates $1.5\times$--$10.4\times$ TPOT speedups and $2.4\times$--$19.4\times$ tail latency (P99.9) improvements over on-demand loading, while enabling $4$--$8\times$ longer context lengths within the same memory budget. Critically, prediction accuracy remains 67--98\% across all cross-domain pairs, confirming that Prophet learns model-intrinsic routing patterns rather than dataset-specific features.

\noindent\textbf{Limitations.} Prophet targets memory-constrained inference where expert offloading is necessary; when GPU memory suffices for all experts, prediction-based prefetching provides diminishing returns. Our evaluation uses a custom inference harness rather than production serving frameworks; while Prophet's mechanisms are compatible with continuous batching, full integration remains future work. The predictor requires one-time training per MoE architecture.
%; architectures with significantly different routing behavior may require retraining.

These results demonstrate that predictive memory management fundamentally shifts MoE execution towards compute-bound, alleviating the dominant I/O barrier. The underlying principles 
%information-theoretic analysis guiding predictor design, cross-layer neural prediction, and confidence-aware resource allocation
hold promise for broader classes of sparse architectures where scalability and deployment efficiency are critical.
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 