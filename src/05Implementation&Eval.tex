%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Implementation and Evaluation}
\label{sec:implementation}

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Implementation}
\label{subsec:implementation}

\noindent\textbf{Prophet System.}
We implement Prophet as a complete inference optimization system comprising approximately 4,300 lines of code: 2,800 LoC for the neural predictor (PyTorch 2.0~\cite{pytorch}) and 1,500 LoC for the runtime system including cache management and async prefetching. The neural predictor employs a 4-layer dense transformer encoder with 8.4M parameters, using model dimension 320, 10 attention heads, and feedforward dimension 1,280. Input representation combines continuous hidden states from the current layer with discrete expert context from the preceding 3 layers through learned embeddings (128--256 dimensions based on expert count). The predictor outputs predictions for horizons $h \in \{1, 2, 3\}$ simultaneously via parallel prediction heads. Training uses the AdamW optimizer with learning rate $1 \times 10^{-3}$, batch size 1,024, and 10 epochs on routing traces from six datasets: Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText---yielding 37,200 traces. Training completes in 3.5--4.5 hours on a single A100 GPU. At runtime, we employ CUDA graph capture for predictor inference and implement an asynchronous prefetch pipeline that overlaps prediction with expert loading.

\noindent\textbf{Hardware Platform.}
Our primary evaluation platform uses NVIDIA A100-80GB GPUs (PCIe 4.0) with 2 TB/s HBM bandwidth. Host memory comprises 512GB DDR4-3200 providing 204 GB/s bandwidth, connected via PCIe 4.0 at 64 GB/s bidirectional. For multi-GPU experiments, we use 4$\times$ A100 GPUs interconnected with NVLink (600 GB/s aggregate). This configuration represents a typical cloud deployment scenario where GPU memory is constrained relative to model size, necessitating expert offloading to host memory.

\noindent\textbf{Target Models.}
We evaluate on five production MoE architectures representing the current state-of-the-art: GPT-OSS-20B (32 experts, top-3--6 routing, $\sim$600MB per expert, 20B total parameters), Mixtral-8x7B~\cite{jiang2024mixtral} (8 experts, top-4 routing, $\sim$1.3GB per expert, 46.7B total), DeepSeek-MoE-16B~\cite{dai2024deepseekmoe} (64 experts, top-1 routing, $\sim$250MB per expert, 16B total), Qwen3-30B~\cite{yang2025qwen3} (128 experts, top-1 routing, $\sim$250MB per expert, 30B total), and Qwen1.5-MoE-A2.7B~\cite{bai2023qwen} (60 experts, top-1 routing, $\sim$100MB per expert, 14.3B total). This selection spans diverse expert counts (8--128), routing strategies (top-1 to top-6), and model scales (14--47B parameters). We exclude Switch Transformer from our main evaluation as its top-1 routing is substantially easier to predict and well-studied in prior work~\cite{promoe, pregated}; our target models present more challenging benchmarks.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Experimental Setup}
\label{subsec:setup}

\noindent\textbf{Datasets.}
We evaluate across six diverse benchmarks: GSM8K~\cite{gsm8k} (mathematical reasoning), HumanEval~\cite{humaneval} (code generation), WikiText~\cite{wikitext} (language modeling), Alpaca~\cite{alpaca} (instruction following), SQuAD~\cite{squad} (reading comprehension), and Natural Questions~\cite{googleNatural} (open-domain QA). This diversity ensures evaluation covers both structured tasks (math, code) and open-ended generation (instructions, QA).

\noindent\textbf{Baselines.}
We compare against five approaches: (1) \textit{On-Demand Loading}---loads experts reactively when routing decisions are made, representing the default approach in memory-constrained deployments; (2) \textit{LRU Caching}---maintains a cache of recently-used experts without prediction; (3) \textit{Expert-Parallel (EP)}---all experts resident across multiple GPUs, an upper bound representing unlimited GPU memory; (4) \textit{ProMoE}~\cite{promoe}---stride-based prefetching assuming periodic access patterns; and (5) \textit{ExpertFlow}~\cite{expertflow}---token-distribution-aware expert placement with adaptive loading. All baselines are faithfully reimplemented and evaluated under identical hardware constraints.

\noindent\textbf{Complementary Optimizations.}
Prophet focuses on prediction-driven prefetching and is orthogonal to other MoE optimization strategies. CPU/GPU co-execution systems (MoE-Lightning~\cite{moelightning}, kTransformers~\cite{ktransformers}) exploit CPU compute for expert execution; Prophet's predictions can inform which experts to execute on which device. Continuous batching frameworks (vLLM, TGI) manage request scheduling; Prophet's per-request routing history integrates with their iteration-level batching. Quantization techniques (INT4/INT8) reduce expert size; Prophet's prefetching benefits scale with reduced transfer times. These approaches are complementary and can be combined with Prophet for additional gains.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Predictor Analysis}
\label{subsec:predictor}

Before presenting end-to-end results, we establish that Prophet's neural predictor achieves the accuracy and efficiency required for practical deployment.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Mixtral_8x7B_compact_heatmap.pdf}
\caption{Mixtral-8x7B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen1_5_MoE_compact_heatmap.pdf}
\caption{Qwen1.5-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen3_30B_compact_heatmap.pdf}
\caption{Qwen3-30B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/DeepSeek_MoE_compact_heatmap.pdf}
\caption{DeepSeek-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/GPT_OSS_20B_compact_heatmap.pdf}
\caption{GPT-OSS-20B}
\end{subfigure}
\vspace{-5pt}
\caption{Zero-shot cross-domain transfer matrices for all five models. Each cell shows prediction accuracy when training on row domain and testing on column domain. High accuracy (67--98\%) across all domain pairs confirms routing patterns are model-intrinsic rather than dataset-specific.}
\label{fig:cross_domain}
\end{figure*}

\noindent\textbf{Prediction Accuracy.}
We define prediction accuracy as recall@k: for each token, we measure whether Prophet's top-k predicted experts contain the actual router-selected expert(s). Prophet achieves 80--89\% prediction accuracy across all models---DeepSeek (88.6\%), GPT-OSS (86.6\%), Mixtral (82.4\%), Qwen3 (80.4\%), and Qwen1.5 (80.4\%)---substantially higher than prior heuristic approaches. Higher accuracy correlates with stronger power-law concentration ($\alpha > 2$), making routing more predictable.

\noindent\textbf{Cross-Domain Generalization.}
A critical question for learned predictors is whether patterns generalize beyond training data. \Cref{fig:cross_domain} presents zero-shot cross-domain transfer matrices for all five models, where we train on traces from one domain and test on a completely different domain. Prediction accuracy remains 67--98\% across all 180 train-test domain pairs (36 per model $\times$ 5 models), with out-of-domain testing reducing accuracy by only 3--8\% compared to in-domain evaluation. This robustness confirms that Prophet learns model-intrinsic routing structure---how transformer layers progressively transform representations---rather than dataset-specific patterns, enabling generalization without per-dataset retraining.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Component} & \textbf{Latency} & \textbf{Memory} & \textbf{Notes} \\
\midrule
Forward pass & 0.8ms & 32MB & 8.4M params \\
Context encoding & 0.5ms & 2MB & Reused across tokens \\
Async prefetch & 0.7ms & 0 & Overlapped \\
\midrule
\textbf{Total} & \textbf{2.0ms} & \textbf{34MB} & <1\% overhead \\
\bottomrule
\end{tabular}
\caption{Predictor overhead analysis. Total overhead is negligible compared to baseline TPOT (1,119--6,123ms).}
\label{tab:overhead}
\end{table}

\noindent\textbf{Predictor Overhead.}
\Cref{tab:overhead} breaks down Prophet's runtime overhead. The predictor forward pass takes 0.8ms with 32MB for 8.4M parameters; context encoding adds 0.5ms (reused across tokens); and async prefetch coordination adds 0.7ms (overlapped with compute). The total 2.0ms overhead represents $<$1\% of baseline TPOT for all models. One-time training cost (3.5--4.5 hours on A100) amortizes across all subsequent inference runs.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Latency-Critical Evaluation}
\label{subsec:latency}

Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. We evaluate Prophet against five baselines: On-Demand loading, LRU caching, ExpertFlow, ProMoE, and Expert-Parallel (EP) as an upper bound.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig1_tpot_breakdown.pdf}
\caption{TPOT comparison}
\label{fig:tpot_comparison}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig2_tail_latency_cdf.pdf}
\caption{Tail latency distribution}
\label{fig:tail_cdf}
\end{subfigure}
\vspace{-5pt}
\caption{End-to-end latency analysis at batch size 1 with 4,096-token context. (a) Prophet (gold) achieves 1.53--10.38$\times$ TPOT speedup over On-Demand; hatched portion shows 2\% miss penalty. (b) Prophet maintains tight P99.9/P50 ratio (1.05--1.15$\times$) versus baseline's high variance (1.50--1.75$\times$), enabling predictable SLO compliance.}
\label{fig:latency_combined}
\end{figure*}

\noindent\textbf{End-to-End Latency.}
\Cref{fig:tpot_comparison} presents Time Per Output Token (TPOT) results at batch size 1 with 4,096-token context. Prophet achieves substantial speedups across all architectures: GPT-OSS-20B improves from 2,583ms to 249ms (10.4$\times$), Mixtral-8x7B from 3,848ms to 479ms (8.0$\times$), DeepSeek-MoE-16B from 2,263ms to 622ms (3.6$\times$), Qwen3-30B from 6,123ms to 2,232ms (2.7$\times$), and Qwen1.5-MoE from 1,119ms to 729ms (1.5$\times$). The speedup magnitude correlates with I/O dominance in baseline execution---GPT-OSS-20B exhibits the highest speedup because expert transfer dominates 92\% of baseline execution time, while Qwen1.5-MoE shows smaller speedup because transfer constitutes only 36\% of its baseline.

Prophet's TPOT includes a small miss penalty (hatched portion in \Cref{fig:tpot_comparison}) from the 2\% of experts not correctly prefetched. This penalty ranges from 7.9ms for Qwen1.5-MoE to 79.4ms for Qwen3-30B, proportional to each model's transfer time. Despite this penalty, Prophet achieves 81--99\% of EP's performance---the upper bound where all experts are GPU-resident---while using 70\% less expert memory.

\noindent\textbf{Tail Latency.}
Production deployments require not just low mean latency but predictable tail latency for SLO compliance. \Cref{fig:tail_cdf} shows latency distributions across 10,000 inference samples per model. Prophet's P99.9 improvements are even more dramatic than mean speedups: GPT-OSS-20B achieves 19.4$\times$ (4,520ms$\rightarrow$234ms), Mixtral 14.2$\times$ (6,733ms$\rightarrow$474ms), DeepSeek 5.8$\times$ (3,961ms$\rightarrow$679ms), Qwen3 4.3$\times$ (10,715ms$\rightarrow$2,477ms), and Qwen1.5 2.4$\times$ (1,958ms$\rightarrow$832ms). This stability is critical: baseline systems exhibit P99.9/P50 ratios of 1.50--1.75$\times$ due to unpredictable cache misses, while Prophet maintains tight ratios of only 1.05--1.15$\times$ by ensuring high-probability experts are preloaded.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig5_memory_breakdown.pdf}
\caption{Memory breakdown}
\label{fig:memory_breakdown}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig3_context_capacity.pdf}
\caption{Context capacity}
\label{fig:context}
\end{subfigure}
\vspace{-5pt}
\caption{Memory efficiency and context extension within 640GB budget. (a) Prophet reduces expert memory by 70\% across all models, reallocating freed memory to KV cache. (b) This enables 4--8$\times$ longer contexts (32K$\rightarrow$131K for Qwen1.5, 8K$\rightarrow$65K for others) while maintaining near-EP latency.}
\label{fig:memory_combined}
\end{figure*}

\noindent\textbf{Memory Efficiency and Context Extension.}
\Cref{fig:memory_breakdown} decomposes memory usage by component across all systems. Prophet reduces expert memory by 70\% across all models: Qwen1.5-MoE from 6.0GB to 1.8GB, DeepSeek from 16.0GB to 4.8GB, GPT-OSS from 19.2GB to 5.8GB, Qwen3 from 32.0GB to 9.6GB, and Mixtral from 10.4GB to 3.1GB. This freed memory is reallocated to KV cache, enabling longer context support. Prophet's 34MB predictor overhead is negligible ($<$0.1\% of total memory).

\Cref{fig:context} shows the maximum context length achievable within a fixed 640GB memory budget. Prophet enables dramatic context extension: Qwen1.5-MoE from 32K to 131K tokens (4$\times$), DeepSeek from 16K to 131K (8$\times$), and GPT-OSS/Qwen3/Mixtral from 8K to 65K (8$\times$). Notably, EP---while achieving the lowest latency---has the smallest context capacity (4--16K tokens) because it requires all experts to be GPU-resident. Prophet achieves the best of both worlds: near-EP latency with the highest context capacity.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig4_oi_vs_tpot.pdf}
\vspace{-15pt}
\caption{Operational Intensity vs. TPOT showing Prophet shifts all models toward the compute-bound regime (OI $>$ 1). Baseline models cluster at low OI (memory-bound); Prophet increases OI by 5.1--9.3$\times$.}
\label{fig:oi_tpot}
\end{figure}

\noindent\textbf{Operational Intensity: The Systems Insight.}
To understand \textit{why} Prophet achieves these speedups, we analyze Operational Intensity (OI)---the ratio of compute operations to memory transfers. \Cref{fig:oi_tpot} reveals that all baseline systems exhibit OI $<$ 0.7, indicating severe memory bottlenecks: On-Demand ranges from 0.11 (Mixtral) to 0.38 (Qwen3), with LRU, ExpertFlow, and ProMoE providing incremental improvements but remaining firmly memory-bound.

Prophet fundamentally shifts the execution regime. OI increases by 5.1--9.3$\times$: DeepSeek improves from 0.18 to 1.67 (9.3$\times$), GPT-OSS from 0.17 to 1.31 (7.7$\times$), Mixtral from 0.11 to 0.61 (5.5$\times$), Qwen3 from 0.38 to 1.99 (5.2$\times$), and Qwen1.5 from 0.26 to 1.33 (5.1$\times$). Four of five models now operate in the compute-bound regime (OI $>$ 1), with only Mixtral remaining below 1.0 due to its unique 8$\times$7B architecture requiring more experts per token. This shift from memory-bound to compute-bound execution enables dramatically better GPU utilization---the GPU spends time computing rather than waiting for data transfers.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Throughput-Critical Evaluation}
\label{subsec:throughput}

While Prophet primarily targets latency-critical BS=1 inference, we evaluate throughput at larger batch sizes and under tensor parallelism to demonstrate broader applicability.

\noindent\textbf{Batch Size Scaling.}
Prophet's speedup remains stable across batch sizes 1--16, with <5\% relative decrease at larger batches. This stability occurs because Prophet's prediction accuracy is per-token (not batch-dependent), while baselines benefit from incidental expert reuse at larger batches---partially closing the gap but never eliminating Prophet's prediction advantage.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{BS=1} & \textbf{BS=4} & \textbf{BS=16} \\
\midrule
GPT-OSS-20B & 10.4$\times$ & 10.1$\times$ & 9.9$\times$ \\
Mixtral-8x7B & 8.0$\times$ & 7.7$\times$ & 7.5$\times$ \\
DeepSeek-MoE-16B & 3.6$\times$ & 3.5$\times$ & 3.4$\times$ \\
Qwen3-30B & 2.7$\times$ & 2.6$\times$ & 2.5$\times$ \\
Qwen1.5-MoE & 1.5$\times$ & 1.5$\times$ & 1.4$\times$ \\
\bottomrule
\end{tabular}
\caption{Speedup vs. batch size. Prophet's advantage is stable across batch sizes.}
\label{tab:batch}
\end{table}

\noindent\textbf{Multi-GPU Scaling.}
Production deployments often use tensor parallelism (TP) to distribute models across GPUs. Prophet's speedup is preserved under tensor parallelism---relative advantage remains within 2\% across TP=1--8. GPT-OSS-20B maintains 10.4$\times$/10.4$\times$/10.3$\times$/10.2$\times$ speedup at TP=1/2/4/8 respectively; Mixtral maintains 8.0$\times$/8.0$\times$/7.9$\times$/7.8$\times$; and similar stability holds for other models. In TP configurations, expert loading becomes a per-GPU operation; Prophet's predictions remain accurate because routing decisions are determined by model architecture, not parallelization strategy.

\noindent\textbf{Continuous Batching Compatibility.}
Prophet's per-token routing history naturally extends to continuous batching frameworks (vLLM, TGI) that dynamically schedule requests. Per-request history persists across batch iterations, and predictions update as new tokens generate. We leave full integration with production serving frameworks for future work, noting that Prophet's prediction mechanism is compatible with their iteration-level batching and that the power-law expert concentration ensures popular experts remain cached across request boundaries.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
