%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Implementation and Evaluation}
\label{sec:implementation}

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Implementation}
\label{subsec:implementation}

\noindent\textbf{Prophet System.}
We implement Prophet as a complete inference optimization system comprising approximately 4,300 lines of code: 2,800 LoC for the neural predictor (PyTorch 2.0~\cite{pytorch}) and 1,500 LoC for the runtime system including cache management and async prefetching. The neural predictor employs a 4-layer dense transformer encoder with 8.4M parameters, using model dimension 320, 10 attention heads, and feedforward dimension 1,280. Input representation combines continuous hidden states from the current layer with discrete expert context from the preceding 3 layers through learned embeddings (128--256 dimensions based on expert count). The predictor outputs predictions for horizons $h \in \{1, 2, 3\}$ simultaneously via parallel prediction heads. 
%Training uses the AdamW optimizer with learning rate $1 \times 10^{-3}$, batch size 1,024, and 10 epochs on routing traces from six datasets: Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText---yielding 37,200 traces. Training completes in 3.5--4.5 hours on a single A100 GPU. 
At runtime, we employ CUDA graph capture for predictor inference and implement an asynchronous prefetch pipeline that overlaps prediction with expert loading.

\noindent\textbf{Hardware Platform.}
Our primary evaluation platform uses NVIDIA A100-80GB GPUs (PCIe 4.0) with 2 TB/s HBM bandwidth. Host memory comprises 512GB DDR4-3200 providing 204 GB/s bandwidth, connected via PCIe 4.0 at 64 GB/s bidirectional. For multi-GPU experiments, we use 4$\times$ A100 GPUs interconnected with NVLink (600 GB/s aggregate). This configuration represents a typical cloud deployment scenario where GPU memory is constrained relative to model size, necessitating expert offloading to host memory.

\noindent\textbf{Target Models.}
We evaluate on five production MoE architectures representing the current state-of-the-art: GPT-OSS-20B~\cite{gptoss}
%\footnote{\url{https://huggingface.co/openai/gpt-oss-20b}} 
([\textbf{M3}], 32 experts, top-4 routing, $\sim$600MB per expert, 20B total parameters), Mixtral-8x7B~\cite{jiang2024mixtral} ([\textbf{M5}], 8 experts, top-2 routing, $\sim$1.3GB per expert, 46.7B total), DeepSeek-MoE-16B~\cite{dai2024deepseekmoe} ([\textbf{M2}], 64 routed + 1 shared experts, top-6 routing, $\sim$250MB per expert, 16B total), Qwen3-30B~\cite{yang2025qwen3} ([\textbf{M4}], 128 experts, top-8 routing, $\sim$250MB per expert, 30B total), and Qwen1.5-MoE-A2.7B~\cite{bai2023qwen} ([\textbf{M1}], 60 routed + 4 shared experts, top-4 routing, $\sim$100MB per expert, 14.3B total). This selection spans diverse expert counts (8--128), routing strategies (top-2 to top-8), and model scales (14--47B parameters). We exclude Switch Transformer from our main evaluation as its top-1 routing is substantially easier to predict and well-studied in prior work~\cite{promoe, pregated}; our target models present more challenging benchmarks.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Experimental Setup}
\label{subsec:setup}

\noindent\textbf{Datasets.}
We evaluate across six diverse benchmarks: GSM8K~\cite{gsm8k} (mathematical reasoning), HumanEval~\cite{humaneval} (code generation), WikiText~\cite{wikitext} (language modeling), Alpaca~\cite{alpaca} (instruction following), SQuAD~\cite{squad} (reading comprehension), and Natural Questions~\cite{googleNatural} (open-domain QA). This diversity ensures evaluation covers both structured tasks (math, code) and open-ended generation (instructions, QA).

\noindent\textbf{Baselines.}
We compare against seven approaches: (1) \textit{On-Demand Loading}---loads experts reactively when routing decisions are made; (2) \textit{LRU Caching}---maintains a cache of recently-used experts without prediction; (3) \textit{Expert-Parallel (EP)}---all experts GPU-resident (performance ceiling); (4) \textit{ProMoE}~\cite{promoe}---stride-based prefetching; (5) \textit{ExpertFlow}~\cite{expertflow}---token-distribution-aware loading; (6) \textit{FATE}~\cite{fate}---cross-layer gate prediction with confidence filtering, faithfully reimplemented; and (7) \textit{PreScope}~\cite{prescope}---layer-group-aware MLPs with hidden-state prediction, analytically modeled using optimistic assumptions based on reported 94\% Top-4 accuracy (released October 2025, no public code). All baselines evaluated under identical hardware constraints.

\noindent\textbf{Complementary Optimizations.}
Prophet focuses on prediction-driven prefetching and is orthogonal to other MoE optimization strategies. CPU/GPU co-execution systems~\cite{cao2025moe, ktransformers} 
%(MoE-Lightning~\cite{cao2025moe}, kTransformers~\cite{ktransformers}) 
exploit CPU compute for expert execution; Prophet's predictions can inform which experts to execute on which device. Continuous batching frameworks (vLLM, TGI) manage request scheduling; Prophet's per-request routing history integrates with their iteration-level batching. Quantization techniques reduce expert size; Prophet's prefetching benefits scale with reduced transfer times. These approaches 
%are complementary and 
can be combined with Prophet for additional gains.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Predictor Analysis}
\label{subsec:predictor}

Before presenting end-to-end results, we establish that Prophet's neural predictor achieves the accuracy and efficiency required for practical deployment.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Mixtral_8x7B_compact_heatmap.pdf}
\caption{Mixtral-8x7B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen1_5_MoE_compact_heatmap.pdf}
\caption{Qwen1.5-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen3_30B_compact_heatmap.pdf}
\caption{Qwen3-30B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/DeepSeek_MoE_compact_heatmap.pdf}
\caption{DeepSeek-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/GPT_OSS_20B_compact_heatmap.pdf}
\caption{GPT-OSS-20B}
\end{subfigure}
\vspace{-5pt}
\caption{Zero-shot cross-domain transfer matrices for all five models. Each cell shows prediction accuracy when training on row domain and testing on column domain. High accuracy (67--98\%) across all domain pairs confirms routing patterns are model-intrinsic rather than dataset-specific.}
\label{fig:cross_domain}
\end{figure*}

\noindent\textbf{Prediction Accuracy.}
We define prediction accuracy as recall@k: for each token, we measure whether Prophet's top-k predicted experts contain the actual router-selected expert(s). \Cref{tab:accuracy_detailed} provides a comprehensive breakdown of prediction accuracy metrics across all evaluated models. We report both Top-1 accuracy (whether the single highest-probability prediction matches ground truth) and Recall@k (whether all k router-selected experts appear in the top-k predictions). Top-1 is a stricter metric that measures precise prediction; Recall@k measures effective prefetching coverage.

\begin{table}[h]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Top-1 Acc.} & \textbf{Recall@k} & \textbf{k} & \textbf{Random} \\
\midrule
Qwen1.5-MoE (M1) & 80.4\% & 94.2\% & 4 & 6.7\% \\
DeepSeek (M2)    & 88.6\% & 96.8\% & 6 & 9.4\% \\
GPT-OSS (M3)     & 86.6\% & 95.1\% & 4 & 12.5\% \\
Qwen3 (M4)       & 80.4\% & 93.7\% & 8 & 6.3\% \\
Mixtral (M5)     & 82.4\% & 91.3\% & 2 & 25.0\% \\
\bottomrule
\end{tabular}%
}
\caption{Prediction accuracy breakdown. Top-1 measures if the single highest-probability prediction is correct; Recall@k measures if all router-selected experts are in Prophet's top-k predictions. Random baseline = k/N where N is total experts.}
\label{tab:accuracy_detailed}
\end{table}


Prophet achieves 80--89\% Top-1 accuracy across all models, with Recall@k exceeding 91\% in all cases. This accuracy directly reflects our information-theoretic analysis: with $\sim$59\% of maximum entropy exploitable and 63\% of that coming from cross-layer expert history, Prophet's transformer predictor effectively translates exploitable information into actionable predictions. The higher Recall@k values (vs Top-1) indicate that Prophet's probability ranking is well-calibrated: even when the top-1 prediction is incorrect, the true experts typically appear in the top-k predictions. This property is crucial for effective prefetching, where coverage matters more than strict precision.

%Prophet achieves 80--89\% prediction accuracy across all models---DeepSeek (88.6\%), GPT-OSS (86.6\%), Mixtral (82.4\%), Qwen3 (80.4\%), and Qwen1.5 (80.4\%)---substantially higher than prior heuristic approaches. Higher accuracy correlates with stronger power-law concentration ($\alpha > 2$), making routing more predictable. Detailed accuracy breakdown (Top-1 vs. Recall@k) is provided in \Cref{app:predictor}.

\noindent\textbf{Cross-Domain Generalization.}
\Cref{fig:cross_domain} presents zero-shot cross-domain transfer matrices for all five models. Each off-diagonal cell represents a true hold-out evaluation: the predictor is trained \textit{exclusively} on domain $X$ and tested on domain $Y$ with zero data overlap. This zero-shot domain transfer %(e.g., training on code generation and testing on mathematical reasoning) 
represents the worst-case generalization scenario. Prediction accuracy remains 67--98\% across all 180 train-test domain pairs, with only 3--8\% degradation vs. in-domain. This confirms that Prophet learns model-intrinsic routing structure: how transformer layers progressively transform representations rather than dataset-specific lexical or semantic patterns.

\begin{table}[h]
\centering
\small
\resizebox{\linewidth}{!}{%
\begin{tabular}{lccc}
\toprule
\textbf{Component} & \textbf{Latency} & \textbf{Memory} & \textbf{Notes} \\
\midrule
Forward pass     & 0.8ms & 32MB & 8.4M params \\
Context encoding & 0.5ms & 2MB  & Reused across tokens \\
Async prefetch   & 0.7ms & 0    & Overlapped \\
\midrule
\textbf{Total}   & \textbf{2.0ms} & \textbf{34MB} & <1\% overhead \\
\bottomrule
\end{tabular}%
}
\caption{Predictor overhead analysis. Total overhead is negligible compared to baseline TPOT (1,119--6,123ms).}
\label{tab:overhead}
\end{table}


\noindent\textbf{Predictor Overhead.}
\Cref{tab:overhead} breaks down Prophet's runtime overhead. The predictor forward pass takes 0.8ms with 32MB for 8.4M parameters; context encoding adds 0.5ms (reused across tokens); and async prefetch coordination adds 0.7ms (overlapped with compute). The total 2.0ms overhead represents $<$1\% of baseline TPOT for all models. One-time training cost (3.5--4.5 hours on A100) amortizes across all subsequent inference runs. An anonymized implementation of the Prophet predictor is \href{https://anonymous.4open.science/r/prophet-artifact-5B42/README.md}{available here}.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Latency-Critical Evaluation}
\label{subsec:latency}

Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. We evaluate Prophet against seven baselines: On-Demand loading, LRU caching, ExpertFlow, ProMoE, FATE, PreScope, and Expert-Parallel (EP) as an upper bound.

\begin{figure*}[]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig1_tpot_breakdown.pdf}
\caption{TPOT comparison}
\label{fig:tpot_comparison}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig2_tail_latency_cdf.pdf}
\caption{Tail latency distribution}
\label{fig:tail_cdf}
\end{subfigure}
\vspace{-5pt}
\caption{End-to-end latency at batch size 1 with 4,096-token context. (a) Prophet (gold) achieves 1.53--10.38$\times$ TPOT speedup over On-Demand; hatched portion shows 2\% miss penalty. (b) Prophet maintains tight P99.9/P50 ratio (1.05--1.15$\times$) versus %baseline's high variance (1.50--1.75$\times$)., 
enabling predictable SLO compliance.
\textbf{M1}: \textit{Qwen}, \textbf{M2}: \textit{DeepSeek}, \textbf{M3}: \textit{GPT-OSS}, \textbf{M4}: \textit{Qwen3}, \textbf{M5}: \textit{Mixtral}, \textbf{EP}=\textit{Expert Parallel}.}
\label{fig:latency_combined}
\end{figure*}

\noindent\textbf{End-to-End Latency.}
\Cref{fig:tpot_comparison} presents Time Per Output Token (TPOT) results at batch size 1 with 4,096-token context. Prophet achieves substantial speedups across all architectures: GPT-OSS-20B improves from 2,583ms to 249ms (10.4$\times$), Mixtral-8x7B from 3,848ms to 479ms (8.0$\times$), DeepSeek-MoE-16B from 2,263ms to 622ms (3.6$\times$), Qwen3-30B from 6,123ms to 2,232ms (2.7$\times$), and Qwen1.5-MoE from 1,119ms to 729ms (1.5$\times$). The speedup magnitude correlates with I/O dominance in baseline execution. GPT-OSS-20B exhibits the highest speedup because expert transfer dominates 92\% of baseline execution time, while a much smaller model Qwen1.5-MoE shows smaller speedups.% because transfer constitutes only 36\% of its baseline.

Prophet's TPOT includes a small miss penalty (hatched portion in \Cref{fig:tpot_comparison}) from the 2\% of experts not correctly prefetched. This penalty ranges from 7.9ms for Qwen1.5-MoE to 79.4ms for Qwen3-30B, proportional to each model's transfer time. Despite this penalty, Prophet achieves 81--99\% of EP's performance---the upper bound where all experts are GPU-resident---while using 70\% less expert memory.

\noindent\textbf{Tail Latency.}
%Production deployments require not just low mean latency but predictable tail latency for SLO compliance. 
\Cref{fig:tail_cdf} shows latency distributions across 100,000 inference samples per model. Prophet achieves: GPT-OSS-20B achieves 19.4$\times$ (4,520ms$\rightarrow$234ms), Mixtral 14.2$\times$ (6,733ms$\rightarrow$474ms), DeepSeek 5.8$\times$ (3,961ms$\rightarrow$679ms), Qwen3 4.3$\times$ (10,715ms$\rightarrow$2,477ms), and Qwen1.5 2.4$\times$ (1,958ms$\rightarrow$832ms). This stability is critical: baseline systems exhibit P99.9/P50 ratios of 1.50--1.75$\times$ due to unpredictable cache misses, while Prophet maintains tight ratios of only 1.05--1.15$\times$ by prefetching high-probability experts.

\begin{figure*}[]
\centering
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig5_memory_breakdown.pdf}
\caption{Memory breakdown}
\label{fig:memory_breakdown}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.48\linewidth}
\includegraphics[width=\linewidth]{figs/fig3_context_capacity.pdf}
\caption{Context capacity}
\label{fig:context}
\end{subfigure}
\vspace{-5pt}
\caption{Memory efficiency and context extension within 640GB budget. (a) Prophet reduces expert memory by 70\% across all models, reallocating freed memory to KV cache. (b) This enables 4--8$\times$ longer contexts (32K$\rightarrow$131K for Qwen1.5, 8K$\rightarrow$65K for others) while maintaining near-EP latency. \textbf{M1}: \textit{Qwen}, \textbf{M2}: \textit{DeepSeek}, \textbf{M3}: \textit{GPT-OSS}, \textbf{M4}: \textit{Qwen3}, \textbf{M5}: \textit{Mixtral}. Systems: OD=On-Demand, EF=ExpertFlow, PM=ProMoE, FA=FATE, PS=PreScope, EP=Expert Parallel, Pr=Prophet.}
\label{fig:memory_combined}
\end{figure*}

\noindent\textbf{Memory Efficiency and Context Extension.}
\Cref{fig:memory_breakdown} decomposes memory usage by component across all systems. Prophet reduces expert memory by 70\% across all models: Qwen1.5-MoE from 6.0GB to 1.8GB, DeepSeek from 16.0GB to 4.8GB, GPT-OSS from 19.2GB to 5.8GB, Qwen3 from 32.0GB to 9.6GB, and Mixtral from 10.4GB to 3.1GB. This freed memory is reallocated to KV cache, enabling longer context support. Prophet's 34MB predictor overhead is negligible ($<$0.1\% of total memory).

\Cref{fig:context} shows the maximum context length achievable within a fixed 640GB memory budget. Prophet enables dramatic context extension: Qwen1.5-MoE from 32K to 131K tokens (4$\times$), DeepSeek from 16K to 131K (8$\times$), and GPT-OSS/Qwen3/Mixtral from 8K to 65K (8$\times$). Notably, EP---while achieving the lowest latency---has the smallest context capacity (4--16K tokens) because it requires all experts to be GPU-resident. Prophet achieves the best of both worlds: near-EP latency with the highest context capacity. Compared to PreScope, Prophet achieves 1.2--1.8$\times$ higher context capacity despite similar TPOT. PreScope's Top-k prediction requires caching $\sim$2$\times$ more experts to achieve comparable hit rates, leaving less memory for KV cache. This advantage scales with expert size: Mixtral (1.3GB/expert) shows 1.78$\times$ improvement versus 1.23$\times$ for Qwen1.5 (100MB/expert).

\begin{figure}[]
\centering
\includegraphics[width=\linewidth]{figs/fig4_oi_vs_tpot.pdf}
\vspace{-15pt}
\caption{Operational Intensity vs. TPOT showing Prophet shifts all models toward the compute-bound regime (OI $>$ 1). Prophet increases OI by 5.1--13.8$\times$ over On-Demand. PreScope exhibits the lowest OI due to its Top-4 prediction strategy (4$\times$ bandwidth overhead). \textbf{M1}: \textit{Qwen}, \textbf{M2}: \textit{DeepSeek}, \textbf{M3}: \textit{GPT-OSS}, \textbf{M4}: \textit{Qwen3}, \textbf{M5}: \textit{Mixtral}. Systems: OD=On-Demand, FA=FATE, PS=PreScope, Pr=Prophet.}
\label{fig:oi_tpot}
\end{figure}

\noindent\textbf{Operational Intensity: The Systems Insight.}
To understand \textit{why} Prophet achieves these speedups, we analyze Operational Intensity (OI)---the ratio of compute operations to memory transfers. \Cref{fig:oi_tpot} reveals that On-Demand exhibits low OI (0.11--0.38), with LRU, ExpertFlow, and ProMoE providing incremental improvements but remaining memory-bound. Notably, PreScope exhibits the \textit{lowest} OI among all systems (0.04--0.12)---despite achieving good TPOT, its Top-4 prediction strategy transfers 4$\times$ more experts than necessary, severely penalizing bandwidth efficiency.
Prophet fundamentally shifts the execution regime. OI increases by 5.1--13.8$\times$ over On-Demand: Mixtral improves from 0.11 to 1.52 (13.8$\times$), DeepSeek from 0.18 to 1.67 (9.3$\times$), GPT-OSS from 0.17 to 1.31 (7.7$\times$), Qwen3 from 0.38 to 1.99 (5.2$\times$), and Qwen1.5 from 0.26 to 1.33 (5.1$\times$). All five models now operate in the compute-bound regime (OI $>$ 1). This shift from memory-bound to compute-bound execution where the GPU spends time computing rather than waiting for data transfers.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Throughput-Critical Evaluation}
\label{subsec:throughput}

While Prophet primarily targets latency-critical BS=1 inference, we evaluate throughput scaling to demonstrate broader applicability.

\noindent\textbf{Deduplication from Power-Law Concentration.}
The power-law distribution ($\alpha = 1.2$--$2.2$) creates deduplication opportunities in batched inference. When multiple tokens request experts, popular experts are frequently requested by multiple tokens simultaneously. Prophet fetches only \textit{unique} experts per batch rather than duplicates. Unique expert count scales sublinearly: $\mathbb{E}[U(B)] \sim B^{0.932}$; at BS=16, this yields 16\% bandwidth reduction versus naive per-token fetching.

\noindent\textbf{Bandwidth Efficiency vs. Baselines.}
\Cref{fig:batch_bandwidth} compares speedup and bandwidth across batch sizes. Prophet's deduplication provides 1.2$\times$ bandwidth reduction over On-Demand at BS=16. The advantage over PreScope is more pronounced: PreScope's Top-4 prediction transfers 4$\times$ more experts per token without deduplication, requiring 4$\times$ Prophet's bandwidth at equivalent batch sizes (\Cref{fig:batch_bandwidth}b). Prophet maintains 1.4--10$\times$ speedup across BS=1--16 (\Cref{fig:batch_bandwidth}a).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig6_batch_bandwidth.pdf}
\vspace{-15pt}
\caption{Batch size scaling: (a) Speedup remains stable at 1.4--10$\times$. (b) Prophet's bandwidth normalized to On-Demand (with deduplication) vs.\ PreScope (no deduplication, 4$\times$ experts).}
\label{fig:batch_bandwidth}
\vspace{-10pt}
\end{figure}

\noindent\textbf{When Prediction Matters Less.}
At larger batch sizes (BS$>$16), popular experts (top 20\% handling 75\% of decisions per power-law) remain GPU-resident across iterations. Cache management and eviction policies dominate over prediction accuracy---an orthogonal concern beyond this paper's scope.

\noindent\textbf{Multi-GPU Scaling.}
Prophet's advantage is preserved under tensor parallelism (TP); relative speedup remains within 2\% across TP=1--8. Expert loading becomes a per-GPU operation, but routing decisions are determined by model architecture, not parallelization strategy.

% \noindent\textbf{Continuous Batching Compatibility.}
% Prophet's per-token routing history naturally extends to continuous batching frameworks (vLLM, TGI) that dynamically schedule requests. Per-request history persists across batch iterations, and predictions update as new tokens generate. We leave full integration with production serving frameworks for future work, noting that Prophet's predictor is compatible with their iteration-level batching and that the power-law expert concentration ensures popular experts remain cached across request boundaries.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
