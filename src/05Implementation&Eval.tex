\begin{figure*}[t]
    \centering
    % Switch Transformer
    \subfloat[Switch Transformer]{%
        \includegraphics[width=0.32\linewidth]{figs/switch_memory_usage}%
        \label{fig:switch-memory-usage}
    }
    \hfill
    % Qwen1.5
    \subfloat[Qwen1.5 MoE]{%
        \includegraphics[width=0.32\linewidth]{figs/qwen15_memory_usage}%
        \label{fig:qwen15-memory-usage}
    }
    \hfill
    % Qwen3 MoE
    \subfloat[Qwen3 MoE]{%
     \includegraphics[width=0.32\linewidth]{figs/qwen3_memory_usage}%
        %\includegraphics[width=0.32\linewidth]{figs/qwen15_memory_usage}%
        \label{fig:qwen3-memory-usage}
    }
    \vspace{-10pt}
    \caption{Memory usage overhead comparison of baseline methods relative to Prophet across different MoE architectures. 
    Prophet's batch-aware deduplication reduces memory consumption: (a) on Switch Transformer, baselines use $1.5\times$–$2.5\times$ more memory; 
    (b) on Qwen1.5 MoE, competing methods consume $1.8\times$–$2.2\times$ more memory; 
    (c) on Qwen3 MoE, Prophet maintains efficiency at scale with up to $2.8\times$ memory savings.
    %, enabling deployment on resource-constrained hardware. 
    Here and after: \textcolor{red}{$\times$}: means the GPU hits OoM Error without optimization or the feature is unavailable. We ensure optimizing the memory footprint of the baselines where ever possible.
}
    \label{fig:combined-memory-usage}
\end{figure*}
\begin{figure*}[h]
    \centering
    % Switch Transformer
    \subfloat[Switch Transformer]{%
        \includegraphics[width=0.32\linewidth]{figs/switch_cache_hit_rate}%
        \label{fig:switch-cache-hit-rate}
    }
    \hfill
    % Qwen1.5
    \subfloat[Qwen1.5 MoE]{%
        \includegraphics[width=0.32\linewidth]{figs/qwen15_cache_hit_rate}%
        \label{fig:qwen15-cache-hit-rate}
    }
    \hfill
    % Qwen3 MoE
    \subfloat[Qwen3 MoE]{%
        \includegraphics[width=0.32\linewidth]{figs/qwen3_cache_hit_rate}%
        % \includegraphics[width=0.32\linewidth]{figs/qwen15_cache_hit_rate}%
        \label{fig:qwen3-cache-hit-rate}
    }
    \vspace{-10pt}
    \caption{Cache hit rates of different MoE expert caching strategies across MoE Models. 
    Prophet's neural prediction and hierarchical caching achieve superior hit rates: (a) on Switch Transformer, significantly outperforming simple policies (LRU, LFU) and advanced baselines (ProMoE, ExpertFlow); 
    (b) on Qwen1.5 MoE, sustaining 85–95\% hit rates across batch sizes; 
    (c) on Qwen3 MoE, delivering consistent 90–99\% hit rates, demonstrating scalability to larger architectures with hundreds of experts.}
    \label{fig:combined-cache-hit-rate}
\end{figure*}
 
\section{Implementation and Evaluation}
\label{sec:implementation}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 

\subsection{Implementation}
\label{subsec:implementation-details}

\noindent\textbf{Neural Predictor}
We implement the neural predictor as a 4-layer dense transformer with 8.4M parameters. The predictor processes the 3-layer routing history to predict expert selections for horizons $h \in \{1, 2, 3\}$ simultaneously, enabling the caching system to prefetch experts for multiple future layers---immediate predictions ($h=1$) target L1 cache, while longer horizons ($h=2,3$) enable proactive loading from L2 cache or host memory. Training uses routing traces from the same six datasets used in our information-theoretic analysis (\Cref{sec:Motivation}): Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText. Details of the Neural Predictor are given in \Cref{subsec:neural-predictor} and \cref{app:predictor}.

\noindent\textbf{Baseline Systems}
We compare our proposal against six fundamentally different approaches to expert caching. \textbf{Traditional caching} (LRU, LFU) represents reactive strategies that load experts on-demand without prediction, relying solely on access frequency patterns that prove ineffective for dynamic expert routing.
\textbf{1. ProMoE}~\cite{promoe} employs stride-based prefetching, assuming regular access patterns by predicting that expert $E_i$ will be needed again after a fixed stride interval. This fails when routing exhibits complex temporal dependencies that require sequence modeling. \textbf{2. ExpertFlow}~\cite{expertflow} uses dynamic expert placement with token distribution analysis, but lacks cross-layer prediction capabilities and degrades beyond 32 experts due to capacity assumptions. \textbf{3. FATE}~\cite{fate} implements pipeline-aware scheduling optimized for edge deployment, using cross-layer gates but missing batch-level optimization entirely.
\textbf{4. PreGated-MoE}~\cite{pregated} represents architectural approaches where router $L$ predicts experts for layer $L+1$ during training, achieving near-optimal performance, but requiring costly model retraining. In contrast, \textbf{Prophet} operates as plug-and-play deployment, using cross-layer attention to capture dependencies that single-layer approaches miss, combined with batch deduplication that existing systems cannot exploit.

\noindent\textbf{Target Architectures}
We evaluate on five production MoE architectures representing the current state-of-the-art: Mixtral-8x7B, Qwen1.5-MoE-A2.7B, Qwen3-30B, DeepSeek-MoE-16B, and GPT-OSS-20B. These models exhibit diverse expert counts (64--256), routing strategies (top-4 to top-8), and architectural designs. We exclude Switch Transformer from our main evaluation as its top-1 routing is substantially easier to predict and already well-studied in prior work~\cite{promoe, pregated}; our target models with top-k routing present more challenging and comprehensive benchmarks that better represent modern MoE deployments.

\noindent\textbf{Implementation Details}
Our implementation comprises 15,200 \texttt{LoC} using PyTorch 2.0~\cite{pytorch} and HuggingFace Transformers~\cite{hftf}. We build on existing MoE infrastructures, instrumenting routing decisions through lightweight hooks. We faithfully reimplemented {\em all baselines}, ensuring fair comparison under identical hardware constraints. Our experiments used NVIDIA A100 (Mixtral), RTX 3090 (Qwen1.5), and H100 (Qwen3, DeepSeek, GPT-OSS) GPUs under iso-cache constraints: 20MB for Qwen1.5 MoE, 40MB for Mixtral, and 80MB for Qwen3/DeepSeek/GPT-OSS. We evaluated across batch sizes 1-64 (context length: 256) with 770+ configurations, reporting means over 150 runs with 95\% confidence intervals.


%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\begin{figure*}[ht!]
    \centering
    % Switch Transformer
    \subfloat[Switch Transformer]{%
        \includegraphics[width=0.32\linewidth]{figs/switch_mean_latency}%
        \label{fig:switch-mean-latency}
    }
    \hfill
    % Qwen1.5 (first)
    \subfloat[Qwen1.5 MoE]{%
        \includegraphics[width=0.32\linewidth]{figs/qwen15_mean_latency}%
        \label{fig:qwen15-mean-latency}
    }
    \hfill
    % Qwen3 MoE
    \subfloat[Qwen3 MoE]{%
        %\includegraphics[width=0.32\linewidth]{figs/qwen3_mean_latency}%
        \includegraphics[width=0.32\linewidth]{figs/qwen3_mean_latency}%
        \label{fig:qwen3-mean-latency}
    }
    \vspace{-10pt}
    \caption{Mean latency overhead of baseline methods compared to Prophet across different MoE architectures. 
    Prophet consistently outperforms all baselines: (a) on Switch Transformer, achieving $4\times$ speedup over ProMoE and $2.0\times$--$2.2\times$ over FATE and ExpertFlow; 
    (b) on Qwen1.5 MoE, delivering $1.5\times$--$2.8\times$ speedup; 
    (c) on Qwen3 MoE, maintaining strong performance scaling with $2.1\times$--$4.5\times$ improvements, validating effectiveness across diverse expert configurations.}
    \label{fig:combined-mean-latency}
\end{figure*}
\begin{figure*}[h!]
    \centering
    % Switch Transformer
    \subfloat[Switch Transformer]{%
        \includegraphics[width=0.32\linewidth]{figs/switch_p99_latency}%
        \label{fig:switch-p99-latency}
    }
    \hfill
    % Qwen1.5
    \subfloat[Qwen1.5 MoE]{%
        \includegraphics[width=0.32\linewidth]{figs/qwen15_p99_latency}%
        \label{fig:qwen15-p99-latency}
    }
    \hfill
    % Qwen3 MoE
    \subfloat[Qwen3 MoE]{%
        \includegraphics[width=0.32\linewidth]{figs/qwen3_p99_latency}%
        %\includegraphics[width=0.32\linewidth]{figs/qwen15_p99_latency}
        \label{fig:qwen3-p99-latency}
    }
    \vspace{-10pt}
    \caption{P99 latency overhead of baseline methods compared to Prophet across different MoE architectures. 
    Prophet maintains consistently low tail latencies critical for production deployment: (a) on Switch Transformer, competing methods exhibit $1.5\times$–$2.8\times$ worse P99 performance; 
    (b) on Qwen1.5 MoE, Prophet achieves $1.6\times$–$2.5\times$ lower tail latencies; 
    (c) on Qwen3 MoE, Prophet delivers robust tail-latency guarantees with $1.8\times$–$3.2\times$ improvements 
    {(with additional memory optimizations)}.
    %, ensuring SLA compliance at scale.
    }
    %\vspace{-10pt}
    \label{fig:combined-p99-latency}
\end{figure*}
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%

%\vspace{-20pt}
\subsection{ Analysis of Results}
\label{subsec:results-analysis}

%\todo{Start with memory, then move to end2end latency -- what are we exactly measuring; for deduplication also gide the results; }
\noindent\textbf{Memory Efficiency and Cache Performance}
Prophet's memory efficiency advantage stems from exploiting power-law expert popularity through batch deduplication. For instance, as shown in ~\Cref{fig:combined-memory-usage}, at a batch size of 64, naive loading requires 256 expert transfers for Switch Transformer (4 experts/token × 64 tokens), but power-law concentration means only 32 unique experts are needed (87.6\% reduction). Baselines cannot exploit this structure, consuming $1.5\times$--$15\times$ more memory across different batch sizes and causing GPU memory bloating that forces smaller batch sizes or expert swapping.
Memory scaling follows our validated sublinear relationship because popular experts dominate routing decisions. Expert-23 appears in 45\% of routing decisions across all batch items, so loading it once serves multiple requests. ExpertFlow and ProMoE miss this optimization entirely, while traditional caching loads experts reactively without considering cross-batch reuse patterns.
Hierarchical cache performance (\Cref{fig:combined-cache-hit-rate}) demonstrates prediction quality through confidence calibration. Prophet's 99.4\% hit rate versus 65-80\% for baselines reflects three advantages: (1) \textbf{Prediction accuracy}: $\sim87$\% versus $\sim 60$\% enables better cache placement, (2) \textbf{Confidence estimation}: high-confidence predictions (72.3\% L1 hits) receive priority allocation, and (3) \textbf{Hierarchical coverage}: medium/low confidence predictions provide speculative coverage (27.1\% L2 hits) that traditional binary caching cannot achieve (Additional results in \Cref{sec:additional-results}).

\noindent\textbf{Memory Hierarchy Analysis}
Our analysis highlights Prophet's advantages using the metric of  \textit{AMATE} (Average Memory Access Time per Expert). We regard AMATE as a critical performance indicator because latency in Mixture-of-Experts (MoE) architectures is primarily constrained by the time required to fetch expert parameters. Operationally, AMATE quantifies the mean latency to locate the selected expert(s) and transfer their parameters from the host (CPU) memory to the device (GPU) memory, corresponding to the CPU-side stage depicted in the lower portion of \Cref{fig:predictor_architecture}.
%and \textit{TMAT} (Total Memory Access Time). 
%These capture complementary aspects of memory efficiency---per-expert vs. aggregate system behavior. 

\noindent{\underline{\textbf{AMATE:}} Prophet achieves $8\times$--$12\times$ better per-expert efficiency than baselines. At batch size of 1, improvements stem solely from cache effects, while at batch-32/64, deduplication accounts for $57$--$75\%$ of total gains, reflecting Prophet's ability to exploit expert popularity (top $20\%$ of experts serve $75\%$ of routes). Further details are given in \Cref{app:memory_hierarchy_analysis} and breakdown graphs are shown in \Cref{fig:amate-breakdown}.}


\noindent\textbf{Cross-Architecture Analysis}
Architecture-specific behaviors reveal why different MoE designs create distinct optimization challenges. \textbf{Switch Models} (sparse top-1 routing) shows the highest Prophet speedups ($4\times$ over ProMoE) because prediction errors have minimal impact (i.e., missing one expert prediction affects only one expert per token). The sparse routing pattern amplifies the benefits of prediction accuracy, making our 87\% accuracy highly effective.
\textbf{Qwen architectures} (dense top-k routing) exhibit different trends: smaller relative improvements ($2.8\times$ over baselines) but greater absolute memory savings due to increased expert overlap. Top-8 routing means that each token activates 8 experts, creating more deduplication opportunities when batch items share routing patterns. Prophet automatically adapts by predicting top-16 experts rather than top-3, ensuring adequate coverage for higher expert density.

\noindent\textbf{Baseline degradation patterns differ by architecture:} ExpertFlow performs reasonably on Switch Transformer but collapses on Qwen due to its 32-expert capacity assumption (Qwen has 64). ProMoE's stride-based prediction works marginally for Switch Transformer's more regular patterns but fails entirely on Qwen's complex top-k routing where expert selection depends on fine-grained probability distributions rather than deterministic top-1 choices.

\noindent\textbf{Ablation:} Component ablation reveals architecture dependent synergies: neural prediction provides consistent $1.8\times$--$2.4\times$ gains across all architectures, while batch de-duplication benefits $1.3\times$ gains on sparse Switch Transformer versus $2.1\times$ on dense Qwen. Hierarchical caching contributes uniformly ($1.2\times$--$1.7\times$) because confidence-based placement works regardless of routing strategy. The additive combination delivers $2\times$--$15\times$ improvements, with larger gains on architectures that provide more optimization opportunities.

\noindent\textbf{End-to-End Performance}
Prophet delivers $1.5\times$--$12.7\times$ speedups across all architectures (\Cref{fig:combined-mean-latency}), with performance scaling that reveals fundamental differences between approaches. {\em The key insight is that different baselines fail for different reasons:} ProMoE's stride-based prefetching assumes periodic patterns that do not exist in transformer routing, achieving $\sim 60$\% prediction accuracy versus our 87\%. ExpertFlow's single-layer optimization misses cross-layer dependencies, while FATE's edge focus limits aggressive prefetching when resources are available. Critically, baseline performance \textit{degrades} at higher batch sizes due to three factors: (1) \textbf{Cache thrashing}: traditional LRU/LFU policies cause increased misses as batch diversity grows, (2) \textbf{Bandwidth saturation}: reactive loading creates memory bottlenecks that worsen with batch size, and (3) \textbf{Missed deduplication}: baselines load identical experts multiple times per batch. Prophet's $1.8\times$--$4.2\times$ improvements over ProMoE stem from exploiting cross-layer dependencies: When layer 4 routes to expert-23, layer 7 predictably routes to expert-78, enabling proactive loading. P99 latency results (\Cref{fig:combined-p99-latency}) reveal the Prophet's critical advantage: $2\times$--$4.8\times$ tail latency improvements. Traditional systems exhibit variance due to unpredictable cache misses (a single missed expert causes $15\times$ loading delay). Prophet's confidence-based prefetching eliminates this variance by ensuring that high-probability experts are preloaded, maintaining consistent sub-millisecond access times.

\noindent\textbf{Cross-Domain Generalization}
A critical question for learned predictors is whether patterns generalize beyond training data. We evaluate Prophet's cross-domain robustness by training on a subset of datasets and testing on held-out domains. Our analysis across six datasets (Natural Questions, GSM8K, HumanEval, WikiText, Alpaca, SQuAD) reveals that routing patterns are predominantly \textit{model-intrinsic} rather than dataset-specific. The power-law exponent $\alpha$ varies by dataset (GSM8K: $\alpha=2.1$--$2.4$, more concentrated; HumanEval: $\alpha=1.2$--$1.9$, more diverse), but the underlying structure persists across domains. When training on two datasets and testing on a third, prediction accuracy drops only 3--8\% compared to in-domain evaluation. This robustness stems from Prophet learning layer-level routing progressions (syntactic→semantic→task-specific) that remain consistent regardless of input domain. The information-theoretic foundation (\Cref{sec:Motivation}) explains this: mutual information between cross-layer routing decisions reflects model architecture rather than input semantics, enabling generalization without per-dataset retraining.

\noindent\textbf{Predictor Overhead}
Prophet's neural predictor introduces minimal overhead: 8.4M parameters (0.1--0.2\% of typical MoE models), 0.15--0.18ms inference latency (<2\% of total inference time), and 3.5--4.2 hours training on A100 GPUs. Detailed overhead analysis is provided in \Cref{app:predictor}.

%%%%%
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%% 
