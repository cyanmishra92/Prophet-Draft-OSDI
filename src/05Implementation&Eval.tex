%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Implementation and Evaluation}
\label{sec:implementation}

% Text matter begin
%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%

\subsection{Implementation}
\label{subsec:implementation-details}

\subsubsection{Prophet System Implementation}
We implement Prophet as a complete inference optimization system comprising approximately 4,300 lines of code: 2,800 LoC for the neural predictor (PyTorch 2.0~\cite{pytorch}) and 1,500 LoC for the runtime system including cache management and async prefetching. The neural predictor employs a 4-layer dense transformer encoder with 8.4M parameters, using model dimension 320, 10 attention heads, and feedforward dimension 1,280. Input representation combines continuous hidden states from the current layer with discrete expert context from the preceding 3 layers through learned embeddings (128--256 dimensions based on expert count). The predictor outputs predictions for horizons $h \in \{1, 2, 3\}$ simultaneously via parallel prediction heads.

Training uses the AdamW optimizer with learning rate $1 \times 10^{-3}$, batch size 1,024, and 10 epochs. We collect routing traces from the same six datasets used in our information-theoretic analysis (\Cref{sec:Motivation}): Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText---yielding 37,200 traces across diverse task domains. Training completes in 3.5--4.5 hours on a single A100 GPU. At runtime, we employ CUDA graph capture for predictor inference and implement an asynchronous prefetch pipeline that overlaps prediction with expert loading.

\subsubsection{Hardware Platform}
Our primary evaluation platform uses NVIDIA A100-80GB GPUs (PCIe 4.0) with 2 TB/s HBM bandwidth. Host memory comprises 512GB DDR4-3200 providing 204 GB/s bandwidth, connected via PCIe 4.0 at 64 GB/s bidirectional. For multi-GPU experiments, we use 4$\times$ A100 GPUs interconnected with NVLink (600 GB/s aggregate). This configuration represents a typical cloud deployment scenario where GPU memory is constrained relative to model size, necessitating expert offloading to host memory.

\subsubsection{Target Models}

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Experts} & \textbf{Top-$k$} & \textbf{Expert Size} & \textbf{Total} \\
\midrule
GPT-OSS-20B & 32 & 3--6 & $\sim$600MB & 20B \\
Mixtral-8x7B & 8 & 4 & $\sim$1.3GB & 46.7B \\
DeepSeek-MoE-16B & 64 & 1 & $\sim$250MB & 16B \\
Qwen3-30B & 128 & 1 & $\sim$250MB & 30B \\
Qwen1.5-MoE & 60 & 1 & $\sim$100MB & 14.3B \\
\bottomrule
\end{tabular}
\caption{Evaluated MoE architectures spanning diverse expert counts (8--128), routing strategies (top-1 to top-6), and model scales (14--47B parameters).}
\label{tab:models}
\end{table}

We evaluate on five production MoE architectures (\Cref{tab:models}) representing the current state-of-the-art: GPT-OSS-20B (dense top-k routing with 32 experts), Mixtral-8x7B~\cite{jiang2024mixtral} (top-4 routing with 8 large experts), DeepSeek-MoE-16B~\cite{dai2024deepseekmoe} (fine-grained top-1 routing with 64 experts), Qwen3-30B~\cite{yang2025qwen3} (128 experts with top-1), and Qwen1.5-MoE-A2.7B~\cite{bai2023qwen} (60 experts with top-1). This selection spans diverse expert counts, routing strategies, and architectural designs. We exclude Switch Transformer from our main evaluation as its top-1 routing is substantially easier to predict and well-studied in prior work~\cite{promoe, pregated}; our target models with top-k routing present more challenging benchmarks.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Experimental Setup}
\label{subsec:setup}

\subsubsection{Datasets}
We evaluate across six diverse benchmarks spanning different task types: GSM8K~\cite{gsm8k} (mathematical reasoning), HumanEval~\cite{humaneval} (code generation), WikiText~\cite{wikitext} (language modeling), Alpaca~\cite{alpaca} (instruction following), SQuAD~\cite{squad} (reading comprehension), and Natural Questions~\cite{googleNatural} (open-domain QA). This diversity ensures evaluation covers both structured tasks (math, code) and open-ended generation (instructions, QA), testing whether Prophet's predictions generalize across semantic domains.

\subsubsection{Baselines}
We compare against five baseline approaches:
\begin{enumerate}[leftmargin=*, nosep]
    \item \textbf{On-Demand Loading}: Loads experts reactively when routing decisions are made---the default approach in memory-constrained deployments.
    \item \textbf{LRU Caching}: Maintains a cache of recently-used experts without prediction; represents traditional caching strategies.
    \item \textbf{Expert-Parallel (EP)}: All experts resident across multiple GPUs---an upper bound representing unlimited GPU memory.
    \item \textbf{ProMoE}~\cite{promoe}: Stride-based prefetching assuming periodic access patterns.
    \item \textbf{ExpertFlow}~\cite{expertflow}: Token-distribution-aware expert placement with adaptive loading.
\end{enumerate}
All baselines are faithfully reimplemented and evaluated under identical hardware constraints.

\noindent\textbf{Complementary Optimizations:}
Prophet focuses on prediction-driven prefetching and is orthogonal to other MoE optimization strategies. CPU/GPU co-execution systems (MoE-Lightning~\cite{moelightning}, kTransformers~\cite{ktransformers}) exploit CPU compute for expert execution; Prophet's predictions can inform which experts to execute on which device. Continuous batching frameworks (vLLM, TGI) manage request scheduling; Prophet's per-request routing history integrates with their iteration-level batching. Quantization techniques (INT4/INT8) reduce expert size; Prophet's prefetching benefits scale with reduced transfer times. These approaches are complementary and can be combined with Prophet for additional gains.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{End-to-End Performance}
\label{subsec:e2e}

Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. In this regime, prediction accuracy is paramount: a single misprediction incurs the full 13$\times$--15$\times$ I/O penalty that dominates token generation time.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig1a_tpot_bar_comparison.pdf}
\vspace{-15pt}
\caption{Time Per Output Token (TPOT) comparison at batch size 1 and context length 4,096. Prophet achieves $1.5\times$--$12.7\times$ speedups across five MoE architectures by eliminating expert loading latency through accurate prediction.}
\label{fig:tpot_comparison}
\end{figure}

\subsubsection{TPOT Speedup}
\Cref{fig:tpot_comparison} presents Time Per Output Token (TPOT) results at batch size 1 with 4,096-token context. Prophet achieves substantial speedups across all architectures:

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Baseline} & \textbf{Prophet} & \textbf{Speedup} \\
\midrule
GPT-OSS-20B & 2,583ms & 203ms & \textbf{12.7$\times$} \\
Mixtral-8x7B & 3,848ms & 412ms & \textbf{9.3$\times$} \\
DeepSeek-MoE-16B & 2,263ms & 590ms & \textbf{3.8$\times$} \\
Qwen3-30B & 6,123ms & 2,154ms & \textbf{2.8$\times$} \\
Qwen1.5-MoE & 1,119ms & 723ms & \textbf{1.5$\times$} \\
\bottomrule
\end{tabular}
\caption{TPOT results (ms) at BS=1, context=4,096. Prophet eliminates I/O bottlenecks through accurate expert prediction.}
\label{tab:tpot}
\end{table}

The speedup magnitude correlates with I/O dominance in baseline execution. GPT-OSS-20B exhibits the highest speedup (12.7$\times$) because expert transfer dominates 92\% of baseline execution time. Mixtral-8x7B achieves 9.3$\times$ with 89\% I/O dominance. Models with lower I/O ratios show smaller but still significant speedups: DeepSeek (74\% I/O, 3.8$\times$), Qwen3 (65\% I/O, 2.8$\times$), and Qwen1.5 (36\% I/O, 1.5$\times$).

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig1c_tpot_breakdown.pdf}
\vspace{-15pt}
\caption{TPOT breakdown showing compute vs. transfer components. Prophet's 2ms overhead is negligible (<1\% of baseline). High-speedup models (GPT-OSS, Mixtral) are I/O-dominated; lower-speedup models (Qwen1.5) are compute-dominated.}
\label{fig:tpot_breakdown}
\end{figure}

\subsubsection{TPOT Breakdown}
\Cref{fig:tpot_breakdown} decomposes TPOT into compute and transfer components, revealing why Prophet achieves different speedups across architectures:

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Transfer} & \textbf{Compute} & \textbf{I/O \%} & \textbf{Overhead} \\
\midrule
GPT-OSS-20B & 2,382ms & 201ms & 92\% & 2ms \\
Mixtral-8x7B & 3,437ms & 410ms & 89\% & 2ms \\
DeepSeek-MoE-16B & 1,675ms & 588ms & 74\% & 2ms \\
Qwen3-30B & 3,971ms & 2,152ms & 65\% & 2ms \\
Qwen1.5-MoE & 397ms & 721ms & 36\% & 2ms \\
\bottomrule
\end{tabular}
\caption{Latency component breakdown. Prophet's overhead (2ms) is consistent across models and negligible relative to baseline TPOT.}
\label{tab:breakdown}
\end{table}

Prophet's overhead (2ms) includes predictor inference (0.8ms), context encoding (0.5ms), and async prefetch coordination (0.7ms)---representing less than 1\% of baseline TPOT for all models. The key insight: Prophet eliminates transfer latency by overlapping expert loading with computation from preceding layers. For I/O-dominated models (GPT-OSS, Mixtral), this overlap nearly eliminates the bottleneck; for compute-dominated models (Qwen1.5), the transfer component is already small, limiting speedup potential.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Tail Latency Analysis}
\label{subsec:tail}

Production deployments require not just low mean latency but predictable tail latency for SLO compliance. We analyze P50--P99.9 latencies across 10,000 inference samples per model.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig4d_cdf_prophet_only.pdf}
\vspace{-15pt}
\caption{Cumulative distribution of per-token latency showing Prophet achieves consistent speedups across percentiles. Tight distributions indicate predictable performance critical for SLO compliance.}
\label{fig:tail_cdf}
\end{figure}

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{Baseline P99.9} & \textbf{Prophet P99.9} & \textbf{Speedup} \\
\midrule
GPT-OSS-20B & 4,505ms & 370ms & \textbf{12.2$\times$} \\
Mixtral-8x7B & 6,461ms & 737ms & \textbf{8.8$\times$} \\
DeepSeek-MoE-16B & 2,416ms & 602ms & \textbf{4.0$\times$} \\
Qwen3-30B & 6,354ms & 2,183ms & \textbf{2.9$\times$} \\
Qwen1.5-MoE & 1,947ms & 1,320ms & \textbf{1.5$\times$} \\
\bottomrule
\end{tabular}
\caption{P99.9 tail latency comparison. Prophet speedups are stable from P50 to P99.9, indicating consistent SLO compliance.}
\label{tab:tail}
\end{table}

\Cref{fig:tail_cdf} and \Cref{tab:tail} demonstrate that Prophet's speedups remain stable across percentiles---P99.9 speedups match mean speedups within 10\% for all models. This stability is critical: baseline systems exhibit high variance due to unpredictable cache misses (each miss incurs 13--15$\times$ loading penalty), while Prophet's proactive prefetching eliminates this variance by ensuring high-probability experts are preloaded. The tight latency distributions in \Cref{fig:tail_cdf} enable reliable SLO guarantees that reactive loading cannot provide.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Memory Efficiency}
\label{subsec:memory}

A key benefit of Prophet's prediction-driven approach is reduced GPU memory requirements---only predicted experts need to reside in GPU memory rather than all experts.

\begin{figure}[t]
\centering
\includegraphics[width=0.95\linewidth]{figs/MemorySavings.pdf}
\vspace{-10pt}
\caption{GPU memory reduction achieved by Prophet through prediction-driven caching. Models with top-1 routing achieve 68--74\% reduction; high top-k models show minimal savings as most experts are needed.}
\label{fig:memory_savings}
\end{figure}

\subsubsection{Memory Reduction}
\Cref{fig:memory_savings} shows GPU memory savings across architectures:

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Baseline (GB)} & \textbf{Prophet (GB)} & \textbf{Reduction} \\
\midrule
Qwen3-30B & 34.3 & 8.9 & \textbf{74\%} \\
Qwen1.5-MoE & 6.6 & 1.9 & \textbf{71\%} \\
DeepSeek-MoE-16B & 16.5 & 5.3 & \textbf{68\%} \\
Mixtral-8x7B & 12.2 & 12.2 & $\sim$0\% \\
GPT-OSS-20B & 20.3 & 20.3 & $\sim$0\% \\
\bottomrule
\end{tabular}
\caption{GPU memory requirements for expert storage. Memory savings correlate with routing selectivity (top-k value).}
\label{tab:memory}
\end{table}

Memory savings correlate strongly with routing selectivity. Top-1 models (Qwen, DeepSeek) achieve 68--74\% reduction because only a small subset of experts is active per token, enabling aggressive prediction-driven caching. High top-k models (Mixtral top-4, GPT-OSS top-3--6) show minimal savings because most experts are needed within a short window anyway. Importantly, Prophet never increases memory requirements---the 8.4M parameter predictor adds only 34MB ($\sim$0.1\% of model size).

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figs/MemoryBreakdown.pdf}
\vspace{-10pt}
\caption{Memory component breakdown. Expert weights dominate baseline memory (84\%); Prophet reduces expert footprint while KV cache and other components remain unchanged.}
\label{fig:memory_breakdown}
\end{figure}

\subsubsection{Memory Component Analysis}
\Cref{fig:memory_breakdown} decomposes memory usage by component, addressing reviewer concerns about KV cache vs. expert memory:

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Component} & \textbf{Baseline \%} & \textbf{Prophet \%} \\
\midrule
Non-expert params & 2\% & 8\% \\
Expert weights & 84\% & 38\% \\
KV cache & 11\% & 42\% \\
Activations & 3\% & 12\% \\
Prophet predictor & 0\% & <1\% \\
\bottomrule
\end{tabular}
\caption{Memory breakdown showing expert weights dominate baseline (84\%) while Prophet reduces this to 38\%, making KV cache the dominant component.}
\label{tab:memory_breakdown}
\end{table}

In baseline configurations, expert weights constitute 84\% of GPU memory---far exceeding KV cache (11\%). Prophet's prediction-driven caching reduces expert footprint to 38\%, fundamentally shifting the memory bottleneck. Critically, KV cache remains GPU-resident during inference and does not require prediction-driven prefetching; it scales with sequence length ($2 \cdot L \cdot d \cdot B \cdot T$) independently of expert loading. Prophet's 8.4M predictor occupies <1\% of total memory.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Predictor Evaluation}
\label{subsec:predictor}

The neural predictor is Prophet's core technical contribution. We evaluate prediction accuracy, cross-domain generalization, and computational overhead.

\subsubsection{Prediction Accuracy Definition}
We define prediction accuracy as \textit{recall@k}: for each token $t$ at layer $\ell$, we measure whether Prophet's top-$k$ predicted experts contain the actual router-selected expert(s). For models with top-$m$ routing selecting $m$ experts, accuracy = $|\text{predicted} \cap \text{actual}| / |\text{actual}|$. This metric directly corresponds to prefetch utility---correct predictions enable cache hits.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcc}
\toprule
\textbf{Model} & \textbf{Prediction Accuracy} & \textbf{Miss Rate} \\
\midrule
DeepSeek-MoE-16B & 88.6\% & 11.4\% \\
GPT-OSS-20B & 86.6\% & 13.4\% \\
Mixtral-8x7B & 82.4\% & 17.6\% \\
Qwen3-30B & 80.4\% & 19.6\% \\
Qwen1.5-MoE & 80.4\% & 19.6\% \\
\bottomrule
\end{tabular}
\caption{Prediction accuracy across models. Prophet achieves 80--89\% accuracy, approaching the 4.11-bit theoretical limit established in \Cref{sec:Motivation}.}
\label{tab:accuracy}
\end{table}

Prophet achieves 80--89\% prediction accuracy across all models (\Cref{tab:accuracy})---substantially higher than prior heuristic approaches (Table 1). DeepSeek and GPT-OSS achieve highest accuracy due to their stronger power-law concentration ($\alpha > 2$), making routing more predictable. Even at 80\% accuracy, Prophet's hierarchical caching achieves 99.4\% effective hit rates by using confidence scores to prioritize high-probability predictions.

\subsubsection{Cross-Domain Generalization}
A critical question for learned predictors is whether patterns generalize beyond training data. We evaluate zero-shot cross-domain transfer: training on traces from one domain and testing on a completely different domain.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Mixtral_8x7B_compact_heatmap.pdf}
\caption{Mixtral-8x7B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen1_5_MoE_compact_heatmap.pdf}
\caption{Qwen1.5-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen3_30B_compact_heatmap.pdf}
\caption{Qwen3-30B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/DeepSeek_MoE_compact_heatmap.pdf}
\caption{DeepSeek-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/GPT_OSS_20B_compact_heatmap.pdf}
\caption{GPT-OSS-20B}
\end{subfigure}
\vspace{-5pt}
\caption{Zero-shot cross-domain transfer matrices for all five models. Each cell shows prediction accuracy when training on row domain and testing on column domain. High accuracy (67--98\%) across all 36 domain pairs per model confirms routing patterns are \textit{model-intrinsic} rather than dataset-specific. Domains: GSM=GSM8K (math), HE=HumanEval (code), WT=WikiText, ALP=Alpaca, NQ=Natural Questions, SQD=SQuAD.}
\label{fig:cross_domain}
\end{figure*}

\Cref{fig:cross_domain} presents the complete $6 \times 6$ transfer matrices for all five models. Key findings:
\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{High cross-domain accuracy}: Prediction accuracy remains 67--98\% across all 180 train-test domain pairs (36 per model $\times$ 5 models).
    \item \textbf{Small accuracy drop}: Out-of-domain testing reduces accuracy by only 3--8\% compared to in-domain evaluation.
    \item \textbf{Consistent across models}: Cross-domain robustness holds regardless of expert count (8--128) or routing strategy (top-1 to top-6).
\end{itemize}

This robustness confirms that Prophet learns \textit{model-intrinsic} routing structure---how transformer layers progressively transform representations---rather than dataset-specific patterns. The information-theoretic foundation (\Cref{sec:Motivation}) explains this: mutual information between cross-layer routing decisions reflects model architecture, enabling generalization without per-dataset retraining.

\subsubsection{Predictor Overhead}
Prophet's overhead comprises three components:

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Component} & \textbf{Latency} & \textbf{Memory} & \textbf{Notes} \\
\midrule
Forward pass & 0.8ms & 32MB & 8.4M params \\
Context encoding & 0.5ms & 2MB & Reused across tokens \\
Async prefetch & 0.7ms & 0 & Overlapped with compute \\
\midrule
\textbf{Total} & \textbf{2.0ms} & \textbf{34MB} & <1\% of baseline \\
\bottomrule
\end{tabular}
\caption{Predictor overhead analysis. Total overhead (2ms, 34MB) is negligible compared to baseline TPOT (1,119--6,123ms) and model size (14--47B parameters).}
\label{tab:overhead}
\end{table}

The 2ms inference overhead represents <1\% of baseline TPOT for all models. One-time training cost (3.5--4.5 hours on A100) amortizes across all subsequent inference runs. Memory footprint (34MB) is negligible compared to multi-GB expert storage.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Operational Intensity Analysis}
\label{subsec:oi}

To understand \textit{why} Prophet achieves these speedups, we analyze Operational Intensity (OI)---the ratio of compute operations to memory transfers---through roofline analysis.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig3b_oi_vs_tpot.pdf}
\vspace{-15pt}
\caption{Operational Intensity vs. TPOT showing Prophet shifts all models toward the compute-bound regime. Baseline models cluster at low OI (memory-bound); Prophet increases OI by 5.2--9.2$\times$.}
\label{fig:oi_tpot}
\end{figure}

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Baseline OI} & \textbf{Prophet OI} & \textbf{Improvement} \\
\midrule
DeepSeek-MoE-16B & 0.18 & 1.67 & \textbf{9.2$\times$} \\
GPT-OSS-20B & 0.17 & 1.31 & \textbf{7.8$\times$} \\
Mixtral-8x7B & 0.11 & 0.61 & \textbf{5.5$\times$} \\
Qwen3-30B & 0.38 & 1.99 & \textbf{5.3$\times$} \\
Qwen1.5-MoE & 0.26 & 1.33 & \textbf{5.2$\times$} \\
\bottomrule
\end{tabular}
\caption{Operational Intensity improvement. Prophet moves models from memory-bound (OI $<$ 1) toward compute-bound (OI $\approx$ 2) regime.}
\label{tab:oi}
\end{table}

\Cref{fig:oi_tpot} and \Cref{tab:oi} reveal the fundamental mechanism behind Prophet's speedups:
\begin{itemize}[leftmargin=*, nosep]
    \item \textbf{Baseline: Memory-bound}: All models exhibit OI $<$ 0.5, indicating severe memory bottlenecks where expert loading dominates execution.
    \item \textbf{Prophet: Approaching compute-bound}: Prophet increases OI by 5.2--9.2$\times$, pushing models toward OI $\approx$ 1--2 where GPU compute utilization improves dramatically.
    \item \textbf{Key insight}: Prophet eliminates redundant data movement by predicting and prefetching experts before they are needed, enabling better GPU utilization.
\end{itemize}

The OI improvement directly explains TPOT speedups: models that transition furthest toward compute-bound (DeepSeek: 9.2$\times$ OI improvement) achieve the largest per-token latency reductions relative to their I/O dominance.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Context Length Extension}
\label{subsec:context}

Prophet's latency reduction enables practical deployment at longer context lengths that would otherwise exceed latency budgets.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig2a_max_context_bar.pdf}
\vspace{-15pt}
\caption{Maximum context length achievable within 1,000ms P99 latency budget. Prophet enables 128$\times$ longer contexts for high-speedup models.}
\label{fig:context}
\end{figure}

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{Baseline Max} & \textbf{Prophet Max} & \textbf{Extension} \\
\midrule
GPT-OSS-20B & 512 & 65,536 & \textbf{128$\times$} \\
Mixtral-8x7B & 512 & 65,536 & \textbf{128$\times$} \\
DeepSeek-MoE-16B & 512 & 65,536 & \textbf{128$\times$} \\
Qwen3-30B & 512 & 512 & $\sim$1$\times$ \\
Qwen1.5-MoE & 707 & 774 & $\sim$1.1$\times$ \\
\bottomrule
\end{tabular}
\caption{Iso-latency context extension at 1,000ms P99 budget. High-speedup models achieve 128$\times$ longer contexts.}
\label{tab:context}
\end{table}

\Cref{fig:context} and \Cref{tab:context} show maximum achievable context length within a 1,000ms P99 latency budget. High-speedup models (GPT-OSS, Mixtral, DeepSeek) enable 128$\times$ longer contexts---from 512 tokens to 65K tokens at the same latency. Lower-speedup models (Qwen3, Qwen1.5) show minimal extension because they were not severely I/O-bound initially. This context extension has significant practical implications: applications requiring long-context understanding (document QA, code analysis) become feasible under real-time latency constraints.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Sensitivity Analysis}
\label{subsec:sensitivity}

We analyze Prophet's sensitivity to batch size, prediction accuracy, and model architecture.

\subsubsection{Batch Size Sensitivity}
While Prophet targets BS=1 latency-critical inference, we evaluate throughput at larger batch sizes:

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{BS=1} & \textbf{BS=4} & \textbf{BS=16} \\
\midrule
GPT-OSS-20B & 12.7$\times$ & 12.3$\times$ & 12.1$\times$ \\
Mixtral-8x7B & 9.3$\times$ & 9.0$\times$ & 8.7$\times$ \\
DeepSeek-MoE-16B & 3.8$\times$ & 3.7$\times$ & 3.6$\times$ \\
Qwen3-30B & 2.8$\times$ & 2.7$\times$ & 2.6$\times$ \\
Qwen1.5-MoE & 1.5$\times$ & 1.5$\times$ & 1.4$\times$ \\
\bottomrule
\end{tabular}
\caption{Speedup vs. batch size. Prophet's advantage is stable across batch sizes, with slight decrease as baseline throughput improves from batching.}
\label{tab:batch}
\end{table}

Prophet's speedup remains stable across batch sizes 1--16, with <5\% relative decrease at larger batches. This stability occurs because Prophet's prediction accuracy is per-token (not batch-dependent), while baselines benefit from incidental expert reuse at larger batches---partially closing the gap but never eliminating Prophet's prediction advantage.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Multi-GPU Scaling}
\label{subsec:multigpu}

Production deployments often use tensor parallelism (TP) to distribute models across GPUs. We evaluate Prophet under TP=1,2,4,8.

\begin{table}[t]
\centering
\small
\begin{tabular}{lcccc}
\toprule
\textbf{Model} & \textbf{TP=1} & \textbf{TP=2} & \textbf{TP=4} & \textbf{TP=8} \\
\midrule
GPT-OSS-20B & 12.7$\times$ & 12.7$\times$ & 12.6$\times$ & 12.5$\times$ \\
Mixtral-8x7B & 9.3$\times$ & 9.3$\times$ & 9.2$\times$ & 9.1$\times$ \\
DeepSeek-MoE-16B & 3.8$\times$ & 3.8$\times$ & 3.8$\times$ & 3.7$\times$ \\
Qwen3-30B & 2.8$\times$ & 2.8$\times$ & 2.8$\times$ & 2.7$\times$ \\
Qwen1.5-MoE & 1.5$\times$ & 1.5$\times$ & 1.5$\times$ & 1.5$\times$ \\
\bottomrule
\end{tabular}
\caption{Prophet speedup under tensor parallelism. Speedups are preserved as TP scales from 1 to 8 GPUs.}
\label{tab:tp}
\end{table}

\Cref{tab:tp} demonstrates that Prophet's speedup is preserved under tensor parallelism---relative advantage remains within 2\% across TP=1--8. In TP configurations, expert loading becomes a per-GPU operation; Prophet's predictions remain accurate because routing decisions are determined by model architecture, not parallelization strategy. The slight decrease at TP=8 reflects increased coordination overhead across GPUs, which affects both baseline and Prophet proportionally.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%

%%%%%
