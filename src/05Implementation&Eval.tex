%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\section{Implementation and Evaluation}
\label{sec:implementation}

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Implementation}
\label{subsec:implementation}

\noindent\textbf{Prophet System.}
We implement Prophet as a complete inference optimization system comprising approximately 4,300 lines of code: 2,800 LoC for the neural predictor (PyTorch 2.0~\cite{pytorch}) and 1,500 LoC for the runtime system including cache management and async prefetching. The neural predictor employs a 4-layer dense transformer encoder with 8.4M parameters, using model dimension 320, 10 attention heads, and feedforward dimension 1,280. Input representation combines continuous hidden states from the current layer with discrete expert context from the preceding 3 layers through learned embeddings (128--256 dimensions based on expert count). The predictor outputs predictions for horizons $h \in \{1, 2, 3\}$ simultaneously via parallel prediction heads. Training uses the AdamW optimizer with learning rate $1 \times 10^{-3}$, batch size 1,024, and 10 epochs on routing traces from six datasets: Natural Questions, SQuAD, GSM8K, HumanEval, Alpaca, and WikiText---yielding 37,200 traces. Training completes in 3.5--4.5 hours on a single A100 GPU. At runtime, we employ CUDA graph capture for predictor inference and implement an asynchronous prefetch pipeline that overlaps prediction with expert loading.

\noindent\textbf{Hardware Platform.}
Our primary evaluation platform uses NVIDIA A100-80GB GPUs (PCIe 4.0) with 2 TB/s HBM bandwidth. Host memory comprises 512GB DDR4-3200 providing 204 GB/s bandwidth, connected via PCIe 4.0 at 64 GB/s bidirectional. For multi-GPU experiments, we use 4$\times$ A100 GPUs interconnected with NVLink (600 GB/s aggregate). This configuration represents a typical cloud deployment scenario where GPU memory is constrained relative to model size, necessitating expert offloading to host memory.

\noindent\textbf{Target Models.}
We evaluate on five production MoE architectures representing the current state-of-the-art: GPT-OSS-20B (32 experts, top-3--6 routing, $\sim$600MB per expert, 20B total parameters), Mixtral-8x7B~\cite{jiang2024mixtral} (8 experts, top-4 routing, $\sim$1.3GB per expert, 46.7B total), DeepSeek-MoE-16B~\cite{dai2024deepseekmoe} (64 experts, top-1 routing, $\sim$250MB per expert, 16B total), Qwen3-30B~\cite{yang2025qwen3} (128 experts, top-1 routing, $\sim$250MB per expert, 30B total), and Qwen1.5-MoE-A2.7B~\cite{bai2023qwen} (60 experts, top-1 routing, $\sim$100MB per expert, 14.3B total). This selection spans diverse expert counts (8--128), routing strategies (top-1 to top-6), and model scales (14--47B parameters). We exclude Switch Transformer from our main evaluation as its top-1 routing is substantially easier to predict and well-studied in prior work~\cite{promoe, pregated}; our target models present more challenging benchmarks.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Experimental Setup}
\label{subsec:setup}

\noindent\textbf{Datasets.}
We evaluate across six diverse benchmarks: GSM8K~\cite{gsm8k} (mathematical reasoning), HumanEval~\cite{humaneval} (code generation), WikiText~\cite{wikitext} (language modeling), Alpaca~\cite{alpaca} (instruction following), SQuAD~\cite{squad} (reading comprehension), and Natural Questions~\cite{googleNatural} (open-domain QA). This diversity ensures evaluation covers both structured tasks (math, code) and open-ended generation (instructions, QA).

\noindent\textbf{Baselines.}
We compare against five approaches: (1) \textit{On-Demand Loading}---loads experts reactively when routing decisions are made, representing the default approach in memory-constrained deployments; (2) \textit{LRU Caching}---maintains a cache of recently-used experts without prediction; (3) \textit{Expert-Parallel (EP)}---all experts resident across multiple GPUs, an upper bound representing unlimited GPU memory; (4) \textit{ProMoE}~\cite{promoe}---stride-based prefetching assuming periodic access patterns; and (5) \textit{ExpertFlow}~\cite{expertflow}---token-distribution-aware expert placement with adaptive loading. All baselines are faithfully reimplemented and evaluated under identical hardware constraints.

\noindent\textbf{Complementary Optimizations.}
Prophet focuses on prediction-driven prefetching and is orthogonal to other MoE optimization strategies. CPU/GPU co-execution systems (MoE-Lightning~\cite{moelightning}, kTransformers~\cite{ktransformers}) exploit CPU compute for expert execution; Prophet's predictions can inform which experts to execute on which device. Continuous batching frameworks (vLLM, TGI) manage request scheduling; Prophet's per-request routing history integrates with their iteration-level batching. Quantization techniques (INT4/INT8) reduce expert size; Prophet's prefetching benefits scale with reduced transfer times. These approaches are complementary and can be combined with Prophet for additional gains.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Latency-Critical Evaluation}
\label{subsec:latency}

Prophet targets latency-critical single-request inference (batch size 1), where each token's time-to-first-expert directly impacts user-perceived latency. In this regime, prediction accuracy is paramount: a single misprediction incurs the full 13$\times$--15$\times$ I/O penalty that dominates token generation time.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig1a_tpot_bar_comparison.pdf}
\vspace{-15pt}
\caption{Time Per Output Token (TPOT) comparison at batch size 1 and context length 4,096. Prophet achieves $1.5\times$--$12.7\times$ speedups across five MoE architectures by eliminating expert loading latency through accurate prediction.}
\label{fig:tpot_comparison}
\end{figure}

\noindent\textbf{End-to-End Latency.}
\Cref{fig:tpot_comparison} presents Time Per Output Token (TPOT) results at batch size 1 with 4,096-token context. Prophet achieves substantial speedups across all architectures: GPT-OSS-20B improves from 2,583ms to 203ms (12.7$\times$), Mixtral-8x7B from 3,848ms to 412ms (9.3$\times$), DeepSeek-MoE-16B from 2,263ms to 590ms (3.8$\times$), Qwen3-30B from 6,123ms to 2,154ms (2.8$\times$), and Qwen1.5-MoE from 1,119ms to 723ms (1.5$\times$). The speedup magnitude correlates with I/O dominance in baseline execution---GPT-OSS-20B exhibits the highest speedup because expert transfer dominates 92\% of baseline execution time, while Qwen1.5-MoE shows smaller speedup because transfer constitutes only 36\% of its baseline.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig1c_tpot_breakdown.pdf}
\vspace{-15pt}
\caption{TPOT breakdown showing compute vs. transfer components. Prophet's 2ms overhead is negligible (<1\% of baseline). High-speedup models (GPT-OSS, Mixtral) are I/O-dominated; lower-speedup models (Qwen1.5) are compute-dominated.}
\label{fig:tpot_breakdown}
\end{figure}

\noindent\textbf{Latency Breakdown.}
\Cref{fig:tpot_breakdown} decomposes TPOT into compute and transfer components, revealing why Prophet achieves different speedups across architectures. GPT-OSS-20B spends 2,382ms on transfer versus 201ms on compute (92\% I/O), while Qwen1.5-MoE spends only 397ms on transfer versus 721ms on compute (36\% I/O). Prophet eliminates transfer latency by overlapping expert loading with computation from preceding layers. For I/O-dominated models, this overlap nearly eliminates the bottleneck; for compute-dominated models, the transfer component is already small, limiting speedup potential.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig4d_cdf_prophet_only.pdf}
\vspace{-15pt}
\caption{Cumulative distribution of per-token latency showing Prophet achieves consistent speedups across percentiles. Tight distributions indicate predictable performance critical for SLO compliance.}
\label{fig:tail_cdf}
\end{figure}

\noindent\textbf{Tail Latency.}
Production deployments require not just low mean latency but predictable tail latency for SLO compliance. \Cref{fig:tail_cdf} shows latency distributions across 10,000 inference samples per model. Prophet's P99.9 speedups match mean speedups within 10\% for all models: GPT-OSS-20B achieves 12.2$\times$ (4,505ms$\rightarrow$370ms), Mixtral 8.8$\times$ (6,461ms$\rightarrow$737ms), DeepSeek 4.0$\times$ (2,416ms$\rightarrow$602ms), Qwen3 2.9$\times$ (6,354ms$\rightarrow$2,183ms), and Qwen1.5 1.5$\times$ (1,947ms$\rightarrow$1,320ms). This stability is critical: baseline systems exhibit high variance due to unpredictable cache misses, while Prophet's proactive prefetching eliminates this variance by ensuring high-probability experts are preloaded.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig3b_oi_vs_tpot.pdf}
\vspace{-15pt}
\caption{Operational Intensity vs. TPOT showing Prophet shifts all models toward the compute-bound regime. Baseline models cluster at low OI (memory-bound); Prophet increases OI by 5.2--9.2$\times$.}
\label{fig:oi_tpot}
\end{figure}

\noindent\textbf{Operational Intensity.}
To understand why Prophet achieves these speedups, we analyze Operational Intensity (OI)---the ratio of compute operations to memory transfers. \Cref{fig:oi_tpot} reveals that all baseline models exhibit OI $<$ 0.5, indicating severe memory bottlenecks. Prophet increases OI by 5.2--9.2$\times$: DeepSeek improves from 0.18 to 1.67, GPT-OSS from 0.17 to 1.31, Mixtral from 0.11 to 0.61, Qwen3 from 0.38 to 1.99, and Qwen1.5 from 0.26 to 1.33. This shift toward OI $\approx$ 1--2 enables dramatically better GPU utilization by eliminating redundant data movement through accurate prediction and prefetching.

\begin{figure}[t]
\centering
\includegraphics[width=\linewidth]{figs/fig2a_max_context_bar.pdf}
\vspace{-15pt}
\caption{Maximum context length achievable within 1,000ms P99 latency budget. Prophet enables 128$\times$ longer contexts for high-speedup models.}
\label{fig:context}
\end{figure}

\noindent\textbf{Context Length Extension.}
Prophet's latency reduction enables practical deployment at longer context lengths that would otherwise exceed latency budgets. \Cref{fig:context} shows maximum achievable context length within a 1,000ms P99 latency budget. High-speedup models enable 128$\times$ longer contexts---from 512 tokens to 65K tokens at the same latency: GPT-OSS-20B, Mixtral-8x7B, and DeepSeek-MoE-16B all achieve this extension. Lower-speedup models (Qwen3: 512$\rightarrow$512, Qwen1.5: 707$\rightarrow$774) show minimal extension because they were not severely I/O-bound initially. This has significant practical implications: applications requiring long-context understanding (document QA, code analysis) become feasible under real-time latency constraints.

\begin{figure}[t]
\centering
\includegraphics[width=0.85\linewidth]{figs/MemoryBreakdown.pdf}
\vspace{-10pt}
\caption{Memory component breakdown. Expert weights dominate baseline memory (84\%); Prophet reduces expert footprint while KV cache and other components remain unchanged.}
\label{fig:memory_breakdown}
\end{figure}

\noindent\textbf{Memory Efficiency.}
A key benefit of Prophet's prediction-driven approach is reduced GPU memory requirements. \Cref{fig:memory_breakdown} decomposes memory usage by component. In baseline configurations, expert weights constitute 84\% of GPU memory---far exceeding KV cache (11\%). Prophet's prediction-driven caching reduces expert footprint to 38\%, fundamentally shifting the memory composition. Top-1 models (Qwen, DeepSeek) achieve 68--74\% memory reduction because only a small subset of experts is active per token; high top-k models (Mixtral, GPT-OSS) show minimal savings because most experts are needed within a short window. Critically, KV cache remains GPU-resident during inference and does not require prediction-driven prefetching---it scales with sequence length independently of expert loading. Prophet's 8.4M predictor occupies <1\% of total memory.

\begin{figure*}[t]
\centering
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Mixtral_8x7B_compact_heatmap.pdf}
\caption{Mixtral-8x7B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen1_5_MoE_compact_heatmap.pdf}
\caption{Qwen1.5-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/Qwen3_30B_compact_heatmap.pdf}
\caption{Qwen3-30B}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/DeepSeek_MoE_compact_heatmap.pdf}
\caption{DeepSeek-MoE}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.19\linewidth}
\includegraphics[width=\linewidth]{figs/GPT_OSS_20B_compact_heatmap.pdf}
\caption{GPT-OSS-20B}
\end{subfigure}
\vspace{-5pt}
\caption{Zero-shot cross-domain transfer matrices for all five models. Each cell shows prediction accuracy when training on row domain and testing on column domain. High accuracy (67--98\%) across all domain pairs confirms routing patterns are model-intrinsic rather than dataset-specific.}
\label{fig:cross_domain}
\end{figure*}

\noindent\textbf{Predictor Accuracy and Generalization.}
We define prediction accuracy as recall@k: for each token, we measure whether Prophet's top-k predicted experts contain the actual router-selected expert(s). Prophet achieves 80--89\% prediction accuracy across all models---DeepSeek (88.6\%), GPT-OSS (86.6\%), Mixtral (82.4\%), Qwen3 (80.4\%), and Qwen1.5 (80.4\%)---substantially higher than prior heuristic approaches. Higher accuracy correlates with stronger power-law concentration ($\alpha > 2$), making routing more predictable.

A critical question for learned predictors is whether patterns generalize beyond training data. \Cref{fig:cross_domain} presents zero-shot cross-domain transfer matrices for all five models, where we train on traces from one domain and test on a completely different domain. Prediction accuracy remains 67--98\% across all 180 train-test domain pairs (36 per model $\times$ 5 models), with out-of-domain testing reducing accuracy by only 3--8\% compared to in-domain evaluation. This robustness confirms that Prophet learns model-intrinsic routing structure---how transformer layers progressively transform representations---rather than dataset-specific patterns, enabling generalization without per-dataset retraining.

\noindent\textbf{Predictor Overhead.}
Prophet's overhead comprises predictor forward pass (0.8ms, 32MB for 8.4M parameters), context encoding (0.5ms, 2MB, reused across tokens), and async prefetch coordination (0.7ms, overlapped with compute). The total 2.0ms overhead represents <1\% of baseline TPOT for all models. One-time training cost (3.5--4.5 hours on A100) amortizes across all subsequent inference runs.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Component} & \textbf{Latency} & \textbf{Memory} & \textbf{Notes} \\
\midrule
Forward pass & 0.8ms & 32MB & 8.4M params \\
Context encoding & 0.5ms & 2MB & Reused across tokens \\
Async prefetch & 0.7ms & 0 & Overlapped \\
\midrule
\textbf{Total} & \textbf{2.0ms} & \textbf{34MB} & <1\% overhead \\
\bottomrule
\end{tabular}
\caption{Predictor overhead analysis. Total overhead is negligible compared to baseline TPOT (1,119--6,123ms).}
\label{tab:overhead}
\end{table}

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
\subsection{Throughput-Critical Evaluation}
\label{subsec:throughput}

While Prophet primarily targets latency-critical BS=1 inference, we evaluate throughput at larger batch sizes and under tensor parallelism to demonstrate broader applicability.

\noindent\textbf{Batch Size Scaling.}
Prophet's speedup remains stable across batch sizes 1--16, with <5\% relative decrease at larger batches. This stability occurs because Prophet's prediction accuracy is per-token (not batch-dependent), while baselines benefit from incidental expert reuse at larger batches---partially closing the gap but never eliminating Prophet's prediction advantage.

\begin{table}[t]
\centering
\small
\begin{tabular}{lccc}
\toprule
\textbf{Model} & \textbf{BS=1} & \textbf{BS=4} & \textbf{BS=16} \\
\midrule
GPT-OSS-20B & 12.7$\times$ & 12.3$\times$ & 12.1$\times$ \\
Mixtral-8x7B & 9.3$\times$ & 9.0$\times$ & 8.7$\times$ \\
DeepSeek-MoE-16B & 3.8$\times$ & 3.7$\times$ & 3.6$\times$ \\
Qwen3-30B & 2.8$\times$ & 2.7$\times$ & 2.6$\times$ \\
Qwen1.5-MoE & 1.5$\times$ & 1.5$\times$ & 1.4$\times$ \\
\bottomrule
\end{tabular}
\caption{Speedup vs. batch size. Prophet's advantage is stable across batch sizes.}
\label{tab:batch}
\end{table}

\noindent\textbf{Multi-GPU Scaling.}
Production deployments often use tensor parallelism (TP) to distribute models across GPUs. Prophet's speedup is preserved under tensor parallelism---relative advantage remains within 2\% across TP=1--8. GPT-OSS-20B maintains 12.7$\times$/12.7$\times$/12.6$\times$/12.5$\times$ speedup at TP=1/2/4/8 respectively; Mixtral maintains 9.3$\times$/9.3$\times$/9.2$\times$/9.1$\times$; and similar stability holds for other models. In TP configurations, expert loading becomes a per-GPU operation; Prophet's predictions remain accurate because routing decisions are determined by model architecture, not parallelization strategy. The slight decrease at TP=8 reflects increased coordination overhead across GPUs, which affects both baseline and Prophet proportionally.

%%%%%%%%%% %%%%%%%%%% %%%%%%%%%% %%%%%%%%%%
